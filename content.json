{"meta":{"title":"Riroaki","subtitle":"Riroaki's home","description":"我还要去完成应做之事。","author":"Riroaki","url":"http://riroaki.github.io","root":"/"},"pages":[{"title":"categories","date":"2019-03-24T12:31:34.000Z","updated":"2019-04-09T12:04:49.546Z","comments":true,"path":"categories/index.html","permalink":"http://riroaki.github.io/categories/index.html","excerpt":"","text":""},{"title":"about","date":"2019-03-24T12:33:57.000Z","updated":"2019-04-09T12:06:40.449Z","comments":true,"path":"about/index.html","permalink":"http://riroaki.github.io/about/index.html","excerpt":"","text":"这里是Riroaki的个人小站。 用来标记自己来时的路。"},{"title":"tags","date":"2019-03-24T12:26:06.000Z","updated":"2019-04-09T12:05:24.696Z","comments":true,"path":"tags/index.html","permalink":"http://riroaki.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"机器学不动了第一讲：贝叶斯分类","slug":"Machine-Learning-01-Bayes","date":"2019-06-21T04:19:05.000Z","updated":"2019-06-21T09:54:42.030Z","comments":true,"path":"Machine-Learning-01-Bayes/","link":"","permalink":"http://riroaki.github.io/Machine-Learning-01-Bayes/","excerpt":"","text":"本文是”机器学不动了”系列的第二篇内容，结合个人实现的代码食用口味更佳： https://github.com/Riroaki/LemonML/ 引子贝叶斯分类的核心是贝叶斯公式： $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$ $P(A) = \\Sigma_1^n{P(B|A_i)P(A_i)}$ 贝叶斯公式对多元变量同样适用，与变量是否独立也无关，是普适的公式。 为了介绍这个公式，我们首先来看一道概率题： 现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，而设定从A中抽取的概率和B中抽取的概率为1:2。 现已知从这两个容器里任意抽出了一个红球，问这个球来自容器 A 的概率是多少? 记抽中红球的事件为$P(B)$，记从容器A抽球的概率为$P(A)$。 根据贝叶斯公式，我们有：$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$。其中$P(B|A)$表示从容器A中抽球，抽到红球的概率。 在这个公式中： $P(A|B)$是已知B发生后A的条件概率，也叫做A的后验概率（posterior probability）。 $P(B|A)$是已知A发生后B的条件概率，是B的后验概率，在这里叫A的似然概率（likelihood）。 $P(A)$是事件发生之前我们对A的经验知识，与B无关，叫做A的先验概率（prior probability）。 $P(B)$是B的先验概率，在这里叫做标准化常量（normalized constant）。 根据这个关系，后验$P(A|B)$也可以叫做标准化的似然；似然和后验是可以相互转化的。 贝叶斯分类理论从这个公式引申开，我们可以套用在分类理论上： 已知一个待归类样本$X_i$的特征，那么求$X_i$属于第j个类的概率，就变成了一个后验概率。 把样本属于第j个类的概率记作事件$w_j$，这个后验概率可以表述为：$P(w_j|x=X_i)$，简记作$P(w_j|X_i)$。 那么，根据贝叶斯公式，我们有：$P(w_j|X_i)=\\frac{P(X_i|w_j)P(w_j)}{P(X_i)}$。 这里的似然是$P(X_i|w_j)$，先验概率是$P(w_j)$，标准化常量为$P(X_i)$。 那么，有了某个样本属于各个类别的概率，如何分类呢？ 很自然的，我们选择后验概率比较大的那一个概率对应的类别作为$X_i$的分类。 补充1：当我们只有先验概率的时候，我们选择先验概率较大的那一个类别作为分类。 补充2：当提供了混淆矩阵（confusion matrix）的时候，分类规则会更复杂一些，将在下文中作为拓展内容介绍。 参数估计我们已经有了概率的公式和决策理论，如何估计概率公式中的各个概率？ 答案是：从有类标签的数据（训练数据）中总结提取。 …… 混淆矩阵生活经验中，同样是分类错误，我们对于不同错误分类的容忍度往往是不同的，比如： 垃圾邮件分类问题中，相比重要邮件被错误分类为垃圾邮件而进入垃圾箱，我们更情愿多收到一些被当作正常邮件的垃圾邮件。 …… 本篇完","categories":[{"name":"机器学不动了","slug":"机器学不动了","permalink":"http://riroaki.github.io/categories/机器学不动了/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://riroaki.github.io/tags/Data-Mining/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://riroaki.github.io/tags/Machine-Learning/"}]},{"title":"机器学不动了第零讲：机器学习基础","slug":"Machine-Learning-00-Basics","date":"2019-06-21T04:00:30.000Z","updated":"2019-06-21T09:54:51.903Z","comments":true,"path":"Machine-Learning-00-Basics/","link":"","permalink":"http://riroaki.github.io/Machine-Learning-00-Basics/","excerpt":"","text":"本文是”机器学不动了”系列的第一篇内容，结合个人实现的代码食用口味更佳： https://github.com/Riroaki/LemonML/ 引子如今深度学习、数据挖掘、机器学习这些概念已经🔥到成为满大街都是的概念，由于其门槛低（调包）和某些fancy的功能，加上媒体的宣传和高薪的诱惑，无论计算机专业还是非计算机专业出身的人们都热衷于在其中寻找机会，我这个软工的菜🐔也不例外。当然，目前正处于新手期。 这一个系列主要记录了我学习和梳理的机器学习的知识，包含数学理论和代码实现，希望能够给入门者（包括我自己）提供一个参考。 由于本人懒癌晚期，博客将不定期更新。 看客们如果有问题或者留言可以直接在相关的博文下面留言，我们可以共同探讨解决。 当然也可以邮件联系本人：lilq@1285.com，欢迎理性讨论。 本系列内容属于个人原创，转载请声明出处，商业转载请联系本人，邮箱同上。 机器学习总览机器学习发源于统计学，主要的目标是用数学和程序语言描述事物的规律，从而为预测、决策提供参考。 以全局的视角来看机器学习这一领域的算法，主要分为有监督（Supervised）学习和无监督（Unsupervised）学习两类，此外还有半监督（Half-supervised）学习、强化（Reinforcement）学习： 有监督学习主要任务 回归（Regression）通常目标是得到连续的曲线，输出是连续的值 分类（Classification）通常目标是得到决策的边界，输出的是离散的类别 常见算法 Linear Regression：线性回归，以线性方式组合特征拟合连续曲线 Bayes：贝叶斯分类，通过概率模型计算样本属于各个分类的后验概率，进行分类 Logistic Regression：逻辑回归，在线性回归基础上增加激活函数以进行分类 Support Vector Machine：支持向量机，选取决策面较近的点用来计算决策面的参数 K Nearest Neighbor：K近邻，寻找距离较近的K个样本的标签取众数作为样本归类 Perceptron：感知机，二分类算法，最简单的神经网络 Decision Tree：决策树，可以看作从样本数据中学习if-else语句的组合，每一个判断都是数的一个节点，实现分类 Linear Discriminant Analysis：线性判别分析，通过找到特征的线性组合以用于降维，也是一种分类算法 算法的分类 如果从算法解决的问题分类，可以分为回归和分类两大类算法： 其中，线性回归为回归类的算法，其余算法均主要用于分类，当然也可以有回归的作用。因为这些分类算法大多是在连续的输出外进行处理获得类别，如逻辑回归、感知机、支持向量机等，如果用在回归上则输出的是分类前计算的结果。 如果从决策面的角度来看，上述的分类算法可以分为线性分类算法和非线性分类算法： 线性分类算法：分类面为线性/输出函数为线性形式（本质相同，采用不同的目标函数得到的模型） 包括：逻辑回归、支持向量机、感知机、线性判别分析 非线性算法：分类面为非线性/输出函数的形式为非线性 包括：贝叶斯、K近邻、决策树 如果从算法的实际含义角度看，上述的分类算法可以分为生成模型和判别模型： 生成模型：按照条件概率建立模型，基于高斯分布等假设，学习模型的参数用于分类 包括：贝叶斯模型、线性判别分析 判别模型：出于最大化在测试集上的表现，进行训练 包括：大部分其他分类算法 在基本算法的基础上，现代机器学习常见的还有集成（Ensemble）学习，其核心是”三个臭屁匠，顶个诸葛亮”，并不致力于产生最强的单个分类器，而是通过把训练不同的较弱分类器，并进行集合决策以获得最好的分类效果。 集成学习 Bagging/Bootstrap Aggregating：通过随机切分数据集，并行训练相同模型以获得更好的分类效果。 随机森林（Random Forest）算法正是基于bagging算法实现。 Boosting：通过训练一系列弱分类器并组合获得强分类器。 Stacking：训练一个组合不同模型的高层模型进行分类（上面两种算法对底层模型的组合方式是确定的）。 无监督学习主要任务 降维（Dimensionality Reduction）指的是将样本空间从高维特征投影到较低维度的特征从而实现提高计算效率的作用。 聚类（Clustering）指的是将无标签的样本按照样本之间的距离信息等，将相近的样本归为一个簇的算法，可以理解为没有样本标签的分类算法。 常见算法 Principle Component Analysis：主成分分析，通过提取协方差矩阵中的特征向量作为新特征实现降维。 与线性判别分析（LDA）相似的算法。 K Means：K均值算法，通过抽取相近点簇的重心作为簇的代表来实现聚类。 K Medoids：K中心点算法，和K Means算法相近，不同的是选取簇中最接近重心的点作为簇的代表。 Spectral Clustering：谱聚类，通过降维方法和K Means算法实现聚类。 Gaussian Mixture Model：高斯混合模型，是基于高斯分布的假设，通过点簇的分布估计参数以实现聚类。 K Means算法可以视为GMM的一种特殊形式。 Matrix Factorization：矩阵分解，是一类降维算法，包括奇异值分解、矩阵非负分解和稀疏编码等算法。 算法的分类 如果按照主要任务，可以将算法分为降维算法和聚类算法： 降维算法：主要包括主成分分析、矩阵分解 聚类算法：主要包括K均值、K中心点、谱聚类、高斯混合模型 半监督学习利用少量标注样本和大量未标注样本进行机器学习的算法。 由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。 强化学习没有特定的目标，强调环境的反馈作用，通过应对环境调整策略的算法。 由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。 深度学习这一块是近十年新的方向，也是目前机器学习最火的分支，但是不会在近期内容中出现：） 简言之，深度学习就是基于神经网络的算法，通过组合线性的神经元和非线性的激活层，以及搭建不同结构的网络，来实现回归或者预测、聚类等工作。 其”神经网络”形态的灵感得益于生物大脑的神经元连接结构，让人联想到”机器的大脑🧠”，加上诸如alphaGo等等一些新奇的成就带来的狂热使得众人为之疯狂，许多营销号和媒体甚至脑洞大开，大肆鼓吹”人工智能有害论”。 但目前而言，可解释性差、缺乏较统一的数学理论描述是其硬伤。而且也没有出现强人工智能的迹象，目前的神经网络，本质只是一种复杂的统计模型。 这场资本与舆论的狂欢已经在逐渐冷却…… 机器学习方法论一般来说，机器学习算法分为以下部分： 获取数据数据清洗拟合/训练 交叉验证 留一验证 K折验证 目标函数有两种表现形式： 损失函数，一般指需要最小化的目标函数 优化算法预测/测试评估概念 偏差Bias 方差Variance …… 本篇完。后续内容将大致按照本篇的顺序，依次介绍主要的几种具体算法以及其他技巧等内容。","categories":[{"name":"机器学不动了","slug":"机器学不动了","permalink":"http://riroaki.github.io/categories/机器学不动了/"}],"tags":[{"name":"Data Mining","slug":"Data-Mining","permalink":"http://riroaki.github.io/tags/Data-Mining/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://riroaki.github.io/tags/Machine-Learning/"}]},{"title":"Restart","slug":"Restart","date":"2019-05-28T01:52:55.000Z","updated":"2019-06-02T14:33:47.956Z","comments":true,"path":"Restart/","link":"","permalink":"http://riroaki.github.io/Restart/","excerpt":"","text":"今天把博客文章全都清空了。 主要是之前的文章太乱，缺乏整理；加上近期学了很多东西之后，回头看过去的内容觉得有些浅薄，决心从头开始写。 今后会在这里写一些机器学习，以及数据处理的东西。 当然还有一些工程向的内容，总之我会更加深思熟虑地推送文章。 （是不是也考虑一下换主题呢……哈哈还是算了估计又要挑很久）","categories":[],"tags":[{"name":"Hello, world!","slug":"Hello-world","permalink":"http://riroaki.github.io/tags/Hello-world/"}]}]}