<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Riroaki</title>
  
  <subtitle>Riroaki&#39;s home</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://riroaki.github.io/"/>
  <updated>2019-06-21T18:07:49.629Z</updated>
  <id>http://riroaki.github.io/</id>
  
  <author>
    <name>Riroaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学不动了-2：贝叶斯分类</title>
    <link href="http://riroaki.github.io/Machine-Learning-01-Bayes/"/>
    <id>http://riroaki.github.io/Machine-Learning-01-Bayes/</id>
    <published>2019-06-21T04:00:00.000Z</published>
    <updated>2019-06-21T18:07:49.629Z</updated>
    
    <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第二篇内容，可以结合个人实现的代码食用：</p><p><a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>贝叶斯分类的核心是贝叶斯公式：</p><ul><li>$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li><li>$P(A) = \Sigma_{i=1}^n{P(B|A_i)P(A_i)}$</li><li>贝叶斯公式对多元变量同样适用，与变量是否独立也无关，是普适的公式。</li></ul><p>为了介绍这个公式，我们首先来看一道概率题：</p><blockquote><p>现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，而设定从A中抽取的概率和B中抽取的概率为1:2。</p><p>现已知从这两个容器里任意抽出了一个红球，问这个球来自容器 A 的概率是多少?</p></blockquote><p>记抽中红球的事件为$P(B)$，记从容器A抽球的概率为$P(A)$。</p><p>根据贝叶斯公式，我们有：$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$。其中$P(B|A)$表示从容器A中抽球，抽到红球的概率。</p><p>在这个公式中：</p><ul><li>$P(A|B)$是已知B发生后A的条件概率，也叫做A的后验概率（posterior probability）。</li><li>$P(B|A)$是已知A发生后B的条件概率，是B的后验概率，在这里叫A的似然概率（likelihood）。</li><li>$P(A)$是事件发生之前我们对A的经验知识，与B无关，叫做A的先验概率（prior probability）。</li><li>$P(B)$是B的先验概率，在这里叫做标准化常量（normalized constant）。</li><li>根据这个关系，后验$P(A|B)$也可以叫做<strong>标准化</strong>的似然；似然和后验是可以相互转化的。</li></ul><h2 id="贝叶斯分类理论"><a href="#贝叶斯分类理论" class="headerlink" title="贝叶斯分类理论"></a>贝叶斯分类理论</h2><p>从这个公式引申开，我们可以套用在分类理论上：</p><ul><li>我们可以类比认为每一个类对应一个容器，样本都是这个类中生成（取出）的。</li><li>分类问题可以采用这样的表述：已知一个待归类样本$X_i$的特征，那么求$X_i$属于第j个类的概率，就变成了一个后验概率。</li><li>把样本属于第j个类的概率记作事件$w_j$，这个后验概率可以表述为：$P(w_j|x=X_i)$，简记作$P(w_j|X_i)$。</li><li>那么，根据贝叶斯公式，我们有：$P(w_j|X_i)=\frac{P(X_i|w_j)P(w_j)}{P(X_i)}$。</li><li>这里的似然是$P(X_i|w_j)$，先验概率是$P(w_j)$，标准化常量为$P(X_i)$。</li></ul><p>那么，有了某个样本属于各个类别的概率，如何分类呢？</p><p>很自然的，我们选择后验概率比较大的那一个概率对应的类别作为$X_i$的分类。</p><ul><li>补充1：当我们只有先验概率的时候，我们选择先验概率较大的那一个类别作为分类。</li><li>补充2：当采用混淆矩阵（confusion matrix）进行评估的时候，分类规则会更复杂一些，将在下文中作为拓展内容介绍。</li></ul><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>我们已经有了概率的公式和决策理论，如何估计概率公式中的各个概率？</p><p>答案是：从有类标签的数据（训练数据）中总结提取。</p><h3 id="先验概率的估计"><a href="#先验概率的估计" class="headerlink" title="先验概率的估计"></a>先验概率的估计</h3><p>这里的先验概率，就是在没有训练样本具体特征的值的分布情况下，某个类原始的信息。</p><p>很自然的，我们会把这个类别的样本数占全部有标签的样本的比重当作先验概率，</p><p>即：$P(w_j)=\Sigma_{i=1}^N{I(y_i=c_j)}/N$</p><h3 id="似然概率的估计"><a href="#似然概率的估计" class="headerlink" title="似然概率的估计"></a>似然概率的估计</h3><p>我们有不同的假设可以做出不同的估计。常用的有高斯分布假设、二项分布假设、伯努利分布。</p><p>这里只介绍高斯分布假设对应的参数估计方法。</p><ul><li>高斯分布的具体假设：对于某一个类$c_i$，其生成的样本满足高斯分布，即：$X\sim N(\mu, \Sigma)$</li><li>在这里我们采用极大似然估计（Maximum Likelihood Estimation）的做法来选取最佳参数：<ul><li>目标函数是$L(\mu_j,\Sigma_j)=\prod_{i=1}^N{P(X_i|w_j)}$。</li><li>由于不方便对目标函数求导，我们采用取对数的技巧，将连乘转化为连加：$l(\mu_j,\Sigma_j)=log(L)=\Sigma_{i=1}^NP(X_i|w_j)$</li><li>对对数似然求导，令导数为0求出$\mu,\Sigma$。由于样本是多维，求导过程比较复杂，详情参考附录。</li><li>总之，最终求出的结果和标量形式的惊人一致：<ul><li>$\hat\mu_j=\frac{1}{N_j}\Sigma_{i=0}^{N_j}X_i=\bar{x}$</li><li>$\hat\Sigma=\frac{1}{N_j}\Sigma_{i=1}^{N_j}(X_i-\hat\mu)(X_i-\hat\mu)^T=cov(X_i), where\ y_i = w_j$</li><li>很容易联想到标量情况下，$\hat\mu=\bar{X}, \hat\sigma=var(X)$</li></ul></li></ul></li><li>有了估计的参数以后，我们可以通过高斯概率密度公式求似然概率：$P(X; \mu, \Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(X-\mu)^T\Sigma^{-1}(X-\mu))$</li></ul><h3 id="标准化常量"><a href="#标准化常量" class="headerlink" title="标准化常量"></a>标准化常量</h3><p>这一项，理论上可以通过$P(X_i)=\Sigma_{j=1}^cP(X_i|w_j)P(w_j)$求出，也就是说得算出对样本对每一个类的先验概率和似然的乘积之和，才能计算分母。</p><p>然而正常情况下，它并不在我们的考虑范围内：</p><p>因为，每一个类的计算概率的分母都是这一项。既然我们最终的目标是比较后验概率的大小（或者与混淆矩阵关联后的大小，whatever），这一项作为相同的系数并不会产生影响：）</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>在这里因为我们并不是采取迭代优化，而是通过假设模型直接估计得出参数，所以不需要目标函数可导。</p><p>采用0-1损失函数就可以，即分类错误，结果就加一，分类正确结果就不变化的函数。</p><p>当然存在更复杂的形式，这就是在加入混淆矩阵之后需要考虑的事情：</p><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>生活经验中，同样是分类错误，我们对于不同错误分类的容忍度往往是不同的，比如：</p><ul><li>垃圾邮件分类问题中，相比重要邮件被错误分类为垃圾邮件而进入垃圾箱，我们更情愿多收到一些被当作正常邮件的垃圾邮件。</li><li>……</li></ul><p>待续。</p><h2 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h2><h3 id="多维高斯参数估计推导（Deriving-the-Maximum-Likelihood-Estimators）"><a href="#多维高斯参数估计推导（Deriving-the-Maximum-Likelihood-Estimators）" class="headerlink" title="多维高斯参数估计推导（Deriving the Maximum Likelihood Estimators）"></a>多维高斯参数估计推导（Deriving the Maximum Likelihood Estimators）</h3><p>来源：<a href="https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian">https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian</a></p><p>或者查看文档版本：<a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf">https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf</a></p><p><img src="/Machine-Learning-01-Bayes/appendix1.png" alt></p><p><img src="/Machine-Learning-01-Bayes/appendix2.png" alt></p><p><img src="/Machine-Learning-01-Bayes/appendix3.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;本文是”机器学不动了”系列的第二篇内容，可以结合个人实现的代码食用：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Riroaki/LemonML/&quot;&gt;https://github.com/Riroaki/LemonML/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;引子&quot;&gt;&lt;a href=&quot;#引子&quot; class=&quot;headerlink&quot; title=&quot;引子&quot;&gt;&lt;/a&gt;引子&lt;/h
        
      
    
    </summary>
    
      <category term="机器学不动了" scheme="http://riroaki.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B8%8D%E5%8A%A8%E4%BA%86/"/>
    
    
      <category term="Data Mining" scheme="http://riroaki.github.io/tags/Data-Mining/"/>
    
      <category term="Machine Learning" scheme="http://riroaki.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学不动了-1：机器学习基础</title>
    <link href="http://riroaki.github.io/Machine-Learning-00-Basics/"/>
    <id>http://riroaki.github.io/Machine-Learning-00-Basics/</id>
    <published>2019-06-20T16:00:00.000Z</published>
    <updated>2019-06-21T18:06:52.685Z</updated>
    
    <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第一篇内容，可以结合个人实现的代码食用：</p><p><a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>如今深度学习、数据挖掘、机器学习这些概念已经🔥到成为满大街都是的概念，由于其门槛低（调包）和某些fancy的功能，加上媒体的宣传和高薪的诱惑，无论计算机专业还是非计算机专业出身的人们都热衷于在其中寻找机会，我这个软工的菜🐔也不例外。当然，目前正处于新手期。</p><p>这一个系列主要记录了我学习和梳理的机器学习的知识，包含数学理论和代码实现，希望能够给入门者（包括我自己）提供一个参考。</p><p>由于本人懒癌晚期，博客将不定期更新。</p><p>看客们如果有问题或者留言可以直接在相关的博文下面留言，我们可以共同探讨解决。</p><p>当然也可以邮件联系本人：<a href="mailto:lilq1285@163.com">lilq1285@163.com</a>，欢迎理性讨论。</p><p><strong>本系列内容属于个人原创，转载请声明出处，商业转载请联系本人，邮箱同上。</strong></p><h2 id="机器学习总览"><a href="#机器学习总览" class="headerlink" title="机器学习总览"></a>机器学习总览</h2><p>机器学习发源于统计学，主要的目标是用数学和程序语言描述事物的规律，从而为预测、决策提供参考。</p><p>以全局的视角来看机器学习这一领域的算法，主要分为有监督（Supervised）学习和无监督（Unsupervised）学习两类，此外还有半监督（Half-supervised）学习、强化（Reinforcement）学习：</p><h3 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h3><h4 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h4><ul><li>回归（Regression）通常目标是得到连续的曲线，输出是连续的值</li><li>分类（Classification）通常目标是得到决策的边界，输出的是离散的类别</li></ul><h4 id="常见算法"><a href="#常见算法" class="headerlink" title="常见算法"></a>常见算法</h4><ul><li>Linear Regression：线性回归，以线性方式组合特征拟合连续曲线</li><li>Bayes：贝叶斯分类，通过概率模型计算样本属于各个分类的后验概率，进行分类</li><li>Logistic Regression：逻辑回归，在线性回归基础上增加激活函数以进行分类</li><li>Support Vector Machine：支持向量机，选取决策面较近的点用来计算决策面的参数</li><li>K Nearest Neighbor：K近邻，寻找距离较近的K个样本的标签取众数作为样本归类</li><li>Perceptron：感知机，二分类算法，最简单的神经网络</li><li>Decision Tree：决策树，可以看作从样本数据中学习if-else语句的组合，每一个判断都是数的一个节点，实现分类</li><li>Linear Discriminant Analysis：线性判别分析，通过找到特征的线性组合以用于降维，也是一种分类算法</li></ul><h4 id="算法的分类"><a href="#算法的分类" class="headerlink" title="算法的分类"></a>算法的分类</h4><ol><li>如果从算法解决的问题分类，可以分为回归和分类两大类算法：</li></ol><ul><li>其中，线性回归为回归类的算法，其余算法均主要用于分类，当然也可以有回归的作用。因为这些分类算法大多是在连续的输出外进行处理获得类别，如逻辑回归、感知机、支持向量机等，如果用在回归上则输出的是分类前计算的结果。</li></ul><ol start="2"><li>如果从决策面的角度来看，上述的分类算法可以分为线性分类算法和非线性分类算法：</li></ol><ul><li>线性分类算法：分类面为线性/输出函数为线性形式（本质相同，采用不同的目标函数得到的模型）<ul><li>包括：逻辑回归、支持向量机、感知机、线性判别分析</li></ul></li><li>非线性算法：分类面为非线性/输出函数的形式为非线性<ul><li>包括：贝叶斯、K近邻、决策树</li></ul></li></ul><ol start="3"><li>如果从算法的实际含义角度看，上述的分类算法可以分为生成模型和判别模型：</li></ol><ul><li>生成模型：按照条件概率建立模型，基于高斯分布等假设，学习模型的参数用于分类<ul><li>包括：贝叶斯模型、线性判别分析</li></ul></li><li>判别模型：出于最大化在测试集上的表现，进行训练<ul><li>包括：大部分其他分类算法</li></ul></li></ul><p>在基本算法的基础上，现代机器学习常见的还有集成（Ensemble）学习，其核心是”三个臭屁匠，顶个诸葛亮”，并不致力于产生最强的单个分类器，而是通过把训练不同的较弱分类器，并进行集合决策以获得最好的分类效果。</p><h4 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h4><ul><li>Bagging/Bootstrap Aggregating：通过随机切分数据集，并行训练相同模型以获得更好的分类效果。<ul><li>随机森林（Random Forest）算法正是基于bagging算法实现。</li></ul></li><li>Boosting：通过训练一系列弱分类器并组合获得强分类器。</li><li>Stacking：训练一个组合不同模型的高层模型进行分类（上面两种算法对底层模型的组合方式是确定的）。</li></ul><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><h4 id="主要任务-1"><a href="#主要任务-1" class="headerlink" title="主要任务"></a>主要任务</h4><ul><li>降维（Dimensionality Reduction）指的是将样本空间从高维特征投影到较低维度的特征从而实现提高计算效率的作用。</li><li>聚类（Clustering）指的是将无标签的样本按照样本之间的距离信息等，将相近的样本归为一个簇的算法，可以理解为没有样本标签的分类算法。</li></ul><h4 id="常见算法-1"><a href="#常见算法-1" class="headerlink" title="常见算法"></a>常见算法</h4><ul><li>Principle Component Analysis：主成分分析，通过提取协方差矩阵中的特征向量作为新特征实现降维。<ul><li>与线性判别分析（LDA）相似的算法。</li></ul></li><li>K Means：K均值算法，通过抽取相近点簇的重心作为簇的代表来实现聚类。</li><li>K Medoids：K中心点算法，和K Means算法相近，不同的是选取簇中最接近重心的点作为簇的代表。</li><li>Spectral Clustering：谱聚类，通过降维方法和K Means算法实现聚类。</li><li>Gaussian Mixture Model：高斯混合模型，是基于高斯分布的假设，通过点簇的分布估计参数以实现聚类。<ul><li>K Means算法可以视为GMM的一种特殊形式。</li></ul></li><li>Matrix Factorization：矩阵分解，是一类降维算法，包括奇异值分解、矩阵非负分解和稀疏编码等算法。</li></ul><h4 id="算法的分类-1"><a href="#算法的分类-1" class="headerlink" title="算法的分类"></a>算法的分类</h4><ol><li>如果按照主要任务，可以将算法分为降维算法和聚类算法：</li></ol><ul><li>降维算法：主要包括主成分分析、矩阵分解</li><li>聚类算法：主要包括K均值、K中心点、谱聚类、高斯混合模型</li></ul><h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><p>利用少量标注样本和大量未标注样本进行机器学习的算法。</p><p>由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。</p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>没有特定的目标，强调环境的反馈作用，通过应对环境调整策略的算法。</p><p>由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。</p><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p>这一块是近十年新的方向，也是目前机器学习最火的分支，但是预计不会在近期内容中出现。</p><p>简言之，深度学习就是基于神经网络的算法，通过组合线性的神经元和非线性的激活层，以及搭建不同结构的网络，来实现回归或者预测、聚类等工作。</p><p>其”神经网络”形态的灵感得益于生物大脑的神经元连接结构，让人联想到”机器的大脑🧠”，加上诸如alphaGo等等一些新奇的成就带来的狂热使得众人为之疯狂，许多营销号和媒体甚至脑洞大开，大肆鼓吹”人工智能有害论”。</p><p>但目前而言，可解释性差、缺乏较统一的数学理论描述是其硬伤。而且也没有出现强人工智能的迹象，目前的神经网络，本质只是一种复杂的统计模型。</p><p>随着研究陷入瓶颈，这场资本与舆论的狂欢已经在逐渐冷却，未来究竟如何发展也未可知：）</p><h2 id="机器学习方法论"><a href="#机器学习方法论" class="headerlink" title="机器学习方法论"></a>机器学习方法论</h2><p><strong>机器学习的本质在于从数据或者假设中建立模型、学习参数，去拟合一个未知的函数。</strong></p><p>根据论文《A Few Useful Things to Know about Machine Learning》，机器学习的过程可以表示为：</p><p>$LEARNING = REPRESENTATION + EVALUATION + OPTIMIZATION$</p><p>也就是说，机器学习主要分为三个过程：</p><ol><li>表示（Representation）：使用计算机能够执行的语言描述算法。这个阶段确定了模型的类型，所以决定了拟合/分类函数的假设空间（Hypothesis Space）——也就是说，在这一步，模型的参数个数和模型的计算方式已经确定，比如线性模型的$y=WX+B$，那么模型无法模拟非线性的分类/回归，这是选取的模型导致的。而具体是如何线性的函数，需要在接下来的过程中确定。</li><li>评估（Evaluation）：用于评估模型的好坏。根据任务的不同（回归、分类）确定了不同的种类，同时这个评估方法应当是能够方便地找到对应的优化函数的（更明确一点，评估的函数应该是可导的）。我们训练的目标就是最小化目标函数（误差型）或者最大化目标函数（精度型）。</li><li>优化（Optimization）：评估函数就像考试，有了考试我们就可以知道自己的薄弱环节，从而确定努力的方向。而有了评估函数，就有一个对应的优化函数用于调整模型的参数。<ul><li>通常我们采用基于梯度的方法，具体会在下面梯度下降这一概念中解释。</li></ul></li></ol><p>论文中列出了一个关于这三个部分的表格，在这里贴出来：</p><p><img src="/Machine-Learning-00-Basics/table.png" alt></p><p>从表格也可以看出来，对某一种算法，并非所有的评估函数都能够使用，有些算法是绑定了评估函数的。</p><p>同时，评估函数与优化函数存在对应关系，选择某一类评估函数时，对应的优化策略也就决定了。</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降（Gradient Descent）是在通常模型中通用的迭代型参数估计方法。</p><p>我们可以认为，目标函数是一个自变量为模型参数的函数，而我们希望达到它的最大/最小值。</p><p>我们知道，一个函数在最大或者最小值的位置，它的一阶梯度为全0的向量。至于它究竟是最大值还是最小值，得看其二阶导数，或者进行测试局部变化来验证。</p><p>当然，通常我们会希望目标函数是纯凸/纯凹的，因为这样它的驻点（极大极小值点）只有一个，一旦找到极值点就能够确定它是最优的。在这一理论的驱动下，诞生了凸优化这一学科，目标就是把各种非凸问题转化成凸的问题。</p><p>因此我们会对它进行求导，寻找一阶导数为0的点对应的自变量，也就是模型参数的值。</p><p>此时，我们可以直接利用等式的梯度为0求解出参数，也可以采用迭代求解的梯度下降方法。</p><p>前者看起来不是更直接嘛？但是事实上我们大多采用的是后者。理由就是，第一个方法的实质是计算方程组的解，涉及求逆矩阵的过程，但是一来计算量大，二来难以保证矩阵非奇异或者非病态的情况下，计算过程对方程组值的扰动非常敏感，噪声带来的误差较大导致结果偏离理论解。</p><p>那么，后者是如何操作的？</p><p>在每次训练时，减去梯度值和学习率的乘积。对于一个局部凸的部分，我们可以看到在减去梯度之后我们的参数坐标会向极值点（最低点）靠近，且梯度绝对值越大，下降越快。</p><p>理论依据：梯度的反方向就是函数局部值下降最快的方向。</p><p><img src="/Machine-Learning-00-Basics/grad.png" alt></p><p>为了快速收敛、避免震荡的目的，也出现了很多学习率优化算法，如自适应性优化（Adam）、Adagrad和随机梯度下降（SGD）、Momentum等策略，这一块暂时不做介绍。</p><h3 id="泛化-、方差、偏差和噪声"><a href="#泛化-、方差、偏差和噪声" class="headerlink" title="泛化 、方差、偏差和噪声"></a>泛化 、方差、偏差和噪声</h3><p>首先，泛化（Generalization）是指模型在经过一定的数据训练之后对现实数据进行测试，我们希望模型能够最小化测试误差（testing error），而训练数据集上的误差与测试数据集上的误差就是泛化误差（Generalization Error）。</p><p>而泛化误差分为方差（Variance）与偏差（Bias），通过这两个方面可以描述模型与现实模型的误差：</p><ul><li>Bias是模型预测与真实结果的差距，可以直观理解为训练误差（training error），表现了模型的拟合能力；</li><li>Variance则是“<strong>（大小相同的）不同训练数据集</strong>训练出的模型”的训练误差之间的差异，表现了数据扰动的影响。</li></ul><p>通常来说，模型越复杂（组成模型的参数越多），方差越大，偏差越小，这是因为模型的描述能力越强；模型越简单，偏差也容易大，很可能无法拟合训练数据。</p><p>通常模型复杂程度与方差/偏差的关系：</p><p><img src="/Machine-Learning-00-Basics/complexity.png" alt></p><p>在图中可以看到，随着模型变得复杂，训练误差/偏差变小，而方差（在图中可以看作测试误差与训练误差之间的差值）变大，训练误差在中间有一个较低值。</p><p>由此启发我们寻找一个复杂度的平衡点，使得模型具有较低的bias和variance；至于noise则是无法改变的。</p><p>此外，还有一个概念叫做噪声（noise）：噪声在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p><p>模型的误差主要来自三部分的总和。</p><h3 id="过拟合-欠拟合"><a href="#过拟合-欠拟合" class="headerlink" title="过拟合/欠拟合"></a>过拟合/欠拟合</h3><ul><li><p>欠拟合主要描述的是模型复杂度过低，难以拟合训练数据，此时偏差过大（上图左侧部分）</p></li><li><p>过拟合是指模型过于复杂，虽然在训练数据上能够较好拟合，但是在测试数据上误差极大，此时偏差较小而误差较大（上图右侧部分）</p></li></ul><p>需要注意的是，测试误差较大不能说明是过拟合还是欠拟合；需要看训练误差的大小以区分。</p><h2 id="训练技巧"><a href="#训练技巧" class="headerlink" title="训练技巧"></a>训练技巧</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证（Cross Validation）是一种避免过拟合的训练技巧。</p><p>具体思路在于将训练切分，每一次用不同的数据集来训练，优化平均的误差，从而降低不同数据集带来模型性能的变化，达到降低方差的目的。</p><p>主要有两种方法：</p><ul><li>K折验证（K-Fold）：指将数据切分为K份，每一份轮流作为验证集（Validation Set），其他数据作为训练数据，训练K轮次获得训练误差。</li><li>留一验证（Leave-One-Out）：是K=n的K折验证，通过每次取一个样本作为验证集进行交叉验证训练。</li></ul><h3 id="批量梯度下降-随机梯度下降"><a href="#批量梯度下降-随机梯度下降" class="headerlink" title="批量梯度下降/随机梯度下降"></a>批量梯度下降/随机梯度下降</h3><p>这是梯度下降的两种操作方式。</p><ul><li>随机梯度下降（Stochastic Gradient Descent）是指，对每一个训练的样本都计算一次梯度并且用梯度执行更新参数的操作。这种方法的好处是更新次数快，且存在一定的随机性不会陷入局部极小值；但是也因为随机性强，往往梯度的波动大，某一两个样本带来的参数变化太大，更新不稳定，甚至导致不收敛 。</li><li>批量梯度下降（Batch Gradient Descent）是指，每次对所有训练样本进行计算梯度并且只用所有梯度的平均值进行一次更新。这种方法的好处自然就是稳定更新；但是其更新太慢，在一定的时间里难以达到收敛，而且也容易陷入局部最小值，最终在较小的梯度下停止更新。</li></ul><p>一般来说现有的技巧在于折衷两种方案，进行小批量的梯度下降，并且打乱样本以获取随机性。</p><p>这样做的好处在于：</p><ol><li>利用了随机梯度下降的随机性，一般不会陷入局部极小值。</li><li>更新速度适中，保持较好的稳定性不会震荡，同时也能够较快达到收敛。</li><li>最重要的是，方便底层GPU优化。因为梯度计算的底层操作是矩阵运算，而GPU由于多核计算能够并行地计算某一行的计算结果，从而加速梯度更新过程。所以一般而言，小批量梯度下降的效率比随机梯度下降更高。</li></ol><hr><p>本篇完。后续内容将大致按照本篇的顺序，依次介绍主要的几种具体算法以及其他技巧等内容。</p>]]></content>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;本文是”机器学不动了”系列的第一篇内容，可以结合个人实现的代码食用：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Riroaki/LemonML/&quot;&gt;https://github.com/Riroaki/LemonML/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;引子&quot;&gt;&lt;a href=&quot;#引子&quot; class=&quot;headerlink&quot; title=&quot;引子&quot;&gt;&lt;/a&gt;引子&lt;/h
        
      
    
    </summary>
    
      <category term="机器学不动了" scheme="http://riroaki.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B8%8D%E5%8A%A8%E4%BA%86/"/>
    
    
      <category term="Data Mining" scheme="http://riroaki.github.io/tags/Data-Mining/"/>
    
      <category term="Machine Learning" scheme="http://riroaki.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Restart</title>
    <link href="http://riroaki.github.io/Restart/"/>
    <id>http://riroaki.github.io/Restart/</id>
    <published>2019-05-28T01:52:55.000Z</published>
    <updated>2019-06-02T14:33:47.956Z</updated>
    
    <content type="html"><![CDATA[<p>今天把博客文章全都清空了。</p><p>主要是之前的文章太乱，缺乏整理；加上近期学了很多东西之后，回头看过去的内容觉得有些浅薄，决心从头开始写。</p><p>今后会在这里写一些机器学习，以及数据处理的东西。</p><p>当然还有一些工程向的内容，总之我会更加深思熟虑地推送文章。</p><p>（是不是也考虑一下换主题呢……哈哈还是算了估计又要挑很久）</p>]]></content>
    
    <summary type="html">
    
      
      
        
        
          &lt;p&gt;今天把博客文章全都清空了。&lt;/p&gt;
&lt;p&gt;主要是之前的文章太乱，缺乏整理；加上近期学了很多东西之后，回头看过去的内容觉得有些浅薄，决心从头开始写。&lt;/p&gt;
&lt;p&gt;今后会在这里写一些机器学习，以及数据处理的东西。&lt;/p&gt;
&lt;p&gt;当然还有一些工程向的内容，总之我会更加深思熟虑地推送文章
        
      
    
    </summary>
    
    
      <category term="Hello, world!" scheme="http://riroaki.github.io/tags/Hello-world/"/>
    
  </entry>
  
</feed>
