<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Riroaki</title>
  
  <subtitle>Riroaki&#39;s home</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://riroaki.github.io/"/>
  <updated>2019-04-22T03:40:16.191Z</updated>
  <id>http://riroaki.github.io/</id>
  
  <author>
    <name>Riroaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ML学习日记7——自己实现一个神经网络</title>
    <link href="http://riroaki.github.io/2019/04/22/ML-7-My-Neuron-Network-Model/"/>
    <id>http://riroaki.github.io/2019/04/22/ML-7-My-Neuron-Network-Model/</id>
    <published>2019-04-22T02:49:54.000Z</published>
    <updated>2019-04-22T03:40:16.191Z</updated>
    
    <content type="html"><![CDATA[<p>这两天受到cs224n启发，决定仿照keras的api自己实现一个最简单的全连接神经网络。</p><p>支持特性：</p><ul><li>多种激活函数（sigmoid, tanh, relu, softmax）</li><li>多种损失函数（cross entropy, mean square error, mean absolute error）</li><li>小批量梯度下降</li></ul><h3 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h3><p>首先，按照基本定义将设计划分成3个类：Neuron，Layer以及Model，层层递进。</p><h4 id="神经元Neuron"><a href="#神经元Neuron" class="headerlink" title="神经元Neuron"></a>神经元Neuron</h4><ul><li><h4 id="层Layer"><a href="#层Layer" class="headerlink" title="层Layer"></a>层Layer</h4></li><li></li></ul><h4 id="模型Model"><a href="#模型Model" class="headerlink" title="模型Model"></a>模型Model</h4><p>- </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这两天受到cs224n启发，决定仿照keras的api自己实现一个最简单的全连接神经网络。&lt;/p&gt;
&lt;p&gt;支持特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;多种激活函数（sigmoid, tanh, relu, softmax）&lt;/li&gt;
&lt;li&gt;多种损失函数（cross entrop
      
    
    </summary>
    
      <category term="machine learning" scheme="http://riroaki.github.io/categories/machine-learning/"/>
    
    
      <category term="deep learning" scheme="http://riroaki.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>计算机网络要点整理</title>
    <link href="http://riroaki.github.io/2019/04/22/Network/"/>
    <id>http://riroaki.github.io/2019/04/22/Network/</id>
    <published>2019-04-22T02:49:01.000Z</published>
    <updated>2019-04-22T04:42:20.465Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4层模型-7层模型"><a href="#4层模型-7层模型" class="headerlink" title="4层模型/7层模型"></a>4层模型/7层模型</h3><p>……</p><h3 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h3><ul><li>get和post区别：内容写在请求头和请求主体；post更安全，可以避免跨站脚本攻击（CSRF）<ul><li>一般使用Token实现防御，就是关键请求需要使用特定的一次性加密Token标记。</li></ul></li><li>HEAD和BODY（可选，可以没有body）之间的区分为两个\r\n；可以包含多个HEAD，用一个\r\n分割。</li><li>Content-Length限制长度，或者是Transfer-Encoding: Chunked表示分块传输编码，每个块前标示块大小；而总长度未知，最后一个块长度是0，所以遇到0表示内容结束；块之间是两个\r\n。</li></ul><h4 id="MIME-TYPE"><a href="#MIME-TYPE" class="headerlink" title="MIME-TYPE"></a>MIME-TYPE</h4><pre class="line-numbers language-lang-json"><code class="language-lang-json">text/plaintext/htmltext/cssimage/jpegimage/pngimage/svg+xmlaudio/mp4video/mp4application/javascriptapplication/pdfapplication/zipapplication/atom+xml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="HTTP1-0"><a href="#HTTP1-0" class="headerlink" title="HTTP1.0"></a>HTTP1.0</h4><ul><li>无状态短连接发完就关闭</li><li>声明Connection: keep-alive表示长连接。</li></ul><h4 id="HTTP1-1（主流）"><a href="#HTTP1-1（主流）" class="headerlink" title="HTTP1.1（主流）"></a>HTTP1.1（主流）</h4><ul><li>HEAD编码用ASCII，BODY可以是ASCII也可以是二进制（图片视频等等）</li><li>长连接（主要是允许复用），最后发送长度为0表示内容结束才会关闭连接。缺点是队头堵塞，需要服务器完全接受才能继续。</li></ul><h4 id="HTTP2-0（2015推出）"><a href="#HTTP2-0（2015推出）" class="headerlink" title="HTTP2.0（2015推出）"></a>HTTP2.0（2015推出）</h4><ul><li>多工通信，非阻塞。</li><li>HEAD加入信息压缩（Content-Encoding表示内容编码，一般是gzip）。</li><li>由于HTTP本质还是没有状态的，每次请求都会发送状态信息，所以修改成服务器和客户端共同维护cookies和user agent等重复字段的表，不需要每次都传相同信息，节约通信成本。</li><li>HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。这一来就可以在空闲的时候预先把可能未来会需要的资源先发送到客户端，提高用户体验。</li></ul><h4 id="HTTPS：HTTP-over-TLS，是一种在加密信道进行-HTTP-内容传输的协议。"><a href="#HTTPS：HTTP-over-TLS，是一种在加密信道进行-HTTP-内容传输的协议。" class="headerlink" title="HTTPS：HTTP over TLS，是一种在加密信道进行 HTTP 内容传输的协议。"></a>HTTPS：HTTP over TLS，是一种在加密信道进行 HTTP 内容传输的协议。</h4><ul><li><p>TLS证书机制，CA信任链，根证书为最安全的</p></li><li><p>从上面的过程可以看到，TLS 的完整过程需要三个算法（协议），密钥交互算法，对称加密算法，和消息认证算法（TLS 的传输会使用 MAC(message authentication code) 进行完整性检查）。</p><p>我们以 Github 网站使用的 TLS 为例，使用浏览器可以看到它使用的加密为 <code>TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256</code>。其中密钥交互算法是 <code>ECDHE_RSA</code>，对称加密算法是 <code>AES_128_GCM</code>，消息认证（MAC）算法为 <code>SHA256</code>。</p></li><li><p>HSTS：强制浏览器使用HTTPS协议</p></li><li><p>中间人攻击、伪造证书攻击</p></li></ul><h3 id="Socket与TCP、UDP"><a href="#Socket与TCP、UDP" class="headerlink" title="Socket与TCP、UDP"></a>Socket与TCP、UDP</h3><h4 id="Socket编程："><a href="#Socket编程：" class="headerlink" title="Socket编程："></a>Socket编程：</h4><ul><li>服务原语作为函数名，不追究底层实现：<ul><li>socket()创建套接字</li><li>bind()分配套接字地址</li><li>listen()等待连接请求</li><li>accept()允许连接请求</li><li>read()/write()数据交换</li><li>close()关闭连接</li></ul></li><li>服务器在不断等待连接请求，如果遇到接入，那么就开一个线程处理通信；</li><li>键盘输入可以中断，pthread_cancel，取消点机制可以在循环的accept阶段（阻塞式IO）终止。</li></ul><h4 id="TCP三次握手"><a href="#TCP三次握手" class="headerlink" title="TCP三次握手"></a>TCP三次握手</h4><ul><li><p>第一次握手(SYN=1, seq=x):</p><p>客户端发送一个 TCP 的 SYN 标志位置1的包，指明客户端打算连接的服务器的端口，以及初始序号 X,保存在包头的序列号(Sequence Number)字段里。</p><p>发送完毕后，客户端进入 <code>SYN_SEND</code> 状态。</p></li><li><p>第二次握手(SYN=1, ACK=1, seq=y, ACKnum=x+1):</p><p>服务器发回确认包(ACK)应答。即 SYN 标志位和 ACK 标志位均为1。服务器端选择自己 ISN 序列号，放到 Seq 域里，同时将确认序号(Acknowledgement Number)设置为客户的 ISN 加1，即X+1。 发送完毕后，服务器端进入 <code>SYN_RCVD</code> 状态。</p></li><li><p>第三次握手(ACK=1，ACKnum=y+1)</p><p>客户端再次发送确认包(ACK)，SYN 标志位为0，ACK 标志位为1，并且把服务器发来 ACK 的序号字段+1，放在确定字段中发送给对方，并且在数据段放写ISN的+1</p><p>发送完毕后，客户端进入 <code>ESTABLISHED</code> 状态，当服务器端接收到这个包时，也进入 <code>ESTABLISHED</code>状态，TCP 握手结束。</p></li></ul><h4 id="TCP四次挥手"><a href="#TCP四次挥手" class="headerlink" title="TCP四次挥手"></a>TCP四次挥手</h4><ul><li><p>第一次挥手(FIN=1，seq=x)</p><p>假设客户端想要关闭连接，客户端发送一个 FIN 标志位置为1的包，表示自己已经没有数据可以发送了，但是仍然可以接受数据。</p><p>发送完毕后，客户端进入 <code>FIN_WAIT_1</code> 状态。</p></li><li><p>第二次挥手(ACK=1，ACKnum=x+1)</p><p>服务器端确认客户端的 FIN 包，发送一个确认包，表明自己接受到了客户端关闭连接的请求，但还没有准备好关闭连接。</p><p>发送完毕后，服务器端进入 <code>CLOSE_WAIT</code> 状态，客户端接收到这个确认包之后，进入 <code>FIN_WAIT_2</code> 状态，等待服务器端关闭连接。</p></li><li><p>第三次挥手(FIN=1，seq=y)</p><p>服务器端准备好关闭连接时，向客户端发送结束连接请求，FIN 置为1。</p><p>发送完毕后，服务器端进入 <code>LAST_ACK</code> 状态，等待来自客户端的最后一个ACK。</p></li><li><p>第四次挥手(ACK=1，ACKnum=y+1)</p><p>客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入 <code>TIME_WAIT</code>状态，等待可能出现的要求重传的 ACK 包。</p><p>服务器端接收到这个确认包之后，关闭连接，进入 <code>CLOSED</code> 状态。</p><p>客户端等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后，没有收到服务器端的 ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入 <code>CLOSED</code> 状态。</p></li></ul><h4 id="SYN攻击"><a href="#SYN攻击" class="headerlink" title="SYN攻击"></a>SYN攻击</h4><ul><li>在三次握手过程中，服务器发送 SYN-ACK 之后，收到客户端的 ACK 之前的 TCP 连接称为半连接(half-open connect)。此时服务器处于 SYN_RCVD 状态。当收到 ACK 后，服务器才能转入 ESTABLISHED 状态.</li><li>SYN 攻击指的是，攻击客户端在短时间内伪造大量不存在的IP地址，向服务器不断地发送SYN包，服务器回复确认包，并等待客户的确认。由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列，正常的SYN请求被丢弃，导致目标系统运行缓慢，严重者会引起网络堵塞甚至系统瘫痪。</li><li>SYN 攻击是一种典型的 DoS/DDoS 攻击。</li></ul><h4 id="TCP心跳检测（Keep-Alive）"><a href="#TCP心跳检测（Keep-Alive）" class="headerlink" title="TCP心跳检测（Keep Alive）"></a>TCP心跳检测（Keep Alive）</h4><ul><li>在交互过程中 ，双方都有可能因为物理原因或者故障而掉线，这个时候需要检测对方是否还在不然无法close释放进程。</li><li>TCP KeepAlive 的基本原理是，隔一段时间给连接对端发送一个探测包，如果收到对方回应的 ACK，则认为连接还是存活的，在超过一定重试次数之后还是没有收到对方的回应，则丢弃该 TCP 连接。</li></ul><h4 id="UDP协议"><a href="#UDP协议" class="headerlink" title="UDP协议"></a>UDP协议</h4><ul><li>只管发不管收，直播流等数据些微损失影响不大，传输速率比较重要的场景。</li><li>无连接，不必握手，直接发就可以；支持广播和多播。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;4层模型-7层模型&quot;&gt;&lt;a href=&quot;#4层模型-7层模型&quot; class=&quot;headerlink&quot; title=&quot;4层模型/7层模型&quot;&gt;&lt;/a&gt;4层模型/7层模型&lt;/h3&gt;&lt;p&gt;……&lt;/p&gt;
&lt;h3 id=&quot;HTTP协议&quot;&gt;&lt;a href=&quot;#HTTP协议&quot; c
      
    
    </summary>
    
      <category term="ingterview" scheme="http://riroaki.github.io/categories/ingterview/"/>
    
    
      <category term="network" scheme="http://riroaki.github.io/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>数据库知识要点整理</title>
    <link href="http://riroaki.github.io/2019/04/22/Database/"/>
    <id>http://riroaki.github.io/2019/04/22/Database/</id>
    <published>2019-04-22T02:48:50.000Z</published>
    <updated>2019-04-22T04:38:53.749Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据库管理系统原则——ACID"><a href="#数据库管理系统原则——ACID" class="headerlink" title="数据库管理系统原则——ACID"></a>数据库管理系统原则——ACID</h3><p><strong>ACID</strong>，是指<a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener">数据库管理系统</a>（<a href="https://zh.wikipedia.org/wiki/DBMS" target="_blank" rel="noopener">DBMS</a>）在写入或更新资料的过程中，为保证<a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1" target="_blank" rel="noopener">事务</a>（transaction）是正确可靠的，所必须具备的四个特性：<a href="https://zh.wikipedia.org/w/index.php?title=%E5%8E%9F%E5%AD%90%E6%80%A7&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">原子性</a>（atomicity，或称不可分割性）、<a href="https://zh.wikipedia.org/w/index.php?title=%E4%B8%80%E8%87%B4%E6%80%A7_(%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F" target="_blank" rel="noopener">一致性</a>&amp;action=edit&amp;redlink=1)（consistency）、<a href="https://zh.wikipedia.org/wiki/%E9%9A%94%E9%9B%A2%E6%80%A7" target="_blank" rel="noopener">隔离性</a>（isolation，又称独立性）、<a href="https://zh.wikipedia.org/w/index.php?title=%E6%8C%81%E4%B9%85%E6%80%A7&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">持久性</a>（durability）。</p><h4 id="Atomicity（原子性）："><a href="#Atomicity（原子性）：" class="headerlink" title="Atomicity（原子性）："></a>Atomicity（原子性）：</h4><p>一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被<a href="https://zh.wikipedia.org/wiki/%E5%9B%9E%E6%BB%9A_(%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86" target="_blank" rel="noopener">回滚</a>)（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。</p><h4 id="Consistency（一致性）："><a href="#Consistency（一致性）：" class="headerlink" title="Consistency（一致性）："></a>Consistency（一致性）：</h4><p>在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设<a href="https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E6%80%A7" target="_blank" rel="noopener">约束</a>、<a href="https://zh.wikipedia.org/wiki/%E8%A7%A6%E5%8F%91%E5%99%A8_(%E6%95%B0%E6%8D%AE%E5%BA%93" target="_blank" rel="noopener">触发器</a>)、<a href="https://zh.wikipedia.org/w/index.php?title=%E7%BA%A7%E8%81%94%E5%9B%9E%E6%BB%9A&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">级联回滚</a>等。</p><p>事务是维护数据一致性的基本单位。要么全部执行，要么全都不执行。</p><h4 id="Isolation（隔离性）："><a href="#Isolation（隔离性）：" class="headerlink" title="Isolation（隔离性）："></a>Isolation（隔离性）：</h4><p>数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。</p><h4 id="Durability（持久性）："><a href="#Durability（持久性）：" class="headerlink" title="Durability（持久性）："></a>Durability（持久性）：</h4><p>事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。（结果一定会写入内存）</p><h3 id="数据库表设计原则"><a href="#数据库表设计原则" class="headerlink" title="数据库表设计原则"></a>数据库表设计原则</h3><ul><li>明确实体和表的对应关系：一对一还是一对多，还是多对多</li><li>对第一、第三、第三范式（三个范式不能生搬硬套，因为适当降低标准可以提高性能）：<ul><li>第一范式：1NF是对属性的原子性约束，要求<strong>属性具有原子性，不可再分解</strong>；</li><li>第二范式：2NF是对记录的惟一性约束，要求<strong>记录有惟一标识，即实体的惟一性</strong>；</li><li>第三范式：3NF是对字段冗余性的约束，即任何字段不能由其他字段派生出来，它要求字段没有冗余。 <ul><li>实际操作我们应当如何减少数据冗余：<ul><li>冗余本质是用空间换时间，比如极端情况我把两张表合并到一起，就可以操作更简单，提高性能；但是代价就是存储开销比较大，完全没有必要这么做。</li><li><strong>支持派生冗余，反对重复冗余</strong>。派生冗余就是比如单价和数量属性计算得到总金额，这个是常用的数据所以引入可以提高性能；重复就是两个表没有必要的重复性。</li></ul></li></ul></li></ul></li><li>完整性约束：<ul><li>最基本的主键不能缺失</li><li>外键定义约束表不发生冲突</li></ul></li><li>一些经验，比如：<ul><li>表较少、字段较少、组成主键的子段少，性能高。</li><li>复杂计算在查询外执行，不要在数据库做。</li><li>表的内容过多就应该分割一些，做子表。</li></ul></li></ul><h3 id="查询优化"><a href="#查询优化" class="headerlink" title="查询优化"></a>查询优化</h3><h4 id="数据库内部实现："><a href="#数据库内部实现：" class="headerlink" title="数据库内部实现："></a>数据库内部实现：</h4><ul><li>对常用结果进行缓存；命中缓存则不需要进行下一步操作。</li><li>解析sql语句之后进行语法树的优化：尝试预测不同方案的查询代价，选择代价最小的那一个查询计划。</li></ul><h4 id="表设计："><a href="#表设计：" class="headerlink" title="表设计："></a>表设计：</h4><ul><li>数据类型优化，单张表的属性少一点之类的，不要用枚举什么的；固定字段长度比可变字段长度效率高。</li><li>建立索引，提高搜索效率</li></ul><h4 id="写查询语句："><a href="#写查询语句：" class="headerlink" title="写查询语句："></a>写查询语句：</h4><ul><li>写的时候估算一下？</li><li>有索引的时候尽量使用</li><li>避免<code>select *</code>，用到什么就取什么</li></ul><h3 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><ul><li>使用B树或者B+树实现，指向记录的物理地址的位置。<ul><li>两种树的区别：<ul><li>B树是二叉树的直接变体，节点存数据指针和下一层的指针；</li><li>B+树的非叶子结点不存数据指针，所以每个节点能够存放的指针更多，更扁平</li></ul></li><li>优缺点：<ul><li>B+树查询速度稳定，因为都要查到叶子结点；</li><li>B+树更方便排序，因为叶子结点是链表串；</li><li>使用B+树做全表遍历更快，因为只涉及最后一层（当然，有索引你还全表遍历，不是直接遍历表就可以了嘛——除非有排序的需求）；</li><li>如果访问数据离根节点快，那么B树更好。</li></ul></li></ul></li><li>通常把节点加载到内存之后，根据查找得到的位置去访问物理空间。</li><li>between范围查询、like的右模糊查询会使用索引，而两侧模糊查询不会</li></ul><h4 id="建立索引原则"><a href="#建立索引原则" class="headerlink" title="建立索引原则"></a>建立索引原则</h4><ul><li>尽量选择主键这种区分度高的字段建立索引</li><li>需要经常范围搜索或者排序的位置，则建立索引</li><li>修改次数远大于检索次数的时候，建索引不好</li><li>如果使用的是多键索引，进行从左到右的匹配，“最左匹配特性”</li></ul><h4 id="索引的坏处"><a href="#索引的坏处" class="headerlink" title="索引的坏处"></a>索引的坏处</h4><ul><li>建立索引的时候内存开销大，容易宕机</li><li>物理空间占用较大，应该适当删除</li><li>修改数据的时候需要较高的维护成本</li></ul><h4 id="有了索引但是查询并不变快的场景"><a href="#有了索引但是查询并不变快的场景" class="headerlink" title="有了索引但是查询并不变快的场景"></a>有了索引但是查询并不变快的场景</h4><p>可能没有走索引。当数据库评估代价的时候，索引并不能帮助提高效率（索引基数较低的情况等等），那它还不如直接遍历全表。</p><h4 id="联合索引"><a href="#联合索引" class="headerlink" title="联合索引"></a>联合索引</h4><h5 id="最左匹配原则"><a href="#最左匹配原则" class="headerlink" title="最左匹配原则"></a>最左匹配原则</h5><ul><li>在mysql建立联合索引时会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配，示例：对列col1、列col2和列col3建一个联合索引<code>(col1, col2)</code>，对查询语句会按照索引的顺序来检查，首先看<code>col1</code>的限制条件，再看<code>col2</code>。</li><li><p>所以在写查询的时候不必在意列的查询顺序。比如，<code>where col1 = &#39;1&#39; and col2 = &#39;2&#39;</code>和<code>where col2 = &#39;2&#39; and col1 = &#39;1&#39;</code>这两个查询语句都会用到索引(col1,col2)，mysql创建联合索引的规则是首先会对联合合索引的最左边的，也就是第一个字段col1的数据进行排序，在第一个字段的排序基础上，然后再对后面第二个字段col2进行排序。其实就相当于实现了类似 order by col1 col2这样一种排序规则。</p></li><li><p>借助mysql查询优化器explain，explain会纠正sql语句该以什么样的顺序执行效率最高，最后才生成真正的执行计划。</p><ul><li><p>具体：<code>EXPLAIN SELECT * FROM test WHERE col2=2\G</code>（\G是为了以列的形式显示出来，不然行来看的话会很长，展不开）</p></li><li><p>例如，我在自己之前用来做你画我猜的数据库测试了一下：</p></li><li><pre class="line-numbers language-lang-mysql"><code class="language-lang-mysql">mysql> explain select * from topics\G*************************** 1. row ***************************           id: 1  select_type: SIMPLE        table: topics   partitions: NULL         type: ALLpossible_keys: NULL          key: NULL      key_len: NULL          ref: NULL         rows: 1     filtered: 100.00        Extra: NULL1 row in set, 1 warning (0.01 sec)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>type字段：ALL全表扫描,通常是不好的，其他的如index、range、const、ref、system则是较好的。</p></li><li><p>possible_keys：可能被用到的索引。</p></li><li><p>key_len：索引基数，也就是不同的键值一共有几种。</p><ul><li><blockquote><p>索引的基数相对于数据表行数较高（也就是说，列中包含很多不同的值，重复的值很少）的时候，它的工作效果最好。如果某数据列含有很多不同的年龄，索引会很快地分辨数据行。如果某个数据列用于记录性别（只有”M”和”F”两种值），那么索引的用处就不大。如果值出现的几率几乎相等，那么无论搜索哪个值都可能得到一半的数据行。在这些情况下，最好根本不要使用索引，因为查询优化器发现某个值出现在表的数据行中的百分比很高的时候，它一般会忽略索引，进行全表扫描。惯用的百分比界线是”30%”。（匹配的数据量超过一定限制的时候查询器会放弃使用索引。尽量不使用会导致索引失效的条件，比如in，用exists子查询代替，或者in的条件少时可以用union all来代替）。</p></blockquote></li></ul></li><li><p>key：按照现在的语句，实际搜索用到的索引，如果是NULL表示没有用到索引。</p></li><li><p>rows：估计要扫描的行数。</p></li></ul></li></ul><h5 id="为什么要使用联合索引？"><a href="#为什么要使用联合索引？" class="headerlink" title="为什么要使用联合索引？"></a>为什么要使用联合索引？</h5><ul><li>减少开销。建一个联合索引(col1,col2,col3)，实际相当于建了(col1),(col1,col2),(col1,col2,col3)三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。对于大量数据的表，使用联合索引会大大的减少开销！</li><li>覆盖索引。对联合索引(col1,col2,col3)，如果有如下的sql: select col1,col2,col3 from test where col1=1 and col2=2。那么MySQL可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机io操作。减少io操作，特别的随机io其实是dba主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。</li><li>效率高。索引列越多，通过索引筛选出的数据越少。有1000W条数据的表，有如下sql:select <em>from table where col1=1 and col2=2 and col3=3,假设假设每个条件可以筛选出10%的数据，如果只有单值索引，那么通过该索引能筛选出1000W</em>10%=100w条数据，然后再回表从100w条数据中找到符合col2=2 and col3= 3的数据，然后再排序，再分页；如果是联合索引，通过索引筛选出1000w<em>10%</em> 10% *10%=1w，效率提升可想而知！</li></ul><h3 id="其他散的知识点"><a href="#其他散的知识点" class="headerlink" title="其他散的知识点"></a>其他散的知识点</h3><h4 id="悲观锁和乐观锁"><a href="#悲观锁和乐观锁" class="headerlink" title="悲观锁和乐观锁"></a>悲观锁和乐观锁</h4><p>乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。</p><p>乐观锁用于多读——冲突较少，悲观锁用于多写——冲突较多。</p><ul><li>悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作<ul><li>synchronized的底层实现主要依靠 <strong>Lock-Free</strong> 的队列，基本思路是 <strong>自旋后阻塞</strong>，<strong>竞争切换后继续竞争锁</strong>，<strong>稍微牺牲了公平性，但获得了高吞吐量</strong>。在线程冲突较少的情况下，可以获得和CAS类似的性能；而线程冲突严重的情况下，性能远高于CAS。</li></ul></li><li>乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性<ul><li>CAS（将比较并修改值作为原子过程，无锁编程，自旋操作），缺点在于值一样的时候无法检查是否有更新过；自旋机制反复比较开销大。适用于较轻、竞争少的情况。</li><li>版本号机制（提交时比较读时版本号与当前版本号区别，提交版本号必须比记录版本号大才允许更新）</li></ul></li></ul><h4 id="drop、delete与truncate分别在什么场景之下使用？"><a href="#drop、delete与truncate分别在什么场景之下使用？" class="headerlink" title="drop、delete与truncate分别在什么场景之下使用？"></a><strong>drop、delete</strong>与<strong>truncate</strong>分别在什么场景之下使用？</h4><ul><li>不再需要一张表的时候，用<strong>drop</strong></li><li>想删除部分数据行时候，用<strong>delete</strong>，并且带上where子句</li><li>保留表而删除所有数据的时候用<strong>truncate</strong></li></ul><h4 id="什么是视图？以及视图的使用场景有哪些？"><a href="#什么是视图？以及视图的使用场景有哪些？" class="headerlink" title="什么是视图？以及视图的使用场景有哪些？"></a>什么是视图？以及视图的使用场景有哪些？</h4><p>视图是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改不影响基本表。它使得我们获取数据更容易，相比多表查询。</p><ul><li>只暴露部分字段给访问者，所以就建一个虚表，就是视图。</li><li>查询的数据来源于不同的表，而查询者希望以统一的方式查询，这样也可以建立一个视图，把多个表查询结果联合起来，查询者只需要直接从视图中获取数据，不必考虑数据来源于不同表所带来的差异</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;数据库管理系统原则——ACID&quot;&gt;&lt;a href=&quot;#数据库管理系统原则——ACID&quot; class=&quot;headerlink&quot; title=&quot;数据库管理系统原则——ACID&quot;&gt;&lt;/a&gt;数据库管理系统原则——ACID&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;ACID&lt;/stro
      
    
    </summary>
    
      <category term="interview" scheme="http://riroaki.github.io/categories/interview/"/>
    
    
      <category term="database" scheme="http://riroaki.github.io/tags/database/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习日记2——Skip-Gram模型</title>
    <link href="http://riroaki.github.io/2019/04/19/NLP-2-Skip-Gram/"/>
    <id>http://riroaki.github.io/2019/04/19/NLP-2-Skip-Gram/</id>
    <published>2019-04-19T13:20:59.000Z</published>
    <updated>2019-04-20T16:24:18.606Z</updated>
    
    <content type="html"><![CDATA[<p>在第一篇日记中提到的<code>Skip-gram</code>模型，它来自<code>word2vec</code>模型，这里补充它的说明。</p><h3 id="前置概念介绍"><a href="#前置概念介绍" class="headerlink" title="前置概念介绍"></a>前置概念介绍</h3><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4><p>考虑一下这件事：有些单词之间长得很像，意思和用法也很像；有些单词长得不像，但是意思差不多，比如dog和puppy，kitty和cat，或者说存在比较强的联系。</p><p>另外我们需要表示两个单词的距离，比如上文的dog和puppy之间的关系，cat和kitty的关系其实是一致的——一个是狗和小狗，另一个是猫和小猫。可以说他们彼此的距离是差不多的。但是这样的概念对于非结构化的文本来说，要怎么表示？</p><p>这个时候就需要词向量。我们知道，两个向量之间的距离是可以用公式求出来的（欧式距离、切比雪夫距离、曼哈顿距离等等）。假如每个单词使用一定的特征，如词性、词义等等表示，然后向量之间的距离就能够表示单词之间“语义”的差距的话，岂不是美滋滋？</p><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>这个模型是谷歌搞出来训练词向量的模型集合。主要包括有两种模型：</p><ul><li><p>Skip-Gram模型主要用于给定input词语，预测上下文词语。</p></li><li><p>CBOW模型主要用于给定上下文填空。哈哈做出一个完形填空的算法，从此英语高考再也不用担心啦！</p></li></ul><p>⚠️需要注意的是，因为我们的目的在于获得词向量，所以最终得到的结果并不是这个用于输出的模型，而是这整个语料库的词向量表。</p><h4 id="one-hot编码"><a href="#one-hot编码" class="headerlink" title="one-hot编码"></a>one-hot编码</h4><p>算法需要结构化的数值而非一个字符串。所以我们把单词通过one-hot编码转化为一个初始向量。</p><p>具体来说非常简单粗暴：</p><p>比如，基于“The dog barked at the mailman”可以构建一个大小为5的词汇表（忽略大小写和标点符号）：(“the”, “dog”, “barked”, “at”, “mailman”)，我们对这个词汇表的单词进行编号0-4。那么”dog“就可以被表示为一个5维向量[0, 1, 0, 0, 0]。</p><h3 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h3><p>终于到今天的正文部分了。</p><p>为了训练，我们需要把单词和它的上下文单词联系起来。</p><ul><li>我们设定两个参数，<code>skip_window</code>表示窗口大小，即从单词左侧和右侧分别取一定的个数个单词；<code>num_skips</code>表示选取几个单词作为输出。</li><li>比如：对一句话”<strong>The dog barked at the mailman</strong>“。在这个例子中，如果两个参数都是2，那么用于训练的是两组数据：<strong>(‘dog’, ‘barked’)，(‘dog’, ‘the’)</strong>。</li><li>下图是另一个例子，<code>skip_window=2,num_skips=2</code>：</li></ul><p><img src="/2019/04/19/NLP-2-Skip-Gram/skip_gram.png" alt></p><h4 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h4><p>首先贴出模型的结构图：</p><p><img src="/2019/04/19/NLP-2-Skip-Gram/skip-gram1.png" alt></p><p>假设语料库的单词集合大小为n。</p><p>首先，模型的输入就是一个<code>one-hot</code>编码后的单词，是一个n维的向量。</p><p>然后，模型的输出可以使用一个n维的向量表示单词表的概率分布。当然，为了保证最后所有单词概率和为1，我们需要加一层<code>softmax</code>函数，把结果指数归一化。</p><p>需要关注的是中间的隐层：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在第一篇日记中提到的&lt;code&gt;Skip-gram&lt;/code&gt;模型，它来自&lt;code&gt;word2vec&lt;/code&gt;模型，这里补充它的说明。&lt;/p&gt;
&lt;h3 id=&quot;前置概念介绍&quot;&gt;&lt;a href=&quot;#前置概念介绍&quot; class=&quot;headerlink&quot; title=&quot;前
      
    
    </summary>
    
      <category term="machine learning" scheme="http://riroaki.github.io/categories/machine-learning/"/>
    
    
      <category term="NLP" scheme="http://riroaki.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>test</title>
    <link href="http://riroaki.github.io/2019/04/19/test/"/>
    <id>http://riroaki.github.io/2019/04/19/test/</id>
    <published>2019-04-19T12:34:28.000Z</published>
    <updated>2019-04-20T10:33:16.004Z</updated>
    
    <content type="html"><![CDATA[<p>SetExpan: Corpus-Based Set Expansion via<br>Context Feature Selection and Rank Ensemble<br>Jiaming Shen?<br>, Zeqiu Wu?<br>, Dongming Lei, Jingbo Shang, Xiang Ren, Jiawei Han<br>Department of Computer Science, University of Illinois at Urbana-Champaign, USA<br>{js2, zeqiuwu1, dlei5, shang7, xren7, hanj}@illinois.edu<br>Abstract. Corpus-based set expansion (i.e., finding the “complete” set<br>of entities belonging to the same semantic class, based on a given corpus<br>and a tiny set of seeds) is a critical task in knowledge discovery. It<br>may facilitate numerous downstream applications, such as information<br>extraction, taxonomy induction, question answering, and web search.<br>To discover new entities in an expanded set, previous approaches either<br>make one-time entity ranking based on distributional similarity, or resort<br>to iterative pattern-based bootstrapping. The core challenge for these<br>methods is how to deal with noisy context features derived from free-text<br>corpora, which may lead to entity intrusion and semantic drifting. In<br>this study, we propose a novel framework, SetExpan, which tackles this<br>problem, with two techniques: (1) a context feature selection method that<br>selects clean context features for calculating entity-entity distributional<br>similarity, and (2) a ranking-based unsupervised ensemble method for<br>expanding entity set based on denoised context features. Experiments on<br>three datasets show that SetExpan is robust and outperforms previous<br>state-of-the-art methods in terms of mean average precision.<br>Keywords: Set Expansion, Information Extraction, Bootstrapping, Unsupervised Ranking-Based Ensemble<br>1 Introduction<br>Set expansion refers to the problem of expanding a small set of seed entities<br>into a complete set of entities that belong to the same semantic class [29]. For<br>example, if a given seed set is {Oregon, Texas, Iowa}, set expansion should return<br>a hopefully complete set of entities in the same semantic class, “U.S. states”.<br>Set expansion can benefit various downstream applications, such as knowledge<br>extraction [8], taxonomy induction [27], and web search [2].<br>One line of work for solving this task includes Google Set [26], SEAL [29], and<br>Lyretail [2]. In this approach, a query consisting of seed entities is submitted to<br>a search engine to mine top-ranked webpages. While this approach can achieve<br>relatively good quality, the required seed-oriented online data extraction is costly.<br>Therefore, more studies [17][23][10][28][21] are proposed in a corpus-based setting<br>where sets are expanded by offline processing based on a specific corpus.<br>? Equal Contribution<br>2 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han<br>For corpus-based set expansion, there are two general approaches, one-time<br>entity ranking and iterative pattern-based bootstrapping. Based on the assumption<br>that similar entities appear in similar contexts, the first approach [17][23][10]<br>makes a one-time ranking of candidate entities based on their distributional<br>similarity with seed entities. A variety of “contexts” are used, including Web<br>table, Wikipedia list, or just free-text patterns, and entity-entity distributional<br>similarity is calculated based on all context features. However, blindly using all<br>such features can introduce undesired entities into the expanded set because<br>many context features are not representative for defining the target semantic class<br>although they do have connections with some of the seed entities. For example,<br>when expanding the seed set {Oregon, Texas, Iowa}, “located in ” can be a<br>pattern feature (the entity is replaced with a placeholder) strongly connected to<br>all the three seeds. However, it does not clearly convey the semantic meaning of<br>“U.S. states.” and can bring in entities like USA or Ontario when being used to<br>calculate candidate entity’s similarity with seeds. This is entity intrusion error.<br>Another issue with this approach is that it is hard to obtain the full set at once<br>without back and forth refinement. In some sense, iteratively bootstrapped set<br>expansion is a more conservative way and leads to better precision.<br>The second approach, iterative pattern-based bootstrapping [22][8][9], starts<br>from seed entities to extract quality patterns, based on a predefined pattern<br>scoring mechanism, and it then applies extracted patterns to obtain even higher<br>quality entities using another entity scoring method. This process iterates and the<br>high-quality patterns from all previous iterations are accumulated into a pattern<br>pool which will be used for the next round of entity extraction. This approach<br>works only when patterns/entities extracted at each iteration are highly accurate,<br>otherwise, it may cause severe semantic shift problem. Suppose in the previous<br>example, “located in ” is taken as a good pattern from the seed set {Oregon,<br>Texas, Iowa}, and this pattern brings in USA and Ontario. These undesired<br>entities may bring in even lower quality patterns and iteratively cause the set<br>shifting farther away. Thus, the pattern and entity scoring methods are crucial<br>but sensitive in iterative bootstrapping methods. If they are not defined perfectly,<br>the semantic shift can cause big problems. However, it is hard to have a perfect<br>scoring mechanism due to the diversity and noisiness of unstructured text data.<br>This study proposes a new set expansion framework, SetExpan, which addresses both challenges posed above for corpus-based set expansion on free text.<br>It carefully and conservatively extracts each candidate entity and iteratively<br>improves the results. First, to overcome the entity intrusion problem, instead of<br>using all context features, context features are carefully selected by calculating<br>distributional similarity. Second, to overcome the semantic drift problem, different<br>from other bootstrapped approaches, our high-quality feature pool will be reset<br>at the beginning of each iteration. Finally, our carefully designed unsupervised<br>ranking-based ensemble method is used at each iteration to further refine entities<br>and make our system robust to noisy or wrongly extracted pattern features.<br>Figure 1 shows the pipeline at each iteration. SetExpan iteratively expands an<br>entity set through a context feature selection step and an entity selection step. At<br>SetExpan 3<br>Quebec 5 (1/3)<br>Baja California 4 (1/2+1/3)<br>California 3 (1/1)<br>Florida 2 (1/1+1/2)<br>Arizona 1 (1/2+1/3+1/1)<br>Entities Rank (Score)<br>Quebec 3 (1/3)<br>Arizona 2 (1/2)<br>California 1 (1/1)<br>Entities Rank (Score)<br>Arizona 3 (1/3)<br>Baja California 2 (1/2)<br>Florida 1 (1/1)<br>Entities Rank (Score)<br>Baja California 3 (1/3)<br>Florida 2 (1/2)<br>Arizona 1 (1/1)<br>Entities Rank (Score)<br>Pre-ranked entity list 1<br>Pre-ranked entity list 2<br>Pre-ranked entity list 3<br>Final ranked list of entities<br>Denoised context sets<br>City , <strong> , USA<br>US state of </strong> ,<br>Texas and <strong> ,<br>City , </strong> , USA<br>US state of <strong> ,<br>City , </strong> , USA<br>Texas and <strong> ,<br>US state of </strong> ,<br>Texas and <strong> ,<br>the former </strong> governor<br>city , <strong> , USA<br>US state of </strong> .<br>Texas and <strong><br>county , </strong> , on<br>Context features<br>Georgia<br>Illinois<br>Currently<br>expanded<br>entity set<br>Virginia<br>. <br>Context Feature Selection step Entity selection step<br>Rank ensemble<br>Fig. 1: An example showing two steps in one iteration of SetExpan.<br>the context feature selection, each context feature is scored based on its strength<br>with currently expanded entities and top-ranked context features are selected. At<br>the entity selection step, multiple subsets of the selected representative context<br>features are sampled and each subset is used to obtain a ranked entity list.<br>Finally, all the ranked lists are collected to compute the final ranking list of each<br>candidate entity for expansion.<br>The major contributions of this paper are: (1) we propose an iterative set<br>expansion framework with a novel context feature selection approach, to handle<br>the issues of entity intrusion and semantic drift; (2) we develop an unsupervised<br>ranking-based ensemble algorithm for entity selection to make our system robust<br>and further reduce the impact of semantic drift. To evaluate the SetExpan method,<br>we use three publicly available datasets and manually label expanded results of<br>65 queries over 13 semantic classes. Empirical results show that SetExpan outperforms the state-of-the-art baselines in terms of Mean Average Precision. Code1<br>and datasets2 described in this paper are publicly.<br>2 Related Work<br>The problem of completing an entity set given several seed entities has attracted<br>extensive research efforts due to its practical importance. Google Sets [26] was<br>among the earliest work dealing with this problem. It used proprietary algorithms<br>and is no longer publicly accessible. Later, Wang and Cohen proposed SEAL<br>system [29], which first submits a query consisting of all seed entities into a<br>general search engine and then mines the top-ranked webpages. Recently, Chen<br>et al. [2] improved this approach by leveraging a “page-specific” extractor built in<br>a supervised manner and showed good performance on long-tail (i.e., rare) term<br>expansion. All these methods need an external search engine and require seedoriented data extraction. In comparison, our approach conducts corpus-based set<br>expansion without resorting to online data extraction from specific webpages.<br>1<br><a href="https://github.com/mickeystroller/SetExpan" target="_blank" rel="noopener">https://github.com/mickeystroller/SetExpan</a><br>2<br><a href="https://tinyurl.com/SetExpan-data" target="_blank" rel="noopener">https://tinyurl.com/SetExpan-data</a><br>4 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han<br>To tackle the corpus-based set expansion problem, Ghahramani and Heller<br>[6] used a Bayesian method to model the probability that a candidate entity<br>belongs to some unknown cluster that contains the input seeds. Pantel et al.<br>[17] developed a web-scale set expansion pipeline by exploiting distributional<br>similarity on context words for each candidate entity. He et al. proposed the<br>SEISA system [10] that used query logs along with web lists as external evidence<br>besides free text, and designed an iterative similarity aggregation function for<br>set expansion. Recently, Wang et al. [28] leveraged web tables and showed very<br>competitive results when not only seed entities but also intended class name<br>were given. While these semi-structured lists and tables are helpful, they are<br>not always available for some specific domain corpus such as PubMed articles<br>or DBLP papers. Perhaps the most relevant work to ours is by Rong [21]. In<br>that paper, the authors used the skip-gram feature combined with additional<br>user-generated ontologies (i.e., Wikipedia list) for set expansion. However, they<br>targeted the multifaceted expansion and exploited all skip-gram features for<br>calculating the similarity because two entities. In our work, we keep the core<br>idea of distributional similarity but calculate such similarity using only carefully<br>selected denoised context features.<br>In a broader sense, our work is also related to information extraction and<br>named entity recognition. Without given enough training data, bootstrapped<br>entity extraction system [5][7][8] is the most popular and effective choice. At each<br>bootstrap iteration, the system will first create patterns around entities; score<br>patterns based on their ability to extract more positive entities and less negative<br>entities (if provided), and use top-ranked patterns to extract more candidate<br>entities. Multiple pattern scoring and entity scoring functions are proposed. For<br>example, Riloff et al. [20] scored each pattern by calculating the ratio of positive<br>entities among all entities extracted by it, and scored each candidate entity by the<br>number and quality of its matched patterns. Gupta et al. [7] scored patterns using<br>the ratio of scaled frequencies of positive entities among all entities extracted by<br>it. All these methods are heuristic and sensitive to different model parameters.<br>More generally, our work is also related to class label acquisition [24][30]<br>which aims to propagate class labels to data instances based on labeled training<br>examples, and entity clustering [1][12] where the goal is to find clusters of entities.<br>However, the class label acquisition methods require a much larger number of<br>training examples than the typical size of user input seed set, and the entity<br>clustering algorithms can only find semantically related entities instead of entities<br>strictly in the same semantic class.<br>3 Our Methodology: The SetExpan Framework<br>This section introduces first the context features and data model used by SetExpan<br>in Sect. 3.1 and then our context-dependent similarity measure in Sect. 3.2. It<br>then discusses how to select context features in Sect. 3.3 and presents our novel<br>unsupervised ranking-based ensemble method for entity selection in Sect. 3.4.<br>SetExpan 5<br>Location<br>city , <strong> , USA<br>US state of </strong> .<br>senator from <strong> ,<br>county , </strong> , on<br>Ontario<br>Texas<br>Illinois<br>Florida<br>Context features Entities<br>Illinois 2/6<br>Texas 2/6<br>Ontario 4<br>Entities Similarity<br>Similarity with entity “Florida”<br>based on all context features<br>Similarity with entity “Florida”<br>based on selected context features<br>pay <strong> sale tax . {“city , </strong> USA”, “US state of <strong> .”}<br>Ontario 0/2<br>Texas 2/2<br>Illinois 2/2<br>Entities Similarity<br>(a) (b)<br>Fig. 2: (a) A simplified bipartite graph data model. (b) Similarity with seed entity<br>conditioned on two different sets of context features.<br>3.1 Data Model and Context Features<br>We explore two types of context features obtained from the plain text: (1) skipgrams [21] and (2) coarse-grained types [8]. Data is modeled as a bipartite graph<br>(Figure 2(a)), with candidate entities on one side and their context features on<br>the other. Each type of context features are described as follows.<br>Skip-gram: Given a target entity ei<br>in a sentence, one of its skip-gram is “w−1<br>w1” where w−1 and w1 are two context words and ei<br>is replaced with a<br>placeholder. For example, one skip-gram of entity “Illinois” in sentence “We need<br>to pay Illinois sales tax.” is “pay sales”. As suggested in [21], we extract up to<br>six skip-grams of different lengths for one target entity ei<br>in each sentence. One<br>advantage of using skip-grams is that it imposes strong positional constraints.<br>Coarse-grained type: Besides the unstructured skip-gram features, we use<br>coarse-grained type to filter those obviously-wrong entities. For examples, when<br>we expand the “U.S. states”, we will not consider any entity that is typed “Person”.<br>After this process, we can obtain a cleaner subset of candidate entities. This<br>mechanism is also adopted in [8].<br>After obtaining the “nodes” in bipartite graph data model, we need to model<br>the edges in the graph. In this paper, we assign the weight between each pair of<br>entity e and context feature c using the TF-IDF transformation [21], which is<br>calculated as follows:<br>fe,c = log(1 + Xe,c)<br>“<br>log |E| − log X<br>e0<br>Xe0<br>,c!# , (1)<br>where Xe,c is the raw co-occurrence count between entity e and context feature<br>c, |E| is the total number of candidate entities. We refer to such scaling as<br>the TF-IDF transformation since it resembles the tf-idf scoring in information<br>retrieval if we treat each entity e as a “document” and each of its context feature<br>c as a “term”. Empirically, we find such weight scaling performs outperforms some<br>other alternatives such as point-wise mutual information (PMI) [10], truncated<br>PMI [15], and BM25 scoring [19].<br>6 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han<br>3.2 Context-dependent Similarity<br>With the bipartite graph data model constructed, the task of expanding an entity<br>set at each iteration can be viewed as finding a set of entities that are most<br>“similar” to the currently expanded set. In this study, we use the weighted Jaccard<br>similarity measure. Specifically, given a set of context features F, we calculate<br>the context-dependent similarity as follows:<br>Sim(e1, e2|F) =<br>P<br>c∈F min(fe1,c, fe2,c)<br>P<br>c∈F max(fe1,c, fe2,c)<br>. (2)<br>Notice that if we change context feature set F, the similarity between entity pair<br>is likely to change, as demonstrated in the following example.<br>Example 1 Figure 2(a) shows a simplified bipartite graph data model where all<br>edge weights are equal to 1 (and thus omitted from the graph for clarity). The<br>entity “Florida” connects with all 6 different context features, while the entity<br>“Ontario” is associated with top 4 context features including 1 type feature and 3<br>skip-gram features. If we add all the 6 possible context features into the context<br>feature set F, the similarity between “Florida” and “Ontario” is 1+1+1+1<br>1+1+1+1+1+1 =<br>4<br>6<br>.<br>On the other hand, if we put only two context features “city , , USA”, “US<br>state of .” into F, the similarity between same pair of entities will change to<br>1+1<br>1+1 =<br>2<br>2<br>. Therefore, we refer such similarity as context-dependent similarity.<br>Finally, we want to emphasize that our proposed method is general in the sense<br>that other common similarity metrics such as cosine similarity can also be used.<br>In practice, we find the performance of a set expansion method depends not<br>really on the exact choice of base similarity metrics, but more on which contexts<br>are selected for calculating context-dependent similarity. Similar results were also<br>reported in a previous study [10].<br>3.3 Context Feature Selection<br>As shown in Example 1, the similarity between two entities really depends on<br>the selected feature set F. The motivation of context feature selection is to find<br>a feature subset F<br>∗ of fixed size Q that best “profiles” the target semantic class.<br>In other words, we want to select a feature set F<br>∗ based on which entities within<br>target class are most “similar” to each other. Given such F<br>∗<br>, the entity-entity<br>similarity conditioned on it can best reflect their distributional similarity with<br>regard to the target class. In some sense, such F<br>∗ best profiles the target semantic<br>class. Unfortunately, to find such F<br>∗ of fixed size Q, we need to solve the following<br>optimization problem which turns out to be NP-Hard, as shown in [3].<br>F<br>∗ = arg max<br>|F |=Q<br>X<br>|X|<br>i=1<br>X<br>|X|<br>j&gt;i<br>Sim(ei, ej |F), (3)<br>where X is the set of currently expanded entities. Initially, we treat the user<br>input seed set S as X. As iterations proceed, more entities will be added into X.<br>SetExpan 7<br>Given the NP-hardness of finding the optimal context feature set, we resort to<br>a heuristic method that first scores each context feature based on its accumulated<br>strength with entities in X and then selects top Q features with maximum scores.<br>This process is illustrated in the following example:<br>Example 2 For demonstration purpose, we again assume all edge weights in<br>Figure 2(a) are equal to 1 and let the currently expanded entity set X be {“Florida”,<br>“Texas”}. Suppose we want to select two “denoised” context features, we will first<br>score each context feature based on its associated entities in X. The top 4 contexts<br>will obtain a score 1 since they match only one entity in X with strength 1, and<br>the 2 contexts below will get a score 2 because they match both entities in X.<br>Then, we rank context features based on their scores and select 2 contexts with<br>highest scores: “city , , USA”, “US state of .” into F.<br>Finally, we want to emphasize two major differences of our context feature<br>selection method from other heuristic “pattern selection” methods. First, most<br>pattern selection methods require either users to explicitly provide the “negative”<br>examples for the target semantic class [11][8][22], or implicitly expand multiple<br>mutually exclusive classes in which instances in one class serve as negative<br>examples for all the other classes [4][15]. Our method requires only a small<br>number of “positive” examples. In most cases, it is hard for humans to find<br>good discriminative negative examples for one class, or to provide both mutually<br>exclusive and somehow related comparative classes. Second, the bootstrapping<br>method will add its selected “quality patterns” during each iteration into a quality<br>pattern pool, while our method will select high quality context features at each<br>iteration from scratch. If one noisy pattern is selected and added into the pool, it<br>will continue to introduce more irrelevant entities at all the following iterations.<br>Our method can avoid such noise accumulation.<br>3.4 Entity Selection via Rank Ensemble<br>Intuitively, the entity selection problem can be viewed as finding those entities<br>that are most similar to the currently expanded set X conditioned on the selected<br>context feature set F. To achieve this, we can rank each candidate entity based<br>on its score in eq. (4) and then add top-ranked ones into the expanded set:<br>score(e|X, F) = 1<br>|X|<br>X<br>e0∈X<br>Sim(e, e<br>0<br>|F). (4)<br>However, due to the ambiguity of natural language in free-text corpora, the<br>selected context feature set F may still be noisy in the sense that an irrelevant<br>entity is ranked higher than a relevant one. To further reduce such errors, we<br>propose a novel ranking-based ensemble method for entity selection.<br>The key insight of our method is that an inferior entity will not appear<br>frequently in multiple pre-ranked entity lists at top positions. Given a selected<br>context set F, we first use sampling without replacement method to generate T<br>subsets of context features Ft, t = 1, 2, . . . , T. Each subset is of size α|F| where α<br>8 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han<br>USA 1/3<br>Quebec 1/3<br>Baja California 1/2<br>California 1/1 + 1/3<br>Florida 1/1 + 1/2<br>Arizona 1/2 + 1/1<br>Entities Score<br>Quebec 3<br>Arizona 2<br>California 1<br>Entities Rank<br>California 3<br>Baja California 2<br>Florida 1<br>Entities Rank<br>USA 3<br>Florida 2<br>Arizona 1<br>Entities Rank<br>Pre-ranked entity list 1<br>Pre-ranked entity list 2<br>Pre-ranked entity list 3<br>Final ranked list of entities<br>Denoised context sets F<br>city , </strong> , USA<br>US state of <strong> ,<br>pay </strong> sale tax .<br>Texas and <strong> ,<br>city , </strong> , USA<br>US state of <strong> ,<br>pay </strong> sale tax .<br>Context subset F2<br>city , <strong> , USA<br>pay </strong> sale tax .<br>Texas and <strong> ,<br>Context subset F3<br>US state of </strong> ,<br>pay <strong> sale tax .<br>Texas and </strong> ,<br>Context subset F1<br>Fig. 3: A toy example to show entity selection via rank ensemble.<br>is a model parameter within range [0, 1]. For each Ft, we can obtain a pre-ranked<br>list of candidate entities Lt based on score(e|X, Ft) defined in eq. (4). We use r<br>i<br>t<br>to denote the rank of entity ei<br>in list Lt. If entity ei does not appear in Lt, we<br>let r<br>i<br>t = ∞. Finally, we collect T pre-ranked lists and score each entity based on<br>its mean reciprocal rank (mrr). All entities with average rank above r, namely<br>mrr(e) ≤ T /r, will be added into entity set X.<br>mrr(ei) = XT<br>t=1<br>1<br>r<br>i<br>t<br>, r<br>i<br>t =<br>X<br>ej∈E<br>I (score(ei|X, Ft) ≤ score(ej |X, Ft)), (5)<br>where I(·) is the indicator function. Naturally, a relevant entity will rank at<br>top position in multiple pre-ranked lists and thus accumulate a high mrr score,<br>while an irrelevant entity will not consistently appear in multiple lists at high<br>position which leads to low mrr score. Finally, we use the following example to<br>demonstrate the whole process of entity selection.<br>Example 3 In Figure 3, we want to expand the “US states” semantic class given<br>a selected context feature set F with 4 features. We first sample a subset of 3 context<br>features F1 = {“city , , USA”, “US state of ,”, “pay sales tax .”}, and then<br>use F1 to obtain a pre-ranked entity list L1 = h“California”, “Arizona”, “Quebec”i.<br>By repeating this process three times, we get 3 pre-ranked lists and ensemble them<br>into a final ranked list in which entity “Arizona” is scored 1.5 because it is ranked<br>in the 2nd position in L1 and 1st position in L3. Finally, we add those entities<br>with mrr score larger than 1, meaning this entity is ranked at 3rd position on<br>average, into the expanded set X. In this simple example, the model parameters<br>T = 3, α =<br>|F1|<br>|F | = 0.75, and r = 3.<br>Put all together. Algorithm 1 summarizes the whole SetExpan process. The<br>candidate entity set E and bipartite graph data model G are pre-calculated and<br>stored. A user needs only to specify the seed set S and the expected size of output<br>set K. There is a total of 4 model parameters: the number of top quality context<br>SetExpan 9<br>Algorithm 1 SetExpan<br>1: Input: Candidate entity set E, initial seed set S, entity-context graph G, expected size of output<br>set K, model parameters {Q, T , α, r}.<br>2: Output: The expanded set X.<br>3: X = S.<br>4: while |X| ≤ K do<br>5: Set F = ∅ // Select denoised contexts from scratch<br>6: Score context features based on X and add top Q denoised contexts into F .<br>7: // Entity-selection via rank ensemble<br>8: for t = 1, 2, . . . , T do<br>9: Uniformly sample αQ contexts and construct feature subset Ft.<br>10: Score entities based on Eq. (4) given Ft and obtain the pre-ranked list Lt.<br>11: Update the mrr score of each entity based on Eq. (5).<br>12: end for<br>13: X = X ∪ {e|mrr(e) ≥ T<br>r<br>} // Add entities into expanded set X .<br>14: end while<br>15: Return X.<br>features selected in each iteration Q, the number of pre-ranked entity lists T, the<br>relative size of feature subset 0 &lt; α &lt; 1, and final mrr threshold r. The tuning<br>and sensitivity of these parameters will be discussed in the experiment section.<br>4 Experiments<br>4.1 Experimental Setup<br>Datasets preparation. SetExpan is a corpus-based entity set expansion system<br>and thus we use three corpora to evaluate its performance. Table 1 lists 3 datasets<br>we used in experiments. (1) APR is constructed by crawling all 2015 news articles<br>from AP and Reuters. (2) Wiki is a subset of English Wikipedia used in [13]. (3)<br>PubMed-CVD is a collection of research paper abstracts about cardiovascular<br>disease retrieved from PubMed.<br>For APR and PubMed-CVD datasets, we adopt a data-driven phrase mining<br>tool [14] to obtain entity mentions and type them using ClusType [18]. Each<br>entity mention is mapped heuristically to an entity based on its lemmatized<br>surface name. We then extract variable-length skip-grams for all entity mentions<br>as features for their corresponding entities, and construct the bipartite graph<br>data model as introduced in the previous section. For Wiki dataset, the entities<br>have already been extracted and typed using distant supervision. For the type<br>information in each dataset, there are 16 coarse-grained types in APR and 4<br>coarse-grained types in PubMed-CVD. For Wiki, since it originally has about 50<br>fine-grained types, which may reveal too much information, we manually mapped<br>them to 11 more coarse-grained types.<br>Query construction. A query is a set of seed entities of the same semantic<br>class in a dataset, serving as the input for each system to expand the set. The<br>process of query generation is as follows. For each dataset, we first extract 2000<br>most frequent entities in it and construct an entity list. Then, we ask three<br>volunteers to manually scan the entity lists and propose a few semantic classes<br>for each list. The proposed class should be interesting, relatively unambiguous<br>and has a reasonable coverage in its corresponding corpus. These semantic classes<br>10 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han<br>Table 1: Datasets statistics and Query descriptions<br>Dataset FileSize #Sentences #Entities #Test queries<br>APR 775MB 1.01M 122K 40<br>Wiki 1.02GB 1.50M 710K 20<br>PubMed-CVD 9.3GB 23M 179K 5<br>cover a wide variety of topics, including locations, companies as well as political<br>parties, and have different degrees of difficulty for set expansion. After finalizing<br>the semantic classes for each dataset, the students randomly select entities of<br>each semantic class from the frequent entity list to form 5 queries of size 3. To<br>select the queries for PubMed-CVD, we seek help from two additional students<br>with biomedical expertise, following the same previous approach. Due to the large<br>size of PubMed-CVD dataset and runtime limitation, we only select 1 semantic<br>class (hormones) with 5 queries.<br>With all queries selected, we have humans to label all the classes and instances<br>returned by each of the following 7 compared methods. For APR and Wiki<br>datasets, the inter-rater agreements (kappa-value) over three students are 0.7608<br>and 0.7746, respectively. For PubMed-CVD dataset, the kappa-value is 0.9236.<br>All entities with conflicting label results are further resolved after discussions<br>among all human labelers. Thus, we have our ground truth datasets.<br>Compared methods. Since the focus on this work is the corpus-based set<br>expansion, we do not compare with other methods that require online data<br>extractions. Also, to further analyze the effectiveness of each module in SetExpan framework. We implement 3 variations of our framework.<br>– word2vec [16]: We use the “skip-gram” model in word2vec to learn the<br>embedding vector for each entity, and then return k nearest neighbors around<br>seed entities as the expanded set.<br>– PTE [25]: We first construct a heterogeneous information network including<br>entity, skip-gram features, and type features. PTE model is then applied to<br>learn the entity embedding which is used to determine the k nearest neighbors<br>around seed entities.<br>– SEISA [10]: An entity set expansion algorithm based on iterative similarity<br>aggregation. It uses the occurrence of entities in web list and query log as<br>entity features. In our experiments, we replace the web list and query log<br>with our skip-gram and coarse-grained context features.<br>– EgoSet [21]: A multifaceted set expansion system based on skip-gram features,<br>word2vec embeddings and WikiList. The original system is proposed to expand<br>a seed set to multiple entity sets, considering the ambiguities in seed set. To<br>achieve this, we use a community detection method to separate the extracted<br>entities into several communities. However, in order to better compare with<br>EgoSet, we carefully select queries that have little ambiguity or at least the<br>seed set in the query is dominating in one semantic class. Thus, we discard<br>the community detection part in EgoSet and treat all extracted entities as in<br>one semantic class.<br>SetExpan 11<br>Table 2: Overall end-to-end performance evaluation on 3 datasets over all queries.<br>Methods APR Wiki PubMed-CVD<br>MAP@10 MAP@20 MAP@50 MAP@10 MAP@20 MAP@50 MAP@10 MAP@20 MAP@50<br>EgoSet 0.3949 0.3942 0.3706 0.5899 0.5754 0.5622 0.0511 0.0410 0.0441<br>SEISA 0.7423 0.6090 0.3892 0.7643 0.6606 0.4998 - - -<br>word2vec 0.6054 0.5385 0.4180 0.7193 0.6289 0.4510 0.8427 0.7701 0.6895<br>PTE 0.3144 0.2777 0.1996 0.6817 0.5596 0.3839 0.9071 0.7654 0.5641<br>SetExpan−cs 0.8240 0.7997 0.7674 0.9540 0.8955 0.7439 1.000 1.000 0.5991<br>SetExpan−re 0.8509 0.7792 0.7681 0.9392 0.8680 0.7291 1.000 0.9605 0.7371<br>SetExpanful l 0.8967 0.8621 0.7885 0.9571 0.9010 0.7457 1.000 1.000 0.7454<br>– SetExpan−cs: Disable the context feature selection module in SetExpan, and<br>use all context features to calculate distributional similarity.<br>– SetExpan−re: Disable the rank ensemble module in SetExpan. Instead, we<br>use all selected context feature to rank candidate entities at one time and<br>add top-ranked ones into the expanded set.<br>– SetExpanful l: The full version of our proposed method, with both context<br>feature selection and rank ensemble components enabled.<br>For fair comparison, we try different combinations of parameters and report the<br>best performance for each baseline method.<br>Evaluation Metrics. For each test case, the input is a query, which is a set<br>of 3 seed entities of the same semantic class. The output will be a ranked list of<br>entities. For each query, we use the conventional average precision APk(c, r) at k<br>(k = 10, 20, 50) for evaluation, given a ranked list of entities c and an unordered<br>ground-truth set r. For all queries under a semantic class, we calculate the mean<br>average precision (MAP) at k as 1<br>N<br>P<br>i APk(ci<br>, r), where N is the number of<br>queries. To evaluate the performance of each approach on a specific dataset,<br>we calculate the mean-MAP (MMAP) at k over all queried semantic classes as<br>MMAPk =<br>1<br>T<br>PT<br>t=1[( 1<br>Nt<br>)<br>P<br>i APk(cti, rt)], where T is the number of semantic<br>classes, Nt is the number of queries of t-th semantic class, cti is the extracted<br>entity list for i-th query for t-th semantic class, and rt is the ground truth set<br>for t-th semantic class.<br>4.2 Experimental Results<br>Comparison with four baseline methods. Table 2 shows the MMAP scores<br>of all methods on 3 datasets3<br>. We can see the MMAP scores of SetExpan outperforms all four baselines a lot. We further look at their performances on each<br>concept class, as shown in Figure 4. We can see that the performance of these<br>baseline methods varies a lot on different semantic classes, while our SetExpan can<br>consistently beat them. One reason is that none of these methods applies context<br>feature selection or rank ensemble, and a single set of unpruned features can lead<br>to various levels of noise in the results. Another reason is the lack of an iterative<br>mechanism in some of those approaches. For example, even if EgoSet includes<br>3 Results of SEISA on PubMed-CVD are omitted due to the scalability issue.<br>12 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP@50<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(a) APR Country<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP@50<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(b) APR Law<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP@50<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(c) APR Party<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP@50<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(d) Wiki Sport League<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP@50<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(e) Wiki TV Channel<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP@50<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(f) Wiki China Province<br>Fig. 4: Evaluation results for each semantic class.<br>the results from word2vec to help it boost the performance, it still achieves low<br>MAP scores in some semantic classes. Finding the nearest neighbors in only one<br>iteration can be a key reason. And although SEISA is applying the iterative<br>technique, instead of adding a small number of new entities in each iteration, it<br>expands a full set in each iteration based on the coherence score of each candidate<br>entity with the previously expanded set. It pre-calculates the size of the expanded<br>set with the assumption that the feature similarities follow a certain distribution,<br>which does not always hold to all datasets or semantic classes. Thus, if the size is<br>far different from the actual size or is too big to extract a confident set at once,<br>each iteration will introduce a lot of noise and cause semantic drift.<br>Comparison with SetExpan−re and SetExpan−cs<br>. At the dataset level,<br>the MMAP scores of SetExpanful l outperforms its two variation approaches. In<br>the semantic class level, we can see that SetExpan−re and SetExpan−cs sometimes<br>have their MAP much lower than SetExpanful l while sometimes they almost<br>achieve the same performance with SetExpanful l. This means they fail to stably<br>extract entities with good quality. The main reason is still that a single set of<br>features or ensembles over unpruned features can lead to various levels of noise<br>in the results. Only under the circumstances that the single set of features or the<br>unpruned features happen to be nicely selected without too much noise, which<br>tends to happen when the query is relatively “easy”, these variation approaches<br>can achieve good results.<br>Effects of Context Feature Selection. We already see that adding the<br>context feature selection component helps improve the performance. What’s also<br>noticeable is that the addition of context selection process becomes more obvious<br>as the size of the corpus increases. The difference between MMAP scores of<br>SetExpan 13<br>0.0 0.2 0.4 0.6 0.8 1.0<br>Recall<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Precision<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(a) APR Country<br>0.0 0.2 0.4 0.6 0.8 1.0<br>Recall<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Precision<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(b) APR Law<br>0.0 0.2 0.4 0.6 0.8 1.0<br>Recall<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Precision<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(c) APR Party<br>0.0 0.2 0.4 0.6 0.8 1.0<br>Recall<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Precision<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(d) Wiki Sport League<br>0.0 0.2 0.4 0.6 0.8 1.0<br>Recall<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Precision<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(e) Wiki TV Channel<br>0.0 0.2 0.4 0.6 0.8 1.0<br>Recall<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>Precision<br>PTE<br>word2vec<br>EgoSet<br>SEISA<br>SetExpan-re<br>SetExpan-cs<br>SetExpan-full<br>(f) Wiki China Province<br>Fig. 5: Evaluation results for each concept class on individual query<br>SetExpan−cs and SetExpanful l is much larger in PubMed-CVD compared with<br>APR and Wiki datasets. This is because that as the corpus size increases, we will<br>have more noisy features and more candidate entities while the good features to<br>define the target entity set may be limited. Thus, without context selection, noise<br>can damage the performance much more. The evidence can also be found from the<br>performance of EgoSet across the three datasets. It can achieve reasonably good<br>results in APR and Wiki, however, it performs much worse in PubMed-CVD.<br>Effect of Rank Ensemble. From the above experiments, the effect of rank<br>ensemble has variance across the different semantic classes, however, it seems<br>to be more stable across datasets, compared with the effect of context selection.<br>This is because we apply the default set of parameter values in each test case<br>above. In the parameter analysis part, we will show that the number of ensemble<br>batches and the percentage of features to be randomly sampled can affect the<br>contribution of rank ensemble to the set expansion performance.<br>Parameter Analysis. There are totally 4 parameters in SetExpan – Q (the<br>number of selected context features), α (the percentage of features to be sampled),<br>T (the number of ensemble batches), and r (the threshold of a candidate entity’s<br>average rank). We study the influence of each parameter by fixing all other<br>parameters to default values, and present one graph showing the MMAP scores<br>of SetExpan on APR dataset versus the changes of that parameter.<br>– α: From the graph, the performance increases sharply as α increases until it<br>reaches about 0.6. Then, it starts to stay stable and decreases after 0.7.<br>– Q: In the range of 50 - 150, the performance increases sharply as Q increases,<br>which means the majority of top 150 context features can provide rich<br>information to identify entities belonging to the target semantic class. The<br>14 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han<br>0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0<br>alpha<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP<br>MAP@50<br>MAP@20<br>MAP@10<br>(a) α<br>40 60 80 100 120<br>T<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP<br>MAP@50<br>MAP@20<br>MAP@10<br>(b) T<br>50 100 150 200 250 300 350 400<br>Q<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP<br>MAP@50<br>MAP@20<br>MAP@10<br>(c) Q<br>3 4 5 6 7 8 9 10<br>r<br>0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>MAP<br>MAP@50<br>MAP@20<br>MAP@10<br>(d) r<br>Fig. 6: Parameter Sensitivity on two datesets<br>available information gets more and more saturated after Q reaches 150 and<br>start to introduce noises and hamper the performance after around 300.<br>– r: Our experiments show that the performance is not very sensitive to the<br>threshold of a candidate entity’s average rank.<br>– T: The performance keeps increasing as we increase the ensemble batches,<br>due to the robustness to noise of ensembling. The performance becomes more<br>stable after 60 batches.<br>Case Studies. Figure 7 presents three case studies for SetExpan. We show<br>one query for each dataset. In each case, we show top 3 ranked entities and<br>top/bottom 3 skip-gram features after context feature selection for the first 3<br>iterations as well as the coarse-grained type. In all cases, our algorithm successfully<br>extracts correct entities in each iteration, and the top-ranked skip-grams are<br>representative in defining the target semantic class. On the other hand, we<br>notice that most of the bottom 3 skip-grams selected are very general or not<br>representative at all. These context features could potentially introduce noisy<br>entities and thus the rank ensemble can play a rival role in improving the results.<br>5 Conclusion and Future Work<br>In this paper, we study the problem of corpus-based set expansion. First, we<br>propose an iterative set expansion framework with a context feature selection<br>method, to deal with the problem of entity intrusion and semantic drift. Second,<br>we develop a novel unsupervised ranking-based ensemble algorithm for entity<br>selection, to further reduce context noise in free-text corpora. Experimental<br>results on three publicly available datasets corroborate the effectiveness and<br>robustness of our proposed SetExpan.<br>The proposed framework is general and can incorporate other context features<br>besides skip-grams, such as Part-Of-Speech tags or syntactic head tokens. Besides,<br>it would be interesting to study more rank ensemble methods for aggregating<br>multiple pre-ranked lists. In addition, our current framework treats each feature<br>independently, it would be interesting to study how the interaction of context<br>features can influence the expansion result. We leave it for future work.<br>Acknowledgments. Research was sponsored in part by the U.S. Army Research<br>Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), National<br>SetExpan 15<br>APR<br>Dataset<br>PubMed<br>-CVD<br>Wiki Organization<br>Coarsegrained<br>type<br>Proteins<br>and<br>Genes<br>(PRGE)<br>Event<br>Iteration 1:<br>Top 3: “the <strong> provisions”, “provisions of the </strong>”, “defund <strong> .”<br>Bottom 3: “2010 </strong> ,”, “also known as <strong> .”, “under the </strong> , and”<br>Iteration 2:<br>Top 3: “under the <strong> to”, “provisions of the </strong> ,”, “the <strong> into law.”<br>Bottom 3: “the </strong> - which has”, “the _The House”, “the _ , first”<br>Iteration 3:<br>Top 3: “under the <strong> to”, “Under the </strong> ,”, “the <strong> into law”<br>Bottom 3: “of the </strong> passed”, “the <strong> , the most”, “replacing </strong> .”<br>Top/Bottom skip-gram features<br>selected in first 3 iterations<br>Iteration 1:<br>Top 3: “stimulating hormone ( <strong> )“, “hormone ( </strong> ) ,”, “hormone ( <strong> ) and”,<br>…<br>Bottom 3: “g/L , </strong> =”, “, <strong> and prolactin”, “hormone ( </strong> ) -”<br>Iteration 2:<br>Top 3: “hormone ( <strong> ) ,”, “hormone ( </strong> ) and”, “hormone ( <strong> ) .”, …<br>Bottom 3: “, </strong> , estradiol ,”, “, <strong> , and PRL”, “hormone ( </strong> ) -”<br>Iteration 3:<br>Top 3: “hormone ( <strong> ) ,”, “hormone ( </strong> ) and”, “hormone ( <strong> ) .”, …<br>Bottom 3: “( </strong> ) and insulin-like”, “, TSH , <strong> ,”, “levels of </strong> , FSH”<br>Iteration 1:<br>Top 3: “telecast on <strong> .”, “televised on </strong> .”, “televised by <strong> .”<br>Bottom 3: “on </strong> , to the”, “, and perhaps <strong> .”, “from an </strong> website”<br>Iteration 2:<br>Top 3: “the <strong> sitcom”, “the </strong> television network”, “ABC , <strong> ,”<br>Bottom 3: “on </strong> on September”, “broadcast on <strong> on”, “the </strong> soap opera<br>The”<br>Iteration 3:<br>Top 3: “the <strong> sitcom”, “the </strong> soap”, “the <strong> soap opera”, …<br>Bottom 3: “aired on </strong> between”, “of the <strong> show”, “on the </strong> crime”<br>Iteration 1:<br>LH, GH, ACTH, …<br>Iteration 2:<br>LHRH, AMH, GHRH, …<br>Iteration 3:<br>Renin, GnRH-I, AVP, …<br>{FSH,<br>TSH,<br>MSH}<br>Iteration 1:<br>ABC, CBS, NBC, …<br>Iteration 2:<br>BBC, ITV, Channel 4, …<br>Iteration 3:<br>TBS, ITV1, BBC Two, …<br>{ESPN,<br> ESPN2,<br>Spike TV}<br>{Patriot Act,<br>Obamacare,<br>Clery Act}<br>Iteration 1:<br>USA Patriot Act, USA Freedom Act,<br>Voting Rights Act, …<br>Iteration 2:<br>Stock Act, Religious Freedom<br>Restoration Act, Foreign Intelligence<br>Surveillance Act, …<br>Iteration 3:<br>Americans with Disabilities Act, Healthy<br>Families Act, Goonda Act, …<br>Top ranked entities<br>in first 3 iterations Query<br>Fig. 7: Three case studies on each dataset.<br>Science Foundation IIS-1320617, IIS 16-18481, and NSF IIS 17-04532, and grant<br>1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH<br>Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).<br>References</p><ol><li>R. Balasubramanyan, B. B. Dalvi, and W. W. Cohen. From topic models to semisupervised learning: Biasing mixed-membership models to exploit topic-indicative<br>features in entity clustering. In ECML/PKDD, 2013.</li><li>Z. Chen, M. Cafarella, and H. Jagadish. Long-tail vocabulary dictionary extraction<br>from the web. In WSDM, pages 625–634. ACM, 2016.</li><li>F. Chierichetti, R. Kumar, S. Pandey, and S. Vassilvitskii. Finding the jaccard<br>median. In SODA, 2010.</li><li>J. R. Curran, T. Murphy, and B. Scholz. Minimising semantic drift with mutual<br>exclusion bootstrapping. 2007.</li><li>O. Etzioni, M. J. Cafarella, D. Downey, A.-M. Popescu, T. Shaked, S. Soderland,<br>D. S. Weld, and A. Yates. Unsupervised named-entity extraction from the web: An<br>experimental study. Artif. Intell., 165:91–134, 2005.</li><li>Z. Ghahramani and K. A. Heller. Bayesian sets. In NIPS, 2005.</li><li>S. Gupta, D. L. MacLean, J. Heer, and C. D. Manning. Research and applications: Induced lexico-syntactic patterns improve information extraction from online<br>medical forums. JAMIA, 21:902–909, 2014.</li><li>S. Gupta and C. D. Manning. Improved pattern learning for bootstrapped entity<br>extraction. In CoNLL, pages 98–108, 2014.</li><li>S. Gupta and C. D. Manning. Distributed representations of words to guide<br>bootstrapped entity classifiers. In HLT-NAACL, 2015.<br>16 J. Shen, Z. Wu, D. Lei, J. Shang, X. Ren, J. Han</li><li>Y. He and D. Xin. Seisa: set expansion by iterative similarity aggregation. In<br>WWW, 2011.</li><li>P. Jindal and D. Roth. Learning from negative examples in set-expansion. In 2011<br>IEEE 11th International Conference on Data Mining, 2011.</li><li>D. Lin and X. Wu. Phrase clustering for discriminative learning. In ACL/IJCNLP,<br>2009.</li><li>X. Ling and D. S. Weld. Fine-grained entity recognition. In AAAI, 2012.</li><li>J. Liu, J. Shang, C. Wang, X. Ren, and J. Han. Mining quality phrases from<br>massive text corpora. In SIGMOD, pages 1729–1744. ACM, 2015.</li><li>T. McIntosh and J. R. Curran. Weighted mutual exclusion bootstrapping for<br>domain independent lexicon and template acquisition. 2008.</li><li>T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. CoRR, abs/1310.4546,<br>2013.</li><li>P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu, and V. Vyas. Web-scale<br>distributional similarity and entity set expansion. In EMNLP, 2009.</li><li>X. Ren, A. El-Kishky, C. Wang, F. Tao, C. R. Voss, and J. Han. Clustype: Effective<br>entity recognition and typing by relation phrase-based clustering. In WWW, pages<br>995–1004. ACM, 2015.</li><li>X. Ren, Y. Lv, K. Wang, and J. Han. Comparative document analysis for large<br>text corpora. CoRR, abs/1510.07197, 2017.</li><li>E. Riloff. Automatically generating extraction patterns from untagged text. In<br>AAAI/IAAI, Vol. 2, 1996.</li><li>X. Rong, Z. Chen, Q. Mei, and E. Adar. Egoset: Exploiting word ego-networks and<br>user-generated ontology for multifaceted set expansion. In WSDM, pages 645–654.<br>ACM, 2016.</li><li>B. Shi, Z. Zhang, L. Sun, and X. Han. A probabilistic co-bootstrapping method for<br>entity set expansion. In COLING, 2014.</li><li>S. Shi, H. Zhang, X. Yuan, and J.-R. Wen. Corpus-based semantic class mining:<br>Distributional vs. pattern-based approaches. In COLING, 2010.</li><li>P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichandran, R. Bhagat, and F. Pereira.<br>Weakly-supervised acquisition of labeled class instances using graph random walks.<br>In EMNLP, 2008.</li><li>J. Tang, M. Qu, and Q. Mei. Pte: Predictive text embedding through large-scale<br>heterogeneous text networks. In KDD, pages 1165–1174. ACM, 2015.</li><li>S. Tong and J. Dean. System and methods for automatically creating lists, 2008.<br>US Patent 7,350,187.</li><li>P. Velardi, S. Faralli, and R. Navigli. Ontolearn reloaded: A graph-based algorithm<br>for taxonomy induction. Computational Linguistics, 39(3):665–707, 2013.</li><li>C. Wang, K. Chakrabarti, Y. He, K. Ganjam, Z. Chen, and P. A. Bernstein. Concept<br>expansion using web tables. In WWW, 2015.</li><li>R. C. Wang and W. W. Cohen. Language-independent set expansion of named<br>entities using the web. In ICDM, 2007.</li><li>Y.-Y. Wang, R. Hoffmann, X. Li, and J. Szymanski. Semi-supervised learning of<br>semantic classes for query understanding: from the web and for the web. In CIKM,<br>2009.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SetExpan: Corpus-Based Set Expansion via&lt;br&gt;Context Feature Selection and Rank Ensemble&lt;br&gt;Jiaming Shen?&lt;br&gt;, Zeqiu Wu?&lt;br&gt;, Dongming Lei
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>41.Trapping Rain Water</title>
    <link href="http://riroaki.github.io/2019/04/19/42-Trapping-Rain-Water/"/>
    <id>http://riroaki.github.io/2019/04/19/42-Trapping-Rain-Water/</id>
    <published>2019-04-19T12:14:13.000Z</published>
    <updated>2019-04-19T12:17:16.567Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>Given <em>n</em> non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining.</p><p><img src="https://assets.leetcode.com/uploads/2018/10/22/rainwatertrap.png" alt><br>The above elevation map is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped. <strong>Thanks Marcos</strong> for contributing this image!</p><a id="more"></a><p><strong>Example:</strong></p><pre><code>Input: [0,1,0,2,1,0,1,3,2,1,2,1]Output: 6</code></pre><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><p>待补充。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;Given &lt;em&gt;n&lt;/em&gt; non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it is able to trap after raining.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://assets.leetcode.com/uploads/2018/10/22/rainwatertrap.png&quot; alt&gt;&lt;br&gt;The above elevation map is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped. &lt;strong&gt;Thanks Marcos&lt;/strong&gt; for contributing this image!&lt;/p&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="hard" scheme="http://riroaki.github.io/tags/hard/"/>
    
  </entry>
  
  <entry>
    <title>软件架构扫盲</title>
    <link href="http://riroaki.github.io/2019/04/18/Software-architecture/"/>
    <id>http://riroaki.github.io/2019/04/18/Software-architecture/</id>
    <published>2019-04-18T07:24:32.000Z</published>
    <updated>2019-04-22T02:48:12.837Z</updated>
    
    <content type="html"><![CDATA[<p>面试被问到对架构的理解。老实说，我对这一块本来没有深入思考过。</p><p>之前常常集中精力在一些具体理论或者工具上，对架构这一块的了解仅限于感性意识——即，我知道有这样的理论和结构，但是无法清晰地描述它。</p><p>所以借此机会，在网上找了一些资料，了解了一下现代主流的软件架构。</p><p>本文主要基于O’Relly出版的一本介绍软件架构的<a href="https://www.oreilly.com/programming/free/files/software-architecture-patterns.pdf" target="_blank" rel="noopener">免费小册子</a>。</p><a id="more"></a><p>在介绍架构之前，首先引入评价架构的指标：</p><h3 id="架构评价指标"><a href="#架构评价指标" class="headerlink" title="架构评价指标"></a>架构评价指标</h3><ul><li>是否易于部署</li><li>是否易于拓展和修改功能</li><li>是否易于测试</li><li>是否易于伸缩</li><li>是否易于开发</li></ul><p>下面介绍5种架构，并且会针对这些指标，给出关于这些架构的评价。</p><h3 id="分层架构LA"><a href="#分层架构LA" class="headerlink" title="分层架构LA"></a>分层架构LA</h3><p>这是最常见的软件架构，常见分层为4层：展示层，业务层，持久层，和数据库层。</p><p>每一层之间通过接口相连接。比如，用户访问网页并登陆，经过登陆逻辑，继续往下验证输入的用户名和密码，在数据库层获取数据后在后台验证，返回验证结果并显示在前端界面。用户请求将会被每一层处理。</p><p><img src="/2019/04/18/Software-architecture/layer.jpg" alt></p><p>这个结构的特性很好理解，就如神经网络每一层之间的关系，上一层的输出被当作下一层的输入。缺陷也是显而易见的：耦合度比较高，层与层之间必须接口一致（像神经网络，的层间的输入输出维度必须一致）。</p><p>有些分层模型会适当地开放一些层，使得层与层之间可以跨越式访问，但是这样做会让层次之间联系变得更为复杂，提高了耦合度。</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>每层可以单独测试，因为有完整的接口。</li><li>分工简单，因为每个团队（个人）只需要做和他关系最大的那一块就可以。主流的”前端”、”后端”都是根据层来划分技术人员的。</li><li>开发比较容易，只需要面向接口编程就可以了。</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>性能比较差。每个用户请求都需要穿越所有的层。</li><li>维护困难。当我修改数据库层或者展现层的时候，就意味着可能上一层或者下一层需要变动，而发生变动的时候需要将服务停止运行。过高的耦合度使得软件变得牵一发而动全身。</li><li>拓展性不好。同上一点，需要添加或者删减一个功能的时候，都会带来所有层的影响。</li><li>部署不灵活。因为分层架构缺一不可，需要完整地部署整个项目，确保每个层都完成部署才能上线。</li></ul><h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>常见的网页开发（如普通的课程项目<em>图书管理系统</em>）=前端+后端+数据库。</p><h3 id="事件驱动架构EDA"><a href="#事件驱动架构EDA" class="headerlink" title="事件驱动架构EDA"></a>事件驱动架构EDA</h3><p>高度解耦合的架构，适应性很高。包括中介拓扑结构和代理拓扑结构。</p><h4 id="中介拓扑结构Mediator"><a href="#中介拓扑结构Mediator" class="headerlink" title="中介拓扑结构Mediator"></a>中介拓扑结构Mediator</h4><p>处理事件主要包含的环节：</p><ul><li>事件队列Event Queue：所有事件发生之后会进入队列。队列可以有多个。</li><li>事件调停器Event Mediator：调停器通过拓扑排序等手段确定事件的执行顺序并进行处理（以及并行or串行）</li><li>事件管道Event Channel：调停器运输经过处理后的事件到处理器的路径</li><li>事件处理器Event Processor：事件的终点，负责响应事件。负责独立运作，完成单一的工作。</li></ul><p>⚠️注意，事件处理器收到的事件和在消息队列的事件不是同一个概念。它们有关联，但是后者是调停器生成的，和初始事件不完全一样。</p><p>以搬家举例，搬家是初始事件，当调停器通过事件队列收到事件之后，将其分解为多个子事件（搬家当然有很多事情需要做，比如联系方式、通知保险公司等等）。</p><p>此时，这些新的子事件就会被分配到相应的处理器进行响应。</p><p><img src="/2019/04/18/Software-architecture/2-2.png" alt></p><h4 id="代理拓扑结构Broker"><a href="#代理拓扑结构Broker" class="headerlink" title="代理拓扑结构Broker"></a>代理拓扑结构Broker</h4><p>在代理拓扑结构中，核心的调停器被去除，而是由事件处理器代为转发给下一个事件处理器（如同接力赛跑）。如果这一块不是分内之事，就不操作，转发就可以。</p><p>这一个架构的场景是需要处理的事件逻辑都比较简单，没有很复杂的依赖关系（这是我的理解）。</p><p>由于没有中间的过程，效率会高一些。相当于把分解过程由处理器代劳了，有点去中心化的味道。</p><p>还是以搬家的事件为例说明：</p><p><img src="/2019/04/18/Software-architecture/2-4.png" alt></p><h4 id="另一种架构：Proxy"><a href="#另一种架构：Proxy" class="headerlink" title="另一种架构：Proxy"></a>另一种架构：Proxy</h4><p>这一个说实话我本来没注意到，以为是和代理broker差不多（毕竟proxy也是代理的意思）。</p><p>看到描述，引用在这里。暂时看不明白。<a href="https://github.com/hehonghui/android-tech-frontier/blob/master/software-architecture-patterns/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E6%A8%A1%E5%BC%8F.md#appendix" target="_blank" rel="noopener">出处</a></p><blockquote><ul><li>如果我们使用 proxy，那就意味着我自己对需要处理的事件进行了分解，然后把不同的子事件一一委托给不同的 proxy，由被委托的 proxy 帮我完成子事件，从而完成我要做的事件。</li></ul></blockquote><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul><li>便于部署。因为每个处理器是独立的，可以单独部署。当然，如果使用的是中介拓扑结构，那会稍微麻烦一些——修改处理器的同时需要同时修改中介，即调度器。</li><li>性能高。异步处理事件（交付后不必等待其完成处理即可返回），相当于并行操作，提高效率。</li><li>可拓展性强。相比层级结构，易于添加功能。</li></ul><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul><li>实现比较复杂。事件是异步分发处理的。</li><li>难以测试。主要来自其异步的特点。</li></ul><blockquote><p>使用事件驱动架构模式最困难的地方就在于架构的创建、维护、以及对事件处理器的管理。通常每一个事件都拥有其指定的事件处理协议（例如：传递给事件处理器的数据类型、数据格式），这就使得设下标准的数据格式成为使用事件驱动架构模式中至关重要的一环（例如：XML，JSON，Java 对象，等等……），并在架构创建之初就为这些数据格式授权，以便处理。</p></blockquote><h3 id="微核架构MKA"><a href="#微核架构MKA" class="headerlink" title="微核架构MKA"></a>微核架构MKA</h3><p>看到这个就想起操作系统的位内核架构，其实两个差不多——不如说微内核的操作系统也属于一种微核架构的实践？hhh。</p><p>如同微内核操作系统的结构，内核部分只需要提供一个最基本功能的实现即可；而需要什么功能，做相应插件就能够实现。原则上插件相互独立，不应过于依赖彼此，通信也应该少一些。</p><p><img src="/2019/04/18/Software-architecture/microKernal.png" alt></p><blockquote><p>微内核架构是可以被嵌入到其他架构（如层级架构）中间的。</p></blockquote><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul><li>功能拓展性好，需要什么功能开发插件就可以。</li><li>部署较方便，同样是因为插件的特性，可以独立加载或者卸载。</li><li>可定制性高，需要什么就加什么。</li><li>可以渐进式开发。</li></ul><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul><li>内核系统难以做成分布式，只能以独立单元存在。</li><li>开发难度较高。需要设计插件的注册机制，插件与内核通信等等（其实应该是因为这一个领域做的不多？）。</li></ul><h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>说到插件，就自然联想到常见的各种IDE、浏览器了。</p><h3 id="微服务架构MSA"><a href="#微服务架构MSA" class="headerlink" title="微服务架构MSA"></a>微服务架构MSA</h3><p>这一个架构是从SOA（面向服务的架构）发展而来的——SOA也是一种架构，和微服务架构大体相似，也是将功能独立成各种单元，通过良好的接口定义连接起来。与原先的宏模式相比，强调了松耦合和组件化的特征。</p><p>微服务架构将应用的服务分解成不同模块，以管道和用户界面通信。微服务架构模式的另一个关键概念是它是一个<em>分布式</em>的架构，所以所有组件之间是完全解耦的，用户界面层通过某种远程访问协议（如，JMS, AMQP, REST, SOAP, RMI等）进行访问。</p><p>REST、SOAP比较常见，在这里简单介绍一下：</p><ul><li>REST是基于http+json格式传输信息的协议。</li><li>SOAP是基于http+XML格式传输信息的协议。</li></ul><p><img src="/2019/04/18/Software-architecture/4-1.png" alt></p><h4 id="模式的拓扑结构"><a href="#模式的拓扑结构" class="headerlink" title="模式的拓扑结构"></a>模式的拓扑结构</h4><ol><li>基于REST的API拓扑，使用者多为网站，采用粒度非常小的服务分解，每一个小的服务对应一个极小的API。</li><li>基于REST的拓扑，类似上一类，但是将多个REST的API结构进行整合成一个大的REST服务组件。</li><li>集中式拓扑，采用一个代理层分析请求并实现操作。主要是用在控制逻辑比较复杂的场合。</li></ol><p>⚠️注意，使用REST的时候需要合理划分粒度和服务，减少耦合依赖和重复。</p><h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4><ul><li>易于部署。由于服务比较独立，一般采用分布式组件，可以单独部署。由于改变某个组件被隔离，所以避免了月末的爆炸性增长部署工作。</li><li>易于测试。由于服务独立，所以可以分开测试。</li><li>伸缩性好，也就是可以适应不同规模。</li><li>易于开发，因为单个组件功能简单且不会互相影响，所以协调工作减少。</li></ul><h4 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h4><ul><li>性能较低。分布式特性不适合高性能要求的程序。</li></ul><h3 id="基于空间的架构——云架构SBA"><a href="#基于空间的架构——云架构SBA" class="headerlink" title="基于空间的架构——云架构SBA"></a>基于空间的架构——云架构SBA</h3><blockquote><p>大多数基于网站的商务应用都遵循相同的请求流程：一个请求从浏览器发到web服务器，然后到应用服务器，然后到数据库服务器。虽然这个模式在用户数不大的时候工作良好，但随着用户负载的增加,瓶颈会开始出现，首先出现在web服务器层，然后应用服务器层，最后数据库服务器层。</p><p>通常的解决办法就是<strong>向外扩展</strong>，也就是增加web服务器数量。这个方法相对来说简单和廉价，并能够解决问题。然而扩展应用服务器会更复杂，而且成本更高，并且又只是把问题移动到了数据库服务器，那会更复杂，更贵。</p><p>就算你能扩展数据库服务器，你最终会陷入一个金字塔式的情形，在金字塔最下面是web服务器，它会出现最多的问题，但也最好伸缩。金字塔顶部是数据库服务器，问题不多，但最难伸缩。</p></blockquote><p>从上述描述看来，高并发系统需要解决的主要问题在于数据库服务器较差的伸缩性。</p><p>基于空间的架构模型是专门为了<strong>解决伸缩性和并发问题</strong>而设计的。它对于用户数量不可预测且数量级经常变化的情况同样适用。在架构级别来解决这个伸缩性问题通常是比增加服务器数量或者提高缓存技术更好的解决办法。</p><p>主要包括处理单元和虚拟中间件。</p><p>说实话，从这里开始我已经看不懂了。先贴图，日后看懂了再解释吧……</p><ul><li>处理单元</li><li>虚拟中间件<ul><li><strong>消息中间件</strong>（Messaging Grid）：管理用户请求和session，当一个请求进来以后，决定分配给哪一个处理单元。</li><li><strong>数据中间件</strong>（Data Grid）：将数据复制到每一个处理单元，即数据同步。保证某个处理单元都得到同样的数据。</li><li><strong>处理中间件</strong>（Processing Grid）：可选，如果一个请求涉及不同类型的处理单元，该中间件负责协调处理单元</li><li><strong>部署中间件</strong>（Deployment Manager）：负责处理单元的启动和关闭，监控负载和响应时间，当负载增加，就新启动处理单元，负载减少，就关闭处理单元。</li></ul></li></ul><p><img src="/2019/04/18/Software-architecture/5-1.png" alt></p><h3 id="一图流总结"><a href="#一图流总结" class="headerlink" title="一图流总结"></a>一图流总结</h3><p>把所有5种架构的6个维度的特征用图表概括，就是这样：</p><p><img src="/2019/04/18/Software-architecture/a-1.png" alt></p><p>联系其定义，这些特征也比较好理解。</p><p>当然，纸上得来终觉浅，架构的理论没有具体实践经验的支撑，难以深入理解。</p><p>这一篇就属于扫盲文章了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;面试被问到对架构的理解。老实说，我对这一块本来没有深入思考过。&lt;/p&gt;
&lt;p&gt;之前常常集中精力在一些具体理论或者工具上，对架构这一块的了解仅限于感性意识——即，我知道有这样的理论和结构，但是无法清晰地描述它。&lt;/p&gt;
&lt;p&gt;所以借此机会，在网上找了一些资料，了解了一下现代主流的软件架构。&lt;/p&gt;
&lt;p&gt;本文主要基于O’Relly出版的一本介绍软件架构的&lt;a href=&quot;https://www.oreilly.com/programming/free/files/software-architecture-patterns.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;免费小册子&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="interview" scheme="http://riroaki.github.io/categories/interview/"/>
    
    
      <category term="software theory" scheme="http://riroaki.github.io/tags/software-theory/"/>
    
  </entry>
  
  <entry>
    <title>LuckyMoney——红包分配算法</title>
    <link href="http://riroaki.github.io/2019/04/18/LuckyMoney/"/>
    <id>http://riroaki.github.io/2019/04/18/LuckyMoney/</id>
    <published>2019-04-17T19:24:25.000Z</published>
    <updated>2019-04-18T12:58:26.608Z</updated>
    
    <content type="html"><![CDATA[<p>面试被问到这题，记录一下思路～从一开始的复杂算法到后面的简化和优化过程。</p><h3 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h3><ul><li>保证每人有一个金额下限（0.01），以及一个金额上限（占总金额的比重不能超过90%）</li><li>保证结果有2位小数（精确到分），需要考虑浮点数计算误差。</li></ul><a id="more"></a><h3 id="初步思路"><a href="#初步思路" class="headerlink" title="初步思路"></a>初步思路</h3><ol><li>分割法：每次把最大的红包分割成两个小的，进行<code>n-1</code>次操作。<ul><li>注意随机的上界和下界；最后顺序还要shuffle。</li><li>时间复杂度：O(nlogn)​，代码也长，逻辑比较复杂+可读性差。</li></ul></li></ol><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class LuckyMoney {    private static double min = 0.01, maxRate = 0.9;    public static double[] distribute(double total, int n) {        // Corner case: cases with 1 people, or invalid cases.        if (n <= 0 || total <= min * n || maxRate * n < 1)            return new double[]{-1.0};        if (n == 1)            return new double[]{total};        Random rand = new Random();        double[] res = new double[n];        int totalCents = (int) (total * 100), maxCents = (int) (maxRate * totalCents);        // Maximum heap, stores the largest Lucky on the peek.        PriorityQueue<Integer> money = new PriorityQueue<>((o1, o2) -> o2 - o1);        // Split the max Lucky for n - 1 times.        money.offer(totalCents);        for (int i = 0; i < n - 1; ++i) {            int maxMoney = money.poll(), lowerBound = Math.max((int) min * 100, totalCents - (n - i - 1) * maxCents), upperBound = Math.min(maxCents, maxMoney);            int curr = rand.nextInt(upperBound - lowerBound) + lowerBound;            money.offer(curr);            money.offer(maxMoney - curr);        }        // Shuffle the values.        List<Integer> tmpValues = new ArrayList<>(money);        Collections.shuffle(tmpValues);        // Calculate the money.        for (int i = 0; i < n; ++i)            res[i] = tmpValues.get(i) / 100.0;        return res;    }    // This part can also be used for all methods below.    // Check the results.    private static boolean validate(double[] res, double total, int n) {        if (res == null || res.length != n)            return false;        double sum = 0;        for (int i = 0; i < n; ++i)            sum += res[i];        // Check sum.        if (sum - total < 1e-6)            return false;        // Check each.        for (int i = 0; i < n; ++i)            if (res[i] / sum >= maxRate || res[i] < min)                return false;        return true;    }    public static void main(String[] args) {        // test 100.00        double[] test = distribute(100.00, 10);        System.out.println(Arrays.toString(test));        System.out.println(validate(test, 100.00, 10));    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>直接随机求每次的值，定好每次上界和下界即可。<ul><li>结果上看，存在前面分钱多，后面分钱少的问题。可以考虑使用shuffle；</li><li>另外，由于使用的是平均分布的随机函数，所以导致大或者小红包出现概率比较大，比较”不公平”。</li><li>时间复杂度：O(n)</li></ul></li></ol><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class LuckyMoney {    private static double min = 0.01, maxRate = 0.9;    public static double[] distribute(double total, int n) {        int remainCents = (int) (total * 100), minCents = (int) (min * 100), maxCents = (int) (maxRate * remainCents);        double[] res = new double[n];        Random rand = new Random();        for (int i = 0; i < n; ++i) {            int lowerBound = Math.max(minCents, remainCents - (n - i - 1) * maxCents);            int upperBound = Math.min(maxCents, remainCents - (n - i - 1) * minCents);            res[i] = rand.nextInt(upperBound - lowerBound) + lowerBound;            remainCents -= res[i];            res[i] /= 100.0;        }        return res;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="调整思路"><a href="#调整思路" class="headerlink" title="调整思路"></a>调整思路</h3><p>在上面的实现过程中，我遇到了两个问题：</p><ol><li>大小红包出现概率过大，导致不公平；</li><li>前面红包普遍比后面红包大，不够随机。</li></ol><p>经过一些资料查阅，看到一些针对微信红包的测试实验结果：</p><ul><li>在比较靠后抽取的红包金额方差较大，其分布接近指数，容易抽到手气最佳；</li><li>在比较靠前抽取的红包方差较小，平均值差不多。</li></ul><p>此外，我了解到使用正态分布可以使大家的金额数接近平均值，使得大红包和小红包都不容易出现。</p><p>有一些文章推测，微信红包使用的是截尾正态分布算法，即保留部分正态分布图像在指定区间，而区间以外的概率密度为0；当然，为保证所需区间概率密度和为1，需要拔高整体图像，这在公式上通过乘一个系数得到。</p><p>经过一番思考后写下的代码：</p><ol><li>调整平均分布为截尾正态分布，每个人获得的是以当前余额的平均为均值，并明确上下限的截尾正态分布随机获得的值。<ul><li>结果较好，避免了红包金额不公和前后分布不均问题。</li><li>截尾正态分布实现比较粗暴（如果随机结果不在里面就舍弃重来）。</li><li>在正态分布参数的选择上，直接以均值作为方差的开方，不知是否有更好的参数？</li></ul></li></ol><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class LuckyMoney {    private static double min = 0.01, maxRate = 0.9;    public static double[] distribute(double total, int n) {        int remainCents = (int) (total * 100), minCents = (int) (min * 100), maxCents = (int) (maxRate * remainCents);        double[] res = new double[n];        for (int i = 0; i < n; ++i) {            int lowerBound = Math.max(minCents, remainCents - (n - i - 1) * maxCents);            int upperBound = Math.min(maxCents, remainCents - (n - i - 1) * minCents);            int mean = remainCents / (n - i);            res[i] = truncatedNorm(mean, mean, lowerBound, upperBound);            remainCents -= res[i];            res[i] /= 100.0;        }        return res;    }    // A naive implementation of truncated normal distribution.    private static int truncatedNorm(int m, int sqrtv, int lo, int hi) {        if (lo == hi)            return lo;        int res;        Random rand = new Random();        do {            res = (int) (rand.nextGaussian() * sqrtv) + m;        } while (res < lo || res > hi);        return res;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>修改成实时分配（用性能换空间）</li></ol><pre class="line-numbers language-lang-java"><code class="language-lang-java">public class LuckyMoney {    private static double min = 0.01, maxRate = 0.9;    // Index = 1, 2, 3, ..., n    public static double nextMoney(double remain, double total, int index, int n) {        if (index == n)            return remain;        int remainCents = (int) (remain * 100), minCents = (int) (min * 100), maxCents = (int) (total * 100 * maxRate);        int lowerBound = Math.max(minCents, remainCents - (n - index) * maxCents);        int upperBound = Math.min(maxCents, remainCents - (n - index) * minCents);        int mean = remainCents / (n - index + 1);        return truncatedNorm(mean, mean, lowerBound, upperBound) / 100.0;    }    // Truncated normalize distribution function: omitted.    // main function.    public static void main(String[] args) {        double total = 100.0, remain = total;        int n = 10;        for (int i = 0; i < n; ++i) {            double curr = nextMoney(remain, total, i + 1, n);            remain -= curr;            System.out.println(curr);        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol><li>（网传）微信红包算法实现及分析<ul><li>实时分配金额，最小值为0.01，最大值为当前剩余平均值的两倍</li><li>使用红包类，思路清晰；避免了正态计算，时间效率较好。</li><li>当然，这里没有涉及最大红包的数值限制。</li></ul></li></ol><pre class="line-numbers language-lang-java"><code class="language-lang-java">public static double getRandomMoney(LeftMoneyPackage _leftMoneyPackage) {    // remainSize 剩余的红包数量    // remainMoney 剩余的钱    if (_leftMoneyPackage.remainSize == 1) {        _leftMoneyPackage.remainSize--;        return (double) Math.round(_leftMoneyPackage.remainMoney * 100) / 100;    }    Random r     = new Random();    double min   = 0.01; //    double max   = _leftMoneyPackage.remainMoney / _leftMoneyPackage.remainSize * 2;    double money = r.nextDouble() * max;    money = Math.max(money, min);    money = Math.floor(money * 100) / 100;    _leftMoneyPackage.remainSize--;    _leftMoneyPackage.remainMoney -= money;    return money;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="浮点数精度问题"><a href="#浮点数精度问题" class="headerlink" title="浮点数精度问题"></a>浮点数精度问题</h3><ul><li>使用<code>BigDecimal</code>确保精度，商业运算常规做法。具体用法同<code>BigInteger</code>。</li></ul><h3 id="多人抢红包实例"><a href="#多人抢红包实例" class="headerlink" title="多人抢红包实例"></a>多人抢红包实例</h3><ul><li>注意<code>synchronized</code>关键字的使用。</li></ul><pre class="line-numbers language-lang-java"><code class="language-lang-java">import java.util.Random;class LuckyMoney {    int totalSize, remainSize;    double totalMoney, remainMoney, minMoney = 0.01, maxRate = 0.9, maxMoney;    public LuckyMoney(double money, int people) {        totalSize = remainSize = people;        totalMoney = remainMoney = (double) Math.round(money * 100) / 100;        maxMoney = (double) Math.round(totalMoney * maxRate * 100) / 100;    }}public class GetLuckyMoney implements Runnable {    private LuckyMoney money;    private int index;    public GetLuckyMoney(LuckyMoney _money) {        money = _money;    }    public synchronized void run() {        ++index;        System.out.println("Man " + index + " get " + getMoney(money));    }    private double getMoney(LuckyMoney money) {        double res;        if (money.remainSize == 1) {            money.remainSize--;            res = (double) Math.round(money.remainMoney * 100) / 100;            return res;        }        double lowerBound = Math.max(money.minMoney, money.remainMoney - (money.remainSize - 1) * money.maxMoney);        double upperBound = Math.min(money.maxMoney, money.remainMoney - (money.remainSize - 1) * money.minMoney);        double mean = money.remainMoney / money.remainSize;        res = truncatedNorm(mean, mean, lowerBound, upperBound);        res = (double) Math.round(res * 100) / 100;        money.remainSize--;        money.remainMoney -= res;        return res;    }    private double truncatedNorm(double m, double sqrtV, double lo, double hi) {        if (lo == hi)            return lo;        double res;        Random rand = new Random();        do {            res = rand.nextGaussian() * sqrtV + m;        } while (res < lo || res > hi);        return res;    }    public static void main(String[] args) {        LuckyMoney money = new LuckyMoney(100.0, 10);        System.out.println("Start to distribute " + money.totalMoney + " to " + money.totalSize + " people.");        GetLuckyMoney test = new GetLuckyMoney(money);        for (int i = 0; i < 10; ++i)            new Thread(test).start();    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出：</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">Start to distribute 100.0 to 10 people.Man 1 get 7.58Man 2 get 3.19Man 3 get 16.73Man 4 get 11.79Man 5 get 6.53Man 6 get 7.53Man 7 get 13.45Man 8 get 1.92Man 9 get 3.62Man 10 get 27.66<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="关于提高代码可读性的建议"><a href="#关于提高代码可读性的建议" class="headerlink" title="关于提高代码可读性的建议"></a>关于提高代码可读性的建议</h3><ul><li>避免一行一注释，减少注释冗余。</li><li>命名方案一致。</li><li>不重复写相同代码。</li><li>避免层数过深的逻辑。</li><li>限制每行长度。</li><li>一致的临时变量命名。比如都用<code>i</code>。</li><li>加入块注释，例如：</li></ul><pre class="line-numbers language-lang-java"><code class="language-lang-java">    /**     * Return a truncated normal distribution N(m, v) between [lo, hi].     * @param m:            mean of normal distribution     * @param sqrtV:    square of variance of normal distribution     * @param lo:            lower bound of truncation     * @param hi:         upper bound of truncation     * @return round value of truncated normal distribution.     */    private static int truncatedNorm(int m, int sqrtV, int lo, int hi) {        if (lo == hi)            return lo;        int res;        Random rand = new Random();        do {            res = (int)(rand.nextGaussian() * sqrtV);        } while (res < lo || res > hi);        return res;    }// ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;面试被问到这题，记录一下思路～从一开始的复杂算法到后面的简化和优化过程。&lt;/p&gt;
&lt;h3 id=&quot;要点&quot;&gt;&lt;a href=&quot;#要点&quot; class=&quot;headerlink&quot; title=&quot;要点&quot;&gt;&lt;/a&gt;要点&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;保证每人有一个金额下限（0.01），以及一个金额上限（占总金额的比重不能超过90%）&lt;/li&gt;
&lt;li&gt;保证结果有2位小数（精确到分），需要考虑浮点数计算误差。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="interview" scheme="http://riroaki.github.io/categories/interview/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="random" scheme="http://riroaki.github.io/tags/random/"/>
    
  </entry>
  
  <entry>
    <title>19届图森杯划水经历</title>
    <link href="http://riroaki.github.io/2019/04/15/19%E5%B1%8A%E5%9B%BE%E6%A3%AE%E6%9D%AF%E5%88%92%E6%B0%B4%E7%BB%8F%E5%8E%86/"/>
    <id>http://riroaki.github.io/2019/04/15/19届图森杯划水经历/</id>
    <published>2019-04-14T19:11:54.000Z</published>
    <updated>2019-04-15T17:48:55.791Z</updated>
    
    <content type="html"><![CDATA[<p>⚠️流水账警告⚠️</p><p>因为大三时间比较自由，我和两个关系比较好的同学报了图森杯（acm校赛选拔），然后因为没有准备所以变成了丢人现场……另外两个同学不想去，最后变成我一个人单刷(´･ω･`)喵喵喵？</p><p>不过单刷也有单刷的好处嘛，至少我划水不用有心里负担。体验体验这种感觉也是不错滴。</p><p>于是我愉快地睡过了上午的签到……（其实是前一天<s>打铁</s>只狼太晚了，唉破罐子破摔）好在让一个学长帮忙签到了，除了没有试机以外问题不大。</p><p>于是下午我佛系地到考场，结果还遇到些麻烦——平时刷oj用的都是java，intelliJ美滋滋，但是在这里只有eclipse能用，机房的intellij竟然连java环境都没有配好。想了想写c++还要用dev-c++那种手感，还是拿eclipse凑活。</p><p>（事实证明c++才是刷oj的王道啊）</p><h3 id="以下正式开始做题："><a href="#以下正式开始做题：" class="headerlink" title="以下正式开始做题："></a>以下正式开始做题：</h3><p>我慢悠悠地调试了一会，感觉ok了，于是看了一下提交界面，找了一下最简单的题——怎么看呢，就是提交人数最多的题——然后看了下是题E：portion（<a href="http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5973），嗯没什么好说的，正经签到题，写完就ac。其实我更好奇一些单词的意思，奈何机房的win7没法像mac一样查字典。" target="_blank" rel="noopener">http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5973），嗯没什么好说的，正经签到题，写完就ac。其实我更好奇一些单词的意思，奈何机房的win7没法像mac一样查字典。</a></p><p>接下来嘛，继续看大家做什么，我就跟着做什么。接下来的题也没啥意思，甚至很迷：</p><p>J题：Extended Twin Composite Number（<a href="http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5977），讲的是给定整数n，要求两个合数x和y，x+n=y。第一反应，让x=2*n就行了呗，一看x和y范围是1e18，n范围是1e9，这么做应该没问题。半信半疑地提交了，这么简单？？" target="_blank" rel="noopener">http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5977），讲的是给定整数n，要求两个合数x和y，x+n=y。第一反应，让x=2*n就行了呗，一看x和y范围是1e18，n范围是1e9，这么做应该没问题。半信半疑地提交了，这么简单？？</a></p><p>结果wa。我就纳闷了，什么？这难道不对嘛？立刻回去找答案看是不是我忽略了什么要求，看了一下sample，难道x要比n小？也没说呀。</p><p>emmm想了一会之后，决定先不管它，去做另一个热度很高的题：</p><p>G题：Postman（<a href="http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5975）。这题讲的是邮差在数轴上走，从邮局（坐标0）出发，到指定n个目的地送信的最短距离。每次最多携带k封信。" target="_blank" rel="noopener">http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5975）。这题讲的是邮差在数轴上走，从邮局（坐标0）出发，到指定n个目的地送信的最短距离。每次最多携带k封信。</a></p><p>这题真心不难。看了一下样例就心里有数，首先排序目的地，把正负分开算，然后从每一极向0分割出送信个数为k的区间，算作一次送信（即从绝对值大的一侧往里数k个而不是反过来），每个块最大的距离乘2就是这一个块送信的距离。</p><p>然后最后结果减去最大的那一极的绝对值，因为最后不需要回到邮局所以走到最远的地方就不用回来了，把刚才考虑的来回去掉一半。</p><p>满心欢喜地提交想着这下也差不多算不亏了，结果TLE。(´･ω･`)喵喵喵？</p><p>不会吧？排个序就超时？难道目标算法是线性的？可我看那么多人都过了这题呀……</p><h3 id="可达鸭眉头一皱发现事情并不简单。"><a href="#可达鸭眉头一皱发现事情并不简单。" class="headerlink" title="可达鸭眉头一皱发现事情并不简单。"></a>可达鸭眉头一皱发现事情并不简单。</h3><p>苦思冥想没有线性方案，我开始慌了。周围环境十分吵闹，开场已经一个多小时，我一度有退场的打算——太丢人了，我现在也才一个气球。</p><p>对面的小哥也是单刷，拿到一个气球之后默默地离开了。可能也是来体验一下的吧……</p><p>我又刷新了一下排行榜，没怎么变，做A题的人稍有增加。粗粗一看是二分图匹配，打扰了打扰了。匈牙利算法没背过呀，况且也不知道怎么套求权函数，不然大概要用dp？我只想到dfs+回溯的方法，怕是和八皇后一样复杂。害怕TLE就没做下去……也许这是本场比赛最大的失误吧。</p><p>转了一圈还是想从这两个简单题入手。</p><p>回到J题，我心里想的是可能真的有限制吧，虽然说是1e18但是限制1e9这样子，所以wa了？</p><p>那我不妨优化一下。</p><p>其实，如果n是偶数，那么我只要也取偶数，很容易就能达到目标。所以取x=4，y=n+4。</p><p>那么……奇数？</p><p>奇数不妨凑3的倍数？对三取余，剩下0那我就用6加，剩下1我就用8加，剩下2我就用4加，这样y肯定是三的倍数。</p><p>这样一写一提交，竟然ac了！这个是个振奋的信号，让我恢复精神继续怼G题了。</p><p>这个时候大概两个小时过去了。</p><p>然而，我接下来不管怎么怼G，始终发现：</p><h3 id="它就是没法改进了。"><a href="#它就是没法改进了。" class="headerlink" title="它就是没法改进了。"></a>它就是没法改进了。</h3><p>然而始终是TLE，心态又不好了。这个时候，周围有人的讨论、欢呼，此起彼伏的键盘声，以及换座位的响动，无一不在干扰心神。我佛了。。只好转战其他题目。</p><p>转了半天，发现这个B题：Even Number Theory（<a href="http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5970）还有点思路，于是开始写数字找规律。" target="_blank" rel="noopener">http://acm.zju.edu.cn/onlinejudge/showContestProblem.do?problemId=5970）还有点思路，于是开始写数字找规律。</a></p><p>问题是求偶数n的双阶乘的偶质数分解——偶质数分解是我自己口胡的词，就是一个由2和某奇数相乘的到的数——这样的最大分解有几个。n可达1e1000，看来肯定是数学优化问题，估计有公式。</p><p>双阶乘进行质因数分解，自然就是所有不大于它的偶数的偶质数因子个数相加咯。</p><p>我写了一下，找到初步规律：</p><p>偶数序列：2，4，6，8，10，12，14，16，18，20，22，24，26，28，30，……</p><p>分解个数：1，2，1，3，1，2，1，4，1，2，1，3，1，2，1，……</p><p>我立刻看到一个循环节，12131214……没多想就写了一个简单公式，每16就增加（1+2+1+3+1+2+1+4）个数，而不足16再加单独的和。</p><p>然后因为是1e1000，所以用BigInteger类来操作。幸运的是在code jam早就接触了这个类所以遇到的时候自然而然就想到了。</p><p>一切都看起来很完美，于是提交，wa！</p><p>我有些困惑，于是继续写数字——哇，32对应的不是4而是5诶（16对应的是4，而32=2*16，对所以应5），那就不是循环了。有些挫败的我安慰自己说，没关系反正提交次数不太重要，做出来就可以了。</p><p>于是我开始找规律。结果这个规律还真不明显……循环又不是，但是说它增长有规律我也是不信的。</p><p>干脆采取O(n)递推？可以，但是这么做……你确定不会爆栈？？时间复杂度肯定也会超的吧……</p><p>绝望……</p><p>后来又切了几道，然并卵。</p><p>就这样僵持到了结束也没有A出下一题……</p><h3 id="我好菜呀……"><a href="#我好菜呀……" class="headerlink" title="我好菜呀……"></a>我好菜呀……</h3><p>事后和咕咕的队友讨论了一下，队友拿着逻辑完全一致，但是用cpp写的代码一次就ac了，啥玩意……</p><p>有人说，java读输入要用这个：</p><p><code>Scanner sc = new Scanner (new BufferedInputStream(System.in));</code></p><p>然而当我想起要测试一下的时候，已经out of contest time了，于是TLE变成了永远的谜……</p><p>啥都别说了，我也太菜了……这样下去怎么打code jam呀……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;⚠️流水账警告⚠️&lt;/p&gt;
&lt;p&gt;因为大三时间比较自由，我和两个关系比较好的同学报了图森杯（acm校赛选拔），然后因为没有准备所以变成了丢人现场……另外两个同学不想去，最后变成我一个人单刷(´･ω･`)喵喵喵？&lt;/p&gt;
&lt;p&gt;不过单刷也有单刷的好处嘛，至少我划水不用有心里负
      
    
    </summary>
    
      <category term="competition" scheme="http://riroaki.github.io/categories/competition/"/>
    
    
  </entry>
  
  <entry>
    <title>bash自动化部署脚本</title>
    <link href="http://riroaki.github.io/2019/04/11/bash%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2%E8%84%9A%E6%9C%AC/"/>
    <id>http://riroaki.github.io/2019/04/11/bash自动化部署脚本/</id>
    <published>2019-04-11T07:33:43.000Z</published>
    <updated>2019-04-13T09:17:48.359Z</updated>
    
    <content type="html"><![CDATA[<p>这是18年部署战争游戏时，写自动化部署脚本的一点经验。</p><p>感觉<code>shell</code>写起来和<code>python</code>有点像……</p><h3 id="登录服务器"><a href="#登录服务器" class="headerlink" title="登录服务器"></a>登录服务器</h3><p>开头需要指明使用<code>expect</code>执行脚本，因为这些指令不是<code>bash</code>自带的；</p><p>使用的是<code>spawn</code>指令发送<code>ssh</code>连接请求，然后用<code>expect</code>表示等待对方输出后再做反馈，是一个灵活的指令；</p><pre class="line-numbers language-lang-bash"><code class="language-lang-bash">#! /usr/bin/expect# 如果不想明文写在脚本里，可以设置为命令行参数set ipaddr [lindex $argv 0];set username [lindex $argv 1];set password [lindex $argv 2];set timeout 30 # 超时等待时间set prompt "#" # 这里是登陆成功的输出，linux简单用#匹配最后一个字符就可以spawn ssh $username@$ipaddrexpect {    "yes/no" { send "yes\r"; exp_continue } # 继续执行expect语句    "*assword" { send "$password\r" } # 密码输入提示，忽略p大小写    "Permission denied*" { exit } # 密码错误    timeout { puts "Timed out during login"; exit }}expect "$prompt"interact # 交还控制权<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>进入服务器后执行指令要用<code>send</code>指令。</p><p>结束时，如果把权限交还用户则需要用<code>interact</code>。</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># ...# 结束连接send "exit\r"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="文件指令"><a href="#文件指令" class="headerlink" title="文件指令"></a>文件指令</h3><p>文件主要使用<code>scp</code>指令进行本地和远程的传输。它是<code>seccure copy</code>的缩写，和<code>cp</code>比较区别在于，前者可以远程传输，而且是加密的；后者只能本机移动。</p><p>以文件上传到远程服务器为例，展示具体使用格式（下一步要输入密码）：</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell"># 复制单个文件scp $file $username@$ipaddr:$remote_folder# 递归复制整个目录scp -r $folder $username@$ipaddr:$remote_folder<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>在脚本中，我们使用<code>expect</code>模块执行<code>scp</code>指令：</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">#! /usr/bin/expect# 复制文件spawn scp $file $username@$ipaddr:$remote_folderexpect {  "yes/no" { send "yes\r"; exp_continue }  "*assword" { send "$password\r" }  timeout { exit }}# 等待传输结束expect "100%"expect eof<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>对文件夹，只需要把上面<code>spawn scp $file $username…</code>改成<code>spawn scp -r $folder $username…</code>即可。</p><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><p>需要提前在服务器修改nginx配置，并配置好本地python环境（gunicorn+flask等等）。</p><p>这个没什么好说的，常规启动服务即可。</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">send "kill -9 $(pidof gunicorn)"send "nginx -s restart && nohup gunicorn -w 1 -b 127.0.0.1:8080 main:app -D"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>有关部署的细节，详见<a href="https://riroaki.github.io/2019/03/28/Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%EF%BC%9AFlask-gunicorn-nginx/">另一篇文章</a>。</p><p>总体代码：</p><pre class="line-numbers language-lang-shell"><code class="language-lang-shell">#! /usr/bin/expect# 如果不想明文写在脚本里，可以设置为命令行参数set ipaddr [lindex $argv 0];set username [lindex $argv 1];set password [lindex $argv 2];set folder "./war"set remote_folder "~/war"set timeout 30# 复制文件spawn scp $file $username@$ipaddr:$remote_folderexpect {  "yes/no" { send "yes\r"; exp_continue }  "*assword" { send "$password\r" }  timeout { exit }}# 等待传输结束expect "100%"expect eofspawn ssh $username@$ipaddrexpect {    "yes/no" { send "yes\r"; exp_continue } # 继续执行expect语句    "*assword" { send "$password\r" } # 密码输入提示，忽略p大小写    "Permission denied*" { exit } # 密码错误    timeout { puts "Timed out during login"; exit }}expect "#"# 运行send "kill -9 $(pidof gunicorn)\r"send "nginx -s restart && cd ~/war/ && nohup gunicorn -w 1 -b 127.0.0.1:8080 main:app -D\r"<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是18年部署战争游戏时，写自动化部署脚本的一点经验。&lt;/p&gt;
&lt;p&gt;感觉&lt;code&gt;shell&lt;/code&gt;写起来和&lt;code&gt;python&lt;/code&gt;有点像……&lt;/p&gt;
&lt;h3 id=&quot;登录服务器&quot;&gt;&lt;a href=&quot;#登录服务器&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="back end" scheme="http://riroaki.github.io/categories/back-end/"/>
    
    
      <category term="shell" scheme="http://riroaki.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>Easy Painter-iOS绘图实例</title>
    <link href="http://riroaki.github.io/2019/04/11/EasyPainter-iOS%E7%BB%98%E5%9B%BE%E5%AE%9E%E4%BE%8B/"/>
    <id>http://riroaki.github.io/2019/04/11/EasyPainter-iOS绘图实例/</id>
    <published>2019-04-10T19:46:41.000Z</published>
    <updated>2019-04-18T13:00:45.188Z</updated>
    
    <content type="html"><![CDATA[<p>这是去年刚接触iOS编程的时候，入门做的一点小项目——第一个是一个支持多种颜色的画板，第二个是玩纪念碑谷2之后，模仿每一关最后的部分实现的画画功能。</p><a id="more"></a><p>先看一下效果图：</p><h3 id="画板"><a href="#画板" class="headerlink" title="画板"></a>画板</h3><p>可以实现多种颜色和阴影，清空画版、分享绘画等等。</p><p><img src="/2019/04/11/EasyPainter-iOS绘图实例/board.JPG" alt></p><h3 id="纪念碑谷"><a href="#纪念碑谷" class="headerlink" title="纪念碑谷"></a>纪念碑谷</h3><p>很直观的效果，画笔有多种样式：</p><ul><li>直接绘制，表现为对称的白色线条（这是纪念碑谷中的效果——除了没有发光以外）</li></ul><p><img src="/2019/04/11/EasyPainter-iOS绘图实例/white.png" alt></p><ul><li>后来觉得线条太单调，于是增加了绘制树叶</li></ul><p><img src="/2019/04/11/EasyPainter-iOS绘图实例/tree.png" alt></p><ul><li>我又加入了多种颜色～</li></ul><p><img src="/2019/04/11/EasyPainter-iOS绘图实例/colorful.png" alt></p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>占坑，待补充。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是去年刚接触iOS编程的时候，入门做的一点小项目——第一个是一个支持多种颜色的画板，第二个是玩纪念碑谷2之后，模仿每一关最后的部分实现的画画功能。&lt;/p&gt;
    
    </summary>
    
      <category term="front end" scheme="http://riroaki.github.io/categories/front-end/"/>
    
    
      <category term="iOS" scheme="http://riroaki.github.io/tags/iOS/"/>
    
  </entry>
  
  <entry>
    <title>NLP学习日记1——Set Expan</title>
    <link href="http://riroaki.github.io/2019/04/11/NLP-1-Set-Expan/"/>
    <id>http://riroaki.github.io/2019/04/11/NLP-1-Set-Expan/</id>
    <published>2019-04-10T19:02:10.000Z</published>
    <updated>2019-04-20T15:52:52.805Z</updated>
    
    <content type="html"><![CDATA[<p>说来惭愧，其实这是上个学期初实验室的老师布置下来的任务，直到今天才开始认真接触。</p><p>NLP应当是未来大势之一——在视觉、语音等方面已经能够做到炉火纯青的ML和DL，在NLP领域进展却比较慢，可以说是在日渐饱和的AI领域为数不多的未被探索完全的领域了。</p><p>词集拓展的概念和做法来自一篇论文：《SetExpan- Corpus-Based Set Expansion via Context Feature Selection and Rank Ensemble》，指的是根据一些词集种子，从语料库中摘取同一类的词语。作者采用的是英文词语，源代码在<a href="https://github.com/mickeystroller/SetExpan" target="_blank" rel="noopener">这里</a>。</p><p>这个方法说来简单，但是实际做起来并不容易。简单的上下文提取和匹配如果做不好，很容易出现语义漂移的现象，即同一个上下文抽取的词却不一样。</p><p>此外，中文本身的分词难度也为项目带来困难。</p><p>而且分类是重要的一步，为之后建立知识图谱、挖掘联系等等都具有基础性意义。</p><p>那么首先我们来分析一下论文讲的方法。</p><h2 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h2><p>所谓词集拓展，就是将一个小的词语集合（如几个国家名称），从语料库拓展到文章中所有的国家名称。</p><p>目前现有的基于小初始集的无监督词集拓展主要包括两种：</p><ol><li>基于和已有实体的分布相似性，一次性提取全部实体<ul><li>盲目地使用所有这些特征可能将不期望的实体引入扩展集合中，因为许多上下文特征不代表用于定义目标语义类，尽管它们确实与一些种子实体具有连接。比如国家和地区都可以接在“位于”之后，容易混为一谈。</li></ul></li><li>迭代性提取模式，用每次带来的新模式进行提取，不断加入词集<ul><li>依靠每次迭代得到的词集质量。由于文本的非结构化以及噪声存在，很难有完美的评分机制。</li><li>需要注意：实体入侵（实体质量不佳）、语义漂移（模式质量不佳）</li></ul></li></ol><p>还有一种取巧的办法，Google Set，即每次从谷歌搜索引擎中搜索关键词，然后提取前面几页搜索结果的关联关键词。这种办法质量不错但是需要在线连接，成本较高。在这里我们实现一种离线的词集拓展。</p><p>这里需要描述的工作是：</p><ol><li>一种迭代集扩展框架，采用新颖的上下文特征选择方法，处理实体入侵和语义漂移问题; </li><li>一种基于无监督排序的集成算法，用于实体选择，使我们的系统具有鲁棒性，进一步降低了语义漂移的影响。</li></ol><p>好，开始描述具体过程：</p><h3 id="模式建立"><a href="#模式建立" class="headerlink" title="模式建立"></a>模式建立</h3><p>针对单词集中的每个单词构建一个模式。</p><h3 id="模式评估"><a href="#模式评估" class="headerlink" title="模式评估"></a>模式评估</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说来惭愧，其实这是上个学期初实验室的老师布置下来的任务，直到今天才开始认真接触。&lt;/p&gt;
&lt;p&gt;NLP应当是未来大势之一——在视觉、语音等方面已经能够做到炉火纯青的ML和DL，在NLP领域进展却比较慢，可以说是在日渐饱和的AI领域为数不多的未被探索完全的领域了。&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="machine learning" scheme="http://riroaki.github.io/categories/machine-learning/"/>
    
    
      <category term="NLP" scheme="http://riroaki.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>41.First Missing Positive</title>
    <link href="http://riroaki.github.io/2019/04/11/41-First-Missing-Positive/"/>
    <id>http://riroaki.github.io/2019/04/11/41-First-Missing-Positive/</id>
    <published>2019-04-10T18:36:26.000Z</published>
    <updated>2019-04-19T12:13:30.438Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>Given an unsorted integer array, find the smallest missing positive integer.</p><a id="more"></a><p><strong>Example 1:</strong></p><pre><code>Input: [1,2,0]Output: 3</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: [3,4,-1,1]Output: 2</code></pre><p><strong>Example 3:</strong></p><pre><code>Input: [7,8,9,11,12]Output: 1</code></pre><p><strong>Note:</strong></p><p>Your algorithm should run in <em>O</em>(<em>n</em>) time and uses constant extra space.</p><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h4 id="联想merge-interval"><a href="#联想merge-interval" class="headerlink" title="联想merge interval"></a>联想merge interval</h4><p>虽然不是常数空间的算法，但是时间复杂度是线性的。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">class Solution {    class Interval {        int left, right;        Interval(int l, int r) {            left = l;            right = r;        }    }    public int firstMissingPositive(int[] nums) {        Arrays.sort(nums);        List<Interval> intervalList = new ArrayList<>();        for (int num : nums) {            if (num <= 0)                continue;            if (intervalList.isEmpty())                intervalList.add(new Interval(num, num));            else {                Interval tmp = intervalList.get(intervalList.size() - 1);                if (tmp.right + 1 == num) {                    tmp.right++;                    intervalList.set(intervalList.size() - 1, tmp);                } else if (tmp.right + 1 < num)                    break;            }        }        if (intervalList.isEmpty() || intervalList.get(0).left > 1)            return 1;        return intervalList.get(intervalList.size() - 1).right + 1;    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(n)$</li><li>空间复杂度：$O(n)$</li></ul><h4 id="常数空间的做法：交换"><a href="#常数空间的做法：交换" class="headerlink" title="常数空间的做法：交换"></a>常数空间的做法：交换</h4><p>首先，总共有n个数 ，那么缺失的数最大也是<code>n+1</code>，不然就是<code>1-n</code>中间的数。</p><p>假如我们让所有数归位（即，尽量让所有的数满足<code>a[i] = i + 1</code>）</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public int firstMissingPositive(int[] nums) {    int n = nums.length, res = 1;    for (int i = 0; i < n; ++i) {          // 注意这个是循环不是一次判断！          // 最后一个判断条件是必要的，在[1, 1]情况下就会死循环        while(nums[i] > 0 && nums[i] <= n               && nums[nums[i] - 1] != nums[i]) {            int tmp = nums[nums[i] - 1];            nums[nums[i] - 1] = nums[i];            nums[i] = tmp;        }    }    for (int i = 0; i < n; ++i)        if (nums[i] != i + 1)            return i + 1;    return n + 1;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(n)$，别看是两层循环，但是每一次交换都能够保证至少一个数归位，所以总共最多只需要交换n次。</li><li>空间复杂度：$O(1)$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;Given an unsorted integer array, find the smallest missing positive integer.&lt;/p&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="interval" scheme="http://riroaki.github.io/tags/interval/"/>
    
  </entry>
  
  <entry>
    <title>40.Combination Sum II</title>
    <link href="http://riroaki.github.io/2019/04/11/40-Combination-Sum-II/"/>
    <id>http://riroaki.github.io/2019/04/11/40-Combination-Sum-II/</id>
    <published>2019-04-10T18:28:44.000Z</published>
    <updated>2019-04-10T18:35:46.336Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>Given a collection of candidate numbers (<code>candidates</code>) and a target number (<code>target</code>), find all unique combinations in <code>candidates</code> where the candidate numbers sums to <code>target</code>.</p><p>Each number in <code>candidates</code> may only be used <strong>once</strong> in the combination.</p><p><strong>Note:</strong></p><ul><li>All numbers (including <code>target</code>) will be positive integers.</li><li>The solution set must not contain duplicate combinations.</li></ul><a id="more"></a><p><strong>Example 1:</strong></p><pre><code>Input: candidates = [10,1,2,7,6,1,5], target = 8,A solution set is:[  [1, 7],  [1, 2, 5],  [2, 6],  [1, 1, 6]]</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: candidates = [2,5,2,1,2], target = 5,A solution set is:[  [1,2,2],  [5]]</code></pre><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h4 id="回溯-dfs"><a href="#回溯-dfs" class="headerlink" title="回溯+dfs"></a>回溯+dfs</h4><p>和上题类似但是允许重复元素使用。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">class Solution {    private List<List<Integer>> res;    public List<List<Integer>> combinationSum2(int[] nums, int target) {        Arrays.sort(nums);        res = new ArrayList<>();        backTrack(new ArrayList<>(), nums, target, 0);        return res;    }    private void backTrack(List<Integer> curr, int[] nums, int target, int index) {        if (target < 0)            return;        if (target == 0) {            res.add(new ArrayList<>(curr));            return;        }        for (int i = index; i < nums.length; i++) {            if (i > index && nums[i] == nums[i - 1])                continue;            curr.add(nums[i]);            backTrack(curr, nums, target - nums[i], i + 1);            curr.remove(curr.size() - 1);        }    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(n^{m/avg(n)})$</li><li>空间复杂度：$O(mn^{m/avg(n)})$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;Given a collection of candidate numbers (&lt;code&gt;candidates&lt;/code&gt;) and a target number (&lt;code&gt;target&lt;/code&gt;), find all unique combinations in &lt;code&gt;candidates&lt;/code&gt; where the candidate numbers sums to &lt;code&gt;target&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Each number in &lt;code&gt;candidates&lt;/code&gt; may only be used &lt;strong&gt;once&lt;/strong&gt; in the combination.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All numbers (including &lt;code&gt;target&lt;/code&gt;) will be positive integers.&lt;/li&gt;
&lt;li&gt;The solution set must not contain duplicate combinations.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="backtracking" scheme="http://riroaki.github.io/tags/backtracking/"/>
    
      <category term="dfs" scheme="http://riroaki.github.io/tags/dfs/"/>
    
  </entry>
  
  <entry>
    <title>39.Combination Sum</title>
    <link href="http://riroaki.github.io/2019/04/11/39-Combination-Sum/"/>
    <id>http://riroaki.github.io/2019/04/11/39-Combination-Sum/</id>
    <published>2019-04-10T18:23:04.000Z</published>
    <updated>2019-04-10T18:28:14.727Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>Given a <strong>set</strong> of candidate numbers (<code>candidates</code>) <strong>(without duplicates)</strong> and a target number (<code>target</code>), find all unique combinations in <code>candidates</code> where the candidate numbers sums to <code>target</code>.</p><p>The <strong>same</strong> repeated number may be chosen from <code>candidates</code> unlimited number of times.</p><p><strong>Note:</strong></p><ul><li>All numbers (including <code>target</code>) will be positive integers.</li><li>The solution set must not contain duplicate combinations.</li></ul><a id="more"></a><p><strong>Example 1:</strong></p><pre><code>Input: candidates = [2,3,6,7], target = 7,A solution set is:[  [7],  [2,2,3]]</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: candidates = [2,3,5], target = 8,A solution set is:[  [2,2,2,2],  [2,3,3],  [3,5]]</code></pre><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h4 id="回溯-dfs"><a href="#回溯-dfs" class="headerlink" title="回溯+dfs"></a>回溯+dfs</h4><p>理论上有2的n种可能；</p><p>排序保证unique。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public List<List<Integer>> combinationSum(int[] candidates, int target) {    if (candidates == null || candidates.length == 0)          return new ArrayList<>();    return helper(candidates, target, 0);}private List<List<Integer>> helper(int[] candidates, int target, int index) {    List<List<Integer>> res = new ArrayList<>();    if (target < 0)          return res;    for (int i = index; i < candidates.length; i++) {        if (candidates[i] == target) {            List<Integer> tmp = new ArrayList<>();            tmp.add(candidates[i]);            res.add(tmp);        } else if (candidates[i] < target) {            List<List<Integer>> tmp = helper(candidates, target - candidates[i], i);            if (tmp.size() > 0) {                for (List<Integer> vec : tmp)                      vec.add(candidates[i]);                res.addAll(tmp);            }        }    }    return res;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(2^n)$</li><li>空间复杂度：$O(2^n)$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;Given a &lt;strong&gt;set&lt;/strong&gt; of candidate numbers (&lt;code&gt;candidates&lt;/code&gt;) &lt;strong&gt;(without duplicates)&lt;/strong&gt; and a target number (&lt;code&gt;target&lt;/code&gt;), find all unique combinations in &lt;code&gt;candidates&lt;/code&gt; where the candidate numbers sums to &lt;code&gt;target&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;same&lt;/strong&gt; repeated number may be chosen from &lt;code&gt;candidates&lt;/code&gt; unlimited number of times.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All numbers (including &lt;code&gt;target&lt;/code&gt;) will be positive integers.&lt;/li&gt;
&lt;li&gt;The solution set must not contain duplicate combinations.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="backtracking" scheme="http://riroaki.github.io/tags/backtracking/"/>
    
      <category term="dfs" scheme="http://riroaki.github.io/tags/dfs/"/>
    
  </entry>
  
  <entry>
    <title>38.Count and Say</title>
    <link href="http://riroaki.github.io/2019/04/11/38-Count-and-Say/"/>
    <id>http://riroaki.github.io/2019/04/11/38-Count-and-Say/</id>
    <published>2019-04-10T18:18:58.000Z</published>
    <updated>2019-04-10T18:22:06.002Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>The count-and-say sequence is the sequence of integers with the first five terms as following:</p><pre><code>1.     12.     113.     214.     12115.     111221</code></pre><p><code>1</code> is read off as <code>&quot;one 1&quot;</code> or <code>11</code>.<br><code>11</code> is read off as <code>&quot;two 1s&quot;</code> or <code>21</code>.<br><code>21</code> is read off as <code>&quot;one 2</code>, then <code>one 1&quot;</code> or <code>1211</code>.</p><p>Given an integer <em>n</em> where 1 ≤ <em>n</em> ≤ 30, generate the <em>n</em>th term of the count-and-say sequence.</p><p>Note: Each term of the sequence of integers will be represented as a string.</p> <a id="more"></a><p><strong>Example 1:</strong></p><pre><code>Input: 1Output: &quot;1&quot;</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: 4Output: &quot;1211&quot;</code></pre><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h4 id="常规做法"><a href="#常规做法" class="headerlink" title="常规做法"></a>常规做法</h4><pre class="line-numbers language-lang-java"><code class="language-lang-java">public String countAndSay(int n) {    String s = "1";    for(int i=1;i<n;i++) {        StringBuilder sb = new StringBuilder();        for(int j=0;j<s.length();j++) {            char count = '1';            while(j+1<s.length() && s.charAt(j)==s.charAt(j+1)) {                count++;                j++;            }            sb.append(count);            sb.append(s.charAt(j));        }        s = sb.toString();        // System.out.println(s);    }    return s;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(n^2)$</li><li>空间复杂度：$O(n)$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;The count-and-say sequence is the sequence of integers with the first five terms as following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1.     1
2.     11
3.     21
4.     1211
5.     111221
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;1&lt;/code&gt; is read off as &lt;code&gt;&amp;quot;one 1&amp;quot;&lt;/code&gt; or &lt;code&gt;11&lt;/code&gt;.&lt;br&gt;&lt;code&gt;11&lt;/code&gt; is read off as &lt;code&gt;&amp;quot;two 1s&amp;quot;&lt;/code&gt; or &lt;code&gt;21&lt;/code&gt;.&lt;br&gt;&lt;code&gt;21&lt;/code&gt; is read off as &lt;code&gt;&amp;quot;one 2&lt;/code&gt;, then &lt;code&gt;one 1&amp;quot;&lt;/code&gt; or &lt;code&gt;1211&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Given an integer &lt;em&gt;n&lt;/em&gt; where 1 ≤ &lt;em&gt;n&lt;/em&gt; ≤ 30, generate the &lt;em&gt;n&lt;/em&gt;th term of the count-and-say sequence.&lt;/p&gt;
&lt;p&gt;Note: Each term of the sequence of integers will be represented as a string.&lt;/p&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="easy" scheme="http://riroaki.github.io/tags/easy/"/>
    
      <category term="string" scheme="http://riroaki.github.io/tags/string/"/>
    
  </entry>
  
  <entry>
    <title>36.Valid Sudoku</title>
    <link href="http://riroaki.github.io/2019/04/11/36-Valid-Sudoku/"/>
    <id>http://riroaki.github.io/2019/04/11/36-Valid-Sudoku/</id>
    <published>2019-04-10T18:13:34.000Z</published>
    <updated>2019-04-10T18:17:32.351Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>Determine if a 9x9 Sudoku board is valid. Only the filled cells need to be validated <strong>according to the following rules</strong>:</p><ol><li>Each row must contain the digits <code>1-9</code> without repetition.</li><li>Each column must contain the digits <code>1-9</code> without repetition.</li><li>Each of the 9 <code>3x3</code> sub-boxes of the grid must contain the digits <code>1-9</code> without repetition.</li></ol><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Sudoku-by-L2G-20050714.svg/250px-Sudoku-by-L2G-20050714.svg.png" alt="img"><br>A partially filled sudoku which is valid.</p><p>The Sudoku board could be partially filled, where empty cells are filled with the character <code>&#39;.&#39;</code>.</p><a id="more"></a><p><strong>Example 1:</strong></p><pre><code>Input:[  [&quot;5&quot;,&quot;3&quot;,&quot;.&quot;,&quot;.&quot;,&quot;7&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;],  [&quot;6&quot;,&quot;.&quot;,&quot;.&quot;,&quot;1&quot;,&quot;9&quot;,&quot;5&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;],  [&quot;.&quot;,&quot;9&quot;,&quot;8&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;6&quot;,&quot;.&quot;],  [&quot;8&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;6&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;3&quot;],  [&quot;4&quot;,&quot;.&quot;,&quot;.&quot;,&quot;8&quot;,&quot;.&quot;,&quot;3&quot;,&quot;.&quot;,&quot;.&quot;,&quot;1&quot;],  [&quot;7&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;2&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;6&quot;],  [&quot;.&quot;,&quot;6&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;2&quot;,&quot;8&quot;,&quot;.&quot;],  [&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;4&quot;,&quot;1&quot;,&quot;9&quot;,&quot;.&quot;,&quot;.&quot;,&quot;5&quot;],  [&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;8&quot;,&quot;.&quot;,&quot;.&quot;,&quot;7&quot;,&quot;9&quot;]]Output: true</code></pre><p><strong>Example 2:</strong></p><pre><code>Input:[  [&quot;8&quot;,&quot;3&quot;,&quot;.&quot;,&quot;.&quot;,&quot;7&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;],  [&quot;6&quot;,&quot;.&quot;,&quot;.&quot;,&quot;1&quot;,&quot;9&quot;,&quot;5&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;],  [&quot;.&quot;,&quot;9&quot;,&quot;8&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;6&quot;,&quot;.&quot;],  [&quot;8&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;6&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;3&quot;],  [&quot;4&quot;,&quot;.&quot;,&quot;.&quot;,&quot;8&quot;,&quot;.&quot;,&quot;3&quot;,&quot;.&quot;,&quot;.&quot;,&quot;1&quot;],  [&quot;7&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;2&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;6&quot;],  [&quot;.&quot;,&quot;6&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;2&quot;,&quot;8&quot;,&quot;.&quot;],  [&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;4&quot;,&quot;1&quot;,&quot;9&quot;,&quot;.&quot;,&quot;.&quot;,&quot;5&quot;],  [&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;.&quot;,&quot;8&quot;,&quot;.&quot;,&quot;.&quot;,&quot;7&quot;,&quot;9&quot;]]Output: falseExplanation: Same as Example 1, except with the 5 in the top left corner being     modified to 8. Since there are two 8&#39;s in the top left 3x3 sub-box, it is invalid.</code></pre><p><strong>Note:</strong></p><ul><li>A Sudoku board (partially filled) could be valid but is not necessarily solvable.</li><li>Only the filled cells need to be validated according to the mentioned rules.</li><li>The given board contain only digits <code>1-9</code> and the character <code>&#39;.&#39;</code>.</li><li>The given board size is always <code>9x9</code>.</li></ul><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h4 id="分批验证"><a href="#分批验证" class="headerlink" title="分批验证"></a>分批验证</h4><p>按行、列、九宫格验证1～9有且仅有一个即可。</p><pre class="line-numbers language-lang-java"><code class="language-lang-java">public boolean isValidSudoku(char[][] board) {    int[] row = new int[10], col = new int[10], grid = new int[10];    for (int i = 0; i < 9; i++) {        // 分批验证9行，9列以及九宫格块        Arrays.fill(row, 0);        Arrays.fill(col, 0);        Arrays.fill(grid, 0);        int tmpRowStart = i - i % 3;        int tmpColStart = i % 3 * 3;        for (int j = 0; j < 9; j++) {            if (board[i][j] != '.') {                if (row[board[i][j] - '0'] != 0)                      return false;                row[board[i][j] - '0'] = 1;            }            if (board[j][i] != '.') {                if (col[board[j][i] - '0'] != 0)                      return false;                col[board[j][i] - '0'] = 1;            }            if (board[tmpRowStart + j / 3][tmpColStart + j % 3] != '.') {                if (grid[board[tmpRowStart + j / 3][tmpColStart + j % 3] - '0'] != 0)                       return false;                grid[board[tmpRowStart + j / 3][tmpColStart + j % 3] - '0'] = 1;            }        }    }    return true;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(n^2)$</li><li>空间复杂度：$O(n)$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;Determine if a 9x9 Sudoku board is valid. Only the filled cells need to be validated &lt;strong&gt;according to the following rules&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each row must contain the digits &lt;code&gt;1-9&lt;/code&gt; without repetition.&lt;/li&gt;
&lt;li&gt;Each column must contain the digits &lt;code&gt;1-9&lt;/code&gt; without repetition.&lt;/li&gt;
&lt;li&gt;Each of the 9 &lt;code&gt;3x3&lt;/code&gt; sub-boxes of the grid must contain the digits &lt;code&gt;1-9&lt;/code&gt; without repetition.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Sudoku-by-L2G-20050714.svg/250px-Sudoku-by-L2G-20050714.svg.png&quot; alt=&quot;img&quot;&gt;&lt;br&gt;A partially filled sudoku which is valid.&lt;/p&gt;
&lt;p&gt;The Sudoku board could be partially filled, where empty cells are filled with the character &lt;code&gt;&amp;#39;.&amp;#39;&lt;/code&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="medium" scheme="http://riroaki.github.io/tags/medium/"/>
    
  </entry>
  
  <entry>
    <title>35.Search Insert Position</title>
    <link href="http://riroaki.github.io/2019/04/11/35-Search-Insert-Position/"/>
    <id>http://riroaki.github.io/2019/04/11/35-Search-Insert-Position/</id>
    <published>2019-04-10T18:10:39.000Z</published>
    <updated>2019-04-10T18:12:59.539Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.</p><p>You may assume no duplicates in the array.</p><a id="more"></a><p><strong>Example 1:</strong></p><pre><code>Input: [1,3,5,6], 5Output: 2</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: [1,3,5,6], 2Output: 1</code></pre><p><strong>Example 3:</strong></p><pre><code>Input: [1,3,5,6], 7Output: 4</code></pre><p><strong>Example 4:</strong></p><pre><code>Input: [1,3,5,6], 0Output: 0</code></pre><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h3 id="二分"><a href="#二分" class="headerlink" title="二分"></a>二分</h3><p>啥也别说了，就这样。</p><pre class="line-numbers language-lang-python"><code class="language-lang-python">public int searchInsert(int[] nums, int target) {    int lo = 0, hi = nums.length;    while (lo < hi) {        int mi = lo + (hi - lo) / 2;        if (nums[mi] < target)            lo = mi + 1;        else            hi = mi;    }    return lo;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(log(n))​$</li><li>空间复杂度：$O(1)$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order.&lt;/p&gt;
&lt;p&gt;You may assume no duplicates in the array.&lt;/p&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="easy" scheme="http://riroaki.github.io/tags/easy/"/>
    
      <category term="binary search" scheme="http://riroaki.github.io/tags/binary-search/"/>
    
  </entry>
  
  <entry>
    <title>34.Find First and Last Position of Element in Sorted Array</title>
    <link href="http://riroaki.github.io/2019/04/11/34-Find-First-and-Last-Position-of-Element-in-Sorted-Array/"/>
    <id>http://riroaki.github.io/2019/04/11/34-Find-First-and-Last-Position-of-Element-in-Sorted-Array/</id>
    <published>2019-04-10T18:03:56.000Z</published>
    <updated>2019-04-10T18:09:47.409Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>Given an array of integers <code>nums</code> sorted in ascending order, find the starting and ending position of a given <code>target</code> value.</p><p>Your algorithm’s runtime complexity must be in the order of <em>O</em>(log <em>n</em>).</p><p>If the target is not found in the array, return <code>[-1, -1]</code>.</p><a id="more"></a><p><strong>Example 1:</strong></p><pre><code>Input: nums = [5,7,7,8,8,10], target = 8Output: [3,4]</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: nums = [5,7,7,8,8,10], target = 6Output: [-1,-1]</code></pre><h3 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h3><h4 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h4><p>找<code>target + 1</code>的下标，再减1即为<code>target</code>的最后一个下标。</p><ol><li>元素可能不存在。</li><li>找到的元素可能和<code>target</code>相等。</li></ol><pre class="line-numbers language-lang-java"><code class="language-lang-java">public int[] searchRange(int[] nums, int target) {    int[] res = {-1, -1};    if (nums == null || nums.length == 0)        return res;    int first = bs(nums, target);    if (nums[first] != target)        return res;    res[0] = bs(nums, target);    int second = bs(nums, target + 1);    res[1] = nums[second] == target ? second : second - 1;    return res;}private int bs(int[] nums, int target) {    int lo = 0, hi = nums.length - 1;    while (lo < hi) {        int mi = lo + (hi - lo) / 2;        if (nums[mi] < target)            lo = mi + 1;        else            hi = mi;    }    return lo;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>时间复杂度：$O(log(n))$</li><li>空间复杂度：$O(1)$</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h3&gt;&lt;p&gt;Given an array of integers &lt;code&gt;nums&lt;/code&gt; sorted in ascending order, find the starting and ending position of a given &lt;code&gt;target&lt;/code&gt; value.&lt;/p&gt;
&lt;p&gt;Your algorithm’s runtime complexity must be in the order of &lt;em&gt;O&lt;/em&gt;(log &lt;em&gt;n&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;If the target is not found in the array, return &lt;code&gt;[-1, -1]&lt;/code&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="leetcode" scheme="http://riroaki.github.io/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://riroaki.github.io/tags/array/"/>
    
      <category term="medium" scheme="http://riroaki.github.io/tags/medium/"/>
    
      <category term="binary serch" scheme="http://riroaki.github.io/tags/binary-serch/"/>
    
  </entry>
  
  <entry>
    <title>ML学习日记6——深度学习入门</title>
    <link href="http://riroaki.github.io/2019/04/09/ML-6-Intro-to-Deep-Learning/"/>
    <id>http://riroaki.github.io/2019/04/09/ML-6-Intro-to-Deep-Learning/</id>
    <published>2019-04-09T09:09:50.000Z</published>
    <updated>2019-04-21T11:22:32.722Z</updated>
    
    <content type="html"><![CDATA[<p>这一课前半部分是在吹水……</p><a id="more"></a><h2 id="DL的前世今生"><a href="#DL的前世今生" class="headerlink" title="DL的前世今生"></a>DL的前世今生</h2><p>1958到1980年dl的发展。1980年的多层网络其实和现在的DNN已经差不多了。区别在于RBM initialization。</p><p>然而过去的研究表示dl并没有很好的效果……层数多的时候可能效果反而差。</p><p>这个模型，当时看来只是石头汤的石头罢了——其实并无大用，甚至还炒作出来许多虚假人工智能的搞笑的故事（和现在似曾相识？哈哈）。</p><p>2009年GPU是一个关键突破，因为加速了deep learning的过程。</p><p>11年语音辨识使用了这项技术，得到了比较好的结果；12年用在图像识别。这项技术终于火了起来。</p><h2 id="正片开始"><a href="#正片开始" class="headerlink" title="正片开始"></a>正片开始</h2><p>回到上一次的内容，DL其实是把许多小的神经通过sigmoid、softmax等等方法连接起来。</p><p>每一层都有自己的bias和weight。</p><p>最简单的连接方式是全连接前向传播神经网络（<em>Fully Connected FeedForward Network</em>）。</p><p>给定了一个网络结构，而没有确定具体参数，那么就是一个<em>function set</em>；假如参数和结构都确定了，那它就是一个函数。</p><p>我们在一般机器学习过程中，所做的事情也是先确定结构——<em>function set</em>，然后通过梯度下降找到最好的参数。</p><p>神经网络结构：</p><p>input layer、deep layer（指的是中间的许多隐层）、output layer</p><p>network具体做的事情其实就是矩阵运算，如图：</p><p><img src="/2019/04/09/ML-6-Intro-to-Deep-Learning/matrix.png" alt></p><p>写成矩阵运算的形式，这样就可以可以使用GPU执行并行计算加速计算过程。</p><p><img src="/2019/04/09/ML-6-Intro-to-Deep-Learning/matrix2.png" alt></p><h3 id="mnist——手写数字辨识"><a href="#mnist——手写数字辨识" class="headerlink" title="mnist——手写数字辨识"></a>mnist——手写数字辨识</h3><p>input：每个图片的矩阵</p><p>output：10维的one-hot向量</p><ol><li><p>首先，需要决定一个好的<em>function set</em>，即神经网络的结构：</p><ul><li>这一步需要trial&amp;error+intuition</li><li>有一些技巧可以让机器自己确定层数，但是应用暂时不广泛。</li></ul></li><li><p>接下来，找到好的<em>feature</em>，即<em>feature engineering</em>：</p><ul><li>有很多特征不是那么直觉，所以也很难。</li></ul></li><li><p>评估：计算交叉熵</p></li><li>调整：使用梯度下降</li></ol><h3 id="backpropagation反向传播（参数更新过程）"><a href="#backpropagation反向传播（参数更新过程）" class="headerlink" title="backpropagation反向传播（参数更新过程）"></a>backpropagation反向传播（参数更新过程）</h3><p>原理：微分的链式法则。</p><ul><li>每一个神经元更新参数的时候，需要计算<em>loss</em>函数对它的偏微分；</li><li>计算的时候，根据链式法则，继续向下一层寻找与本神经元输出有关的变量的偏微分；</li><li>每一项都依赖下一层，所以计算时从最后向前进行，相当于建立了反向的神经网络进行计算。</li></ul><p><img src="/2019/04/09/ML-6-Intro-to-Deep-Learning/backpropagation.png" alt></p><p>总结图如下：</p><p><img src="/2019/04/09/ML-6-Intro-to-Deep-Learning/summary.png" alt></p><h2 id="Keras实战——mnist"><a href="#Keras实战——mnist" class="headerlink" title="Keras实战——mnist"></a>Keras实战——mnist</h2><p>Keras集成了tensorflow和theano的库，是在这两个基础库上的更好用的接口。</p><p>在希腊语里面，Keras意味着horn——牛角～</p><p>而mnist作为深度学习界的hello world，自然在这里要写一下：</p><p>记住，深度学习和机器学习一样，分成三个步骤：</p><ul><li>确定模型（网络结构）、评估模型（损失函数）、改进模型（挑出最优）</li></ul><p>需要注意的是，tensorflow和keras经过这两年的时间已经有所变化，视频里面的api已经不能用了。</p><p>现在的2.0版本tf和keras竟然会有冲突（<code>AttributeError: module &#39;tensorflow&#39; has no attribute &#39;get_default_graph&#39;</code>），让人很是无语。</p><h3 id="处理输入"><a href="#处理输入" class="headerlink" title="处理输入"></a>处理输入</h3><p>这里需要注意，首先x的data是60000个28*28的0/1矩阵，表示这个格子有没有上色；y的data是60000个0～9的数字，训练前需要变动：</p><ul><li>把x的每个case展平成一个vector（因为特征是一维的）</li><li>把y变成one-hot格式，每个case用一个10维向量表示，原来值对应位置的那一维是1，其余为0。这在NLP中也很常见，主要是用来区分不同的类 ，而原来的数值表示类会有不好的影响（之前在讨论分类器时有涉及到）</li></ul><pre class="line-numbers language-lang-python"><code class="language-lang-python">from tensorflow.keras import Sequentialfrom tensorflow.keras.datasets import mnistfrom tensorflow.keras.layers import Densefrom tensorflow.keras.utils import to_categorical# Load data(x_train, y_train), (x_test, y_test) = mnist.load_data()# Flatten input 28 * 28 matrix into a 784 vectorx_train = x_train.reshape(len(x_train), 28 * 28)x_test= x_test.reshape(len(x_test), 28 * 28)# Make y into one-hot encodingy_train = to_categorical(y_train, 10)y_test = to_categorical(y_test, 10)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="设计模型"><a href="#设计模型" class="headerlink" title="设计模型"></a>设计模型</h3><p>这里模型是参考视频中的定义，用了最基本的<em>Sequential</em>模型。</p><p><em>units</em>表示输出的尺寸，和<em>input_dim</em>对应；除了第一层需要定义输入尺寸以外，后面都不需要声明（和上一层的输出尺寸是一样的）。</p><p>使用了<em>sigmoid</em>和<em>softmax</em>用来做层间整合，交叉熵作为<em>loss</em>函数，梯度下降方法的学习率调整使用<em>adam</em>方法，<em>accuracy</em>表示度量方式——一般都会使用这一个选项。</p><pre class="line-numbers language-lang-python"><code class="language-lang-python"># Build modelmodel = Sequential()model.add(Dense(input_dim=28*28, units=500, activation='sigmoid'))# Activation function could also be:# softplus, softsign, relu, tanhm hard_sigmoid, linearmodel.add(Dense(units=500, activation='sigmoid'))model.add(Dense(units=10, activation='softmax'))# Config loss function and gradient descent methodmodel.compile(loss='categorical_crossentropy',             optimizer='adam',             metrics=['accuracy'])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><code>batch-size</code>表示每次随机选取用于梯度下降的样本数，<code>epochs</code>表示所有样本的迭代次数。</p><h4 id="为什么不计算全部的样本进行迭代？"><a href="#为什么不计算全部的样本进行迭代？" class="headerlink" title="为什么不计算全部的样本进行迭代？"></a>为什么不计算全部的样本进行迭代？</h4><p>这个和之前讨论的BGD与SGD之争如出一辙，其实还是效率和质量的考虑。</p><p>“天下武功，唯快不破”，虽然BGD更为稳定，但是架不住SGD快呀！但是SGD每次只选一个case，随机性太大；所以就使用折衷方案，把所有case分成几个小批量（<em>mini-batch</em>），每次进行一个<em>batch</em>的<em>loss</em>计算，进行一次update。</p><p>另一方面，小批量的梯度下降能够保证一定的随机性，比批量的方法更容易跳出局部极小值。</p><p>而当所有的<em>mini-batch</em>都被计算完，就叫做一个<em>epoch</em>。</p><blockquote><p>虽然一个<em>epoch</em>里面计算<em>loss</em>的次数是一样的，但是可以实现并行计算的加速（也就是说可以同时计算多个case的loss），所以选择适中的<code>batch-size</code>实际效果比<code>batch-size</code>等于1的情况要好。</p><p>具体并行计算加速其实并没有那么复杂：把多个x的向量合并成一个matrix再一起运算，在GPU中矩阵运算得到较好的优化，所以得到了加速。</p></blockquote><pre class="line-numbers language-lang-python"><code class="language-lang-python"># Train modelmodel.fit(x_train, y_train, batch_size=100, epochs=20)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h3><p>可以直接计算准确率：</p><pre class="line-numbers language-lang-python"><code class="language-lang-python"># Loss and accuratyscore = model.evaluate(x_test, y_test)print('Total loss on testing set:', score[0])print('Accuracy of testing set:', score[1])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre><code>10000/10000 [==============================] - 0s 40us/sample - loss: 0.1367 - accuracy: 0.9589Total loss on testing set: 0.13666086520701648Accuracy of testing set: 0.9589</code></pre><p>也可以选择导出结果（不知道正确答案是多少的场景）：</p><pre class="line-numbers language-lang-python"><code class="language-lang-python">result = model.predict(x_test)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>因为采取的小批量梯度下降具有一定的随机性，所以准确率和结果每次都会有一点差别。</li></ul><p>～本文完～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这一课前半部分是在吹水……&lt;/p&gt;
    
    </summary>
    
      <category term="machine learning" scheme="http://riroaki.github.io/categories/machine-learning/"/>
    
    
      <category term="deep learning" scheme="http://riroaki.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>
