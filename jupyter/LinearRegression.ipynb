{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归和梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备阶段：生成训练数据和测试数据，选取多元一次的线性关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1000 cases with 10 features.\n",
      "Split data into 2 parts: 515 train data and 485 test data\n"
     ]
    }
   ],
   "source": [
    "# 生成train data和test data\n",
    "# 随机选择一些x和w，b\n",
    "from random import randint, uniform\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "def genCase(n: int, m: int):\n",
    "    x = random.uniform(-10, 10, (m, n))\n",
    "    w = random.uniform(0, 10, (n, 1))\n",
    "    b = uniform(0, 10)\n",
    "    # 加入噪音\n",
    "    y = [sum(x[i1].dot(w)) + b + uniform(-0.5, 0.5) for i1 in range(len(x))]\n",
    "    print(\"Generated %d cases with %d features.\" % (m, n))\n",
    "    return x, y, w, b\n",
    "\n",
    "\n",
    "def splitCase(x, y):\n",
    "    n = len(x)\n",
    "    if len(y) != n:\n",
    "        return [], [], [], []\n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "    for j1 in range(n):\n",
    "        if randint(1, 4) > 2:\n",
    "            testX.append(x[j1])\n",
    "            testY.append(y[j1])\n",
    "        else:\n",
    "            trainX.append(x[j1])\n",
    "            trainY.append(y[j1])\n",
    "    print(\"Split data into 2 parts: %d train data and %d test data\" % (len(trainX), len(testX)))\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "\n",
    "X, Y, OriginW, OriginB = genCase(10, 1000)\n",
    "TrainX, TrainY, TestX, TestY = splitCase(X, Y)\n",
    "# 导出到文件\n",
    "with open('./data/train.txt', 'w') as f:\n",
    "    for i in range(len(TrainX)):\n",
    "        f.write('\\t'.join(list(map(str, TrainX[i]))) + '\\t' + str(TrainY[i]) + '\\n')\n",
    "with open('./data/test.txt', 'w') as f:\n",
    "    for i in range(len(TestX)):\n",
    "        f.write('\\t'.join(list(map(str, TestX[i]))) + '\\t' + str(TestY[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练阶段：采用批量梯度下降方法拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning ratio: 0.020000\n",
      "Converges after 696 rounds of batch gradient descent.\n",
      "CPU times: user 7.05 s, sys: 22 ms, total: 7.08 s\n",
      "Wall time: 7.09 s\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt, ones\n",
    "\n",
    "\n",
    "def h(x, w, b):\n",
    "    return sum(x.dot(w)) + b\n",
    "\n",
    "\n",
    "def loss(x, y, w, b):\n",
    "    return sum([(h(x[i1], w, b) - y[i1]) ** 2 for i1 in range(len(x))])\n",
    "\n",
    "\n",
    "def batchGradientDescent(x, y, rate=0.1, iterBound=100000, gradBound=1e-6):\n",
    "    m = len(x)\n",
    "    if m == 0 or len(y) != m:\n",
    "        return [], -1\n",
    "    n = len(x[0])\n",
    "    w, b = ones((n, 1)), 1\n",
    "    print(\"Learning ratio: %f\" % rate)\n",
    "    for k in range(iterBound):\n",
    "        gradW = [sum([x[i][j] * (y[i] - h(x[i], w, b)) for i in range(m)]) / m for j in range(n)]\n",
    "        gradB = sum([(y[i] - h(x[i], w, b)) for i in range(m)]) / m\n",
    "        # 跳出循环的条件：梯度值较小\n",
    "        if abs(gradB) <= gradBound:\n",
    "            print(\"Converges after %d rounds of batch gradient descent.\" % k)\n",
    "            return w, b\n",
    "        for i in range(n):\n",
    "            w[i] += gradW[i] * rate\n",
    "        b += gradB * rate\n",
    "    print(\"After %d rounds of gradient descent, the loss is %f\" % (k, loss(x, y, w, b)))\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def readData(file: str):\n",
    "    with open(file, 'r') as f:\n",
    "        data = loadtxt(f)\n",
    "    return [data[i1][:-1] for i1 in range(len(data))], [data[i1][-1] for i1 in range(len(data))]\n",
    "\n",
    "\n",
    "trainX, trainY = readData(\"./data/train.txt\")\n",
    "testX, testY = readData(\"./data/test.txt\")\n",
    "%time W, B = batchGradientDescent(trainX, trainY, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测阶段：使用test数据测试模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for test data: 43.180648\n",
      "Loss for test data with original w and b: 41.600574\n",
      "Difference for w and b: [[ 0.00109262]\n",
      " [-0.00581631]\n",
      " [ 0.00122251]\n",
      " [-0.00210383]\n",
      " [-0.00198792]\n",
      " [-0.00037012]\n",
      " [ 0.00243138]\n",
      " [-0.004199  ]\n",
      " [-0.00064644]\n",
      " [ 0.00013948]] -0.009272296463247759\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss for test data: %f\" % loss(testX, testY, W, B))\n",
    "print(\"Loss for test data with original w and b: %f\" % loss(testX, testY, OriginW, OriginB))\n",
    "print(\"Difference for w and b:\", OriginW - W, OriginB - B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 经过梯度下降之后，达到了较好的精度。\n",
    "- 时间上存在改进空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自适应learning rate：Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges after 251 rounds of batch gradient descent.\n",
      "CPU times: user 2.55 s, sys: 9.66 ms, total: 2.56 s\n",
      "Wall time: 2.57 s\n",
      "Loss for test data: 43.177537\n",
      "Loss for test data with original w and b: 41.600574\n",
      "Difference for w and b: [[ 0.00108563]\n",
      " [-0.00580816]\n",
      " [ 0.00122102]\n",
      " [-0.002008  ]\n",
      " [-0.00198998]\n",
      " [-0.0003545 ]\n",
      " [ 0.00242942]\n",
      " [-0.00419475]\n",
      " [-0.00064363]\n",
      " [ 0.00016461]] -0.009286649191009438\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def improvedBGD(x, y, iterBound=10000, gradBound=1e-6):\n",
    "    m = len(x)\n",
    "    if m == 0 or len(y) != m:\n",
    "        return [], -1\n",
    "    n = len(x[0])\n",
    "    w, b = ones((n, 1)), 1\n",
    "    rateW, rateB = [0 for _ in range(n)], 0\n",
    "    for k1 in range(iterBound):\n",
    "        gradW = [sum([x[i][j] * (y[i] - h(x[i], w, b)) for i in range(m)]) / m for j in range(n)]\n",
    "        gradB = sum([(y[i] - h(x[i], w, b)) for i in range(m)]) / m\n",
    "        # 跳出循环的条件：梯度值较小\n",
    "        if abs(gradB) <= gradBound:\n",
    "            print(\"Converges after %d rounds of batch gradient descent.\" % k1)\n",
    "            return w, b\n",
    "        for i in range(n):\n",
    "            rateW[i] += gradW[i] ** 2\n",
    "            w[i] += gradW[i] / sqrt(rateW[i])\n",
    "        rateB += gradB ** 2\n",
    "        b += gradB / sqrt(rateB)\n",
    "    print(\"After %d rounds of gradient descent, the loss is %f\" % (k1, loss(x, y, w, b)))\n",
    "    return w, b\n",
    "\n",
    "%time W, B = improvedBGD(trainX, trainY)\n",
    "print(\"Loss for test data: %f\" % loss(testX, testY, W, B))\n",
    "print(\"Loss for test data with original w and b: %f\" % loss(testX, testY, OriginW, OriginB))\n",
    "print(\"Difference for w and b:\", OriginW - W, OriginB - B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以看到，时间和精度上都更合理，自动调整学习率能够避免不收敛或者收敛过慢的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用高次拟合，查看loss变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 9999 rounds of gradient descent, the loss is 36.904122\n",
      "CPU times: user 3min 50s, sys: 1.34 s, total: 3min 51s\n",
      "Wall time: 3min 52s\n",
      "After 9999 rounds of gradient descent, the loss is 35.874497\n",
      "CPU times: user 5min 39s, sys: 1.56 s, total: 5min 40s\n",
      "Wall time: 5min 42s\n",
      "After 9999 rounds of gradient descent, the loss is 261.607271\n",
      "CPU times: user 7min 28s, sys: 2.38 s, total: 7min 31s\n",
      "Wall time: 7min 33s\n",
      "After 9999 rounds of gradient descent, the loss is 181030.524181\n",
      "CPU times: user 9min 20s, sys: 3.25 s, total: 9min 24s\n",
      "Wall time: 9min 27s\n"
     ]
    }
   ],
   "source": [
    "def generateAll(x, n):\n",
    "    if n == 1:\n",
    "        return x\n",
    "    res = ones((len(x), n * len(x[0])))\n",
    "    count = len(x[0])\n",
    "    for i1 in range(len(x)):\n",
    "        for j1 in range(n):\n",
    "            for k1 in range(count):\n",
    "                res[i1][k1 + j1 * count] = x[i1][k1] ** (j1 + 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "# 拟合过程可以直接使用batchGradientDescent函数解决\n",
    "# 为了节约时间（一方面learning rate不能高，一高就不收敛；另一方面太低可能半小时都跑不出来）\n",
    "trainLoss = [loss(trainX, trainY, W, B)]\n",
    "testLoss = [loss(testX, testY, W, B)]\n",
    "for i in range(2, 6):\n",
    "    exTrainX, exTestX = generateAll(trainX, i), generateAll(testX, i)\n",
    "    %time Wi, Bi = improvedBGD(exTrainX, trainY)\n",
    "    trainLoss.append(loss(exTrainX, trainY, Wi, Bi))\n",
    "    testLoss.append(loss(exTestX, testY, Wi, Bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange\n",
    "\n",
    "x_axis = arange(len(trainLoss))\n",
    "plt.axis('on')\n",
    "plt.plot(x_axis, trainLoss, label='train')\n",
    "plt.plot(x_axis, testLoss, label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 出现了明显的overfitting现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43.17753677246426, 46.221044463400624, 47.5150256435928, 295.13051190429354, 216220.90071215943]\n"
     ]
    }
   ],
   "source": [
    "print(testLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
