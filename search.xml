<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>自制数独 &amp; 数独求解算法</title>
      <link href="/Sudoku-Auto-Player/"/>
      <url>/Sudoku-Auto-Player/</url>
      
        <content type="html"><![CDATA[<p>本文代码：<a href="https://github.com/Riroaki/Auto-Sudoku-Player">https://github.com/Riroaki/Auto-Sudoku-Player</a></p><p>上次写了自动扫雷，这次打算试试自动解数独。</p><p>当然，解数独之前得写一个生成数独的算法。</p><p>总之，这篇文章也记录了一个无聊时的尝试。</p><h2 id="数独生成算法"><a href="#数独生成算法" class="headerlink" title="数独生成算法"></a>数独生成算法</h2><p>随机生成+检测？效率太低了。</p><p>参考了一个网上的做法，从一个九宫格出发，通过矩阵变换得到其他九宫格，从而获得整体数独。</p><p>首先，在中间的宫格生成一个随机的排列：</p><p><img src="/Sudoku-Auto-Player/20180727195204899.png" alt></p><p>第二步，将中间的宫格向两侧作行变换扩展（注意，原本0-1-2的行排列做变换后只有1-2-0和2-0-1的排列是符合数独规则的，列变换也是一样）</p><p><img src="/Sudoku-Auto-Player/20180727195518407.png" alt></p><p>中间的格子列变换生成上下的宫格：</p><p><img src="/Sudoku-Auto-Player/20180727195645731.png" alt></p><p>然后，（这一步其实可以替换为不同的做法），使用左右两个宫格进行行变换，分别上下拓展。</p><p><img src="/Sudoku-Auto-Player/20180727200836755.png" alt></p><p>这一个做法优缺点如下：</p><ul><li><p>优点：十分高效，并且能够确保生成合法的数独。</p></li><li><p>缺点：生成的数独的宫格之间的相似性较强，符合一定的模式，不能穷举所有的数独（不过，这一点并不重要，毕竟数独一共有：6670903752021072936960个，参考<a href="http://www.afjarvis.staff.shef.ac.uk/sudoku/sudoku.pdf，通过这种方法生成的数独大概在9!*2^4个，所以是五百多万）">http://www.afjarvis.staff.shef.ac.uk/sudoku/sudoku.pdf，通过这种方法生成的数独大概在9!*2^4个，所以是五百多万）</a></p></li></ul><h2 id="数独解法"><a href="#数独解法" class="headerlink" title="数独解法"></a>数独解法</h2><h3 id="局部枚举-求同存异"><a href="#局部枚举-求同存异" class="headerlink" title="局部枚举+求同存异"></a>局部枚举+求同存异</h3><p>这里的做法其实比较简单，是结合上一次<a href="/Mine-Sweeper-Auto-Player">扫雷算法</a>之后想出来的。</p><blockquote><p>数独规则：对于每一个行，列还有宫格，数独的规则规定了1-9的数字出现有且仅有1次。</p></blockquote><p>如果直接对全局进行暴力搜索/回溯，那么恐怕时间效率和内存都会爆炸。</p><p>所以我们还是需要区分【局部】的概念。</p><p>而数独中天然地划分出了行/列/宫格，可以作为一个局部。</p><p>那么，我们对每一个行/列/宫格，都可以找出当前未出现的数字集合D（Digits），和未被填充的格子P（Positions），而目标就是要产生解D&lt;-&gt;P（只是个人的一个记法，即D和P中元素一一对应）。</p><p>然后，我们可以根据数独规则生成一系列满足当前情况（feasible）的解，在这一群解之中，某些格子总是被填上了一个数，那么这个必然是真正的答案。——这便是小标题所说，求同存异的意思。</p><p>具体实现起来，我这里采用了先生成数字的permutation序列，再从中找出确定的解的做法。</p><p>是不是和求解扫雷的思路如出一辙？</p><h3 id="多解"><a href="#多解" class="headerlink" title="多解"></a>多解</h3><p>与扫雷稍有不同的是数独并不依赖概率——在扫雷的游戏中，我们通过搜索当前局部的解，可以估计每一个区块有雷的概率，但是在数独中，一般认为解是确定的，可能存在一个或者多个的情况，但是并没有“概率”一说。</p><p>在某些情况下数独会出现多解（即便是只剩4个空格也可以造出多解，这个应该玩过数独的人都明白我说的是什么情况吧）。</p><p>在多解情况下，如果仅仅采用基本的思路，那么是会卡住的；而概率又不适用于数独，往往情况是先定一个格子，而后其他不确定的格子也因为这一个格子而得以明确（扫雷的死亡二选一就比较坑爹了哈哈）。</p><h2 id="完成图"><a href="#完成图" class="headerlink" title="完成图"></a>完成图</h2><p>看起来还不错。</p><p>![](Sudoku-Auto-Player/屏幕快照 2019-08-07 14.51.28.png)</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>此外，求解数独比较出名的解法还有舞蹈链（Dancing links）算法，以后有空再补充吧。</p>]]></content>
      
      
      <categories>
          
          <category> Toys </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自制扫雷 &amp; 自动扫雷算法探索</title>
      <link href="/Mine-Sweeper-Auto-Player/"/>
      <url>/Mine-Sweeper-Auto-Player/</url>
      
        <content type="html"><![CDATA[<p>本文记录了一个无聊的人的一天。</p><p>代码：<a href="https://github.com/Riroaki/Mine-Sweeper">https://github.com/Riroaki/Mine-Sweeper</a></p><p>网页版：<a href="https://riroaki.github.io/toys/MineSweeper.html">https://riroaki.github.io/toys/MineSweeper.html</a></p><p>闲来无事，有点想玩扫雷。</p><p>（其实是想自己DIY一个……玩是没啥好玩的）</p><p>好，写。</p><h2 id="自制扫雷"><a href="#自制扫雷" class="headerlink" title="自制扫雷"></a>自制扫雷</h2><h3 id="棋盘类（Map）"><a href="#棋盘类（Map）" class="headerlink" title="棋盘类（Map）"></a>棋盘类（Map）</h3><p>首先呢，扫雷肯定要棋盘对吧，这个棋盘得是个矩阵，里面用数值和雷表示格子的状态。</p><ul><li><p>状态表示：用-1表示雷，这样一来就可以和正常的格子（值为0-8）区分开来。</p></li><li><p>初始化棋盘：指定生成雷的个数随机分布。</p><ul><li>但是问题来了，万一第一下点开就是个雷怎么办？那也太冤了吧。</li><li>好，那就在第一下点开的时候初始化，不要让点的地方变成雷就行了。</li><li>具体初始化的时候，先初始化雷，再对所有非雷的格子计算8个邻居的雷数。</li></ul></li></ul><h3 id="游戏类（Game）"><a href="#游戏类（Game）" class="headerlink" title="游戏类（Game）"></a>游戏类（Game）</h3><p>作为整个游戏的管理，需要记录游戏的各项数据：雷的个数、点击个数、游戏状态、游戏时间等等，还要作为接口处理交互，总而言之就是处于棋盘数据和玩家之间的中间人。</p><ul><li>可见棋盘：和棋盘的真实值想对，或者叫做掩码（Mask）比较好，记录了棋盘被翻开（KNOWN）、未被翻开（UNKNOWN）和打上标记（MARKED）的3种状态。</li><li>操作：玩家使用的接口，可以传入两种操作——翻开或者标记。<ul><li>标记比较简单，注意被标记的格子得不能是翻开的，标记第二次就恢复未被翻开的状态。</li><li>翻开操作就复杂一些：<ul><li>首先被翻开的只能是未知状态而不是标记状态的格子。</li><li>其次如果当前的格子是0（周围没有雷），那么需要递归地翻开周围8个方向的格子直到翻出非0的格子。</li><li>如果当前的格子是-1，boom！</li><li>翻开之后记得检查是不是翻完了，是的话就判获胜。</li></ul></li><li>操作完成以后检查游戏状态，如果不是进行中的状态就进入结束游戏的环节。</li></ul></li><li>展示：这部分是我花了比较多的心思的地方，然而并没有什么技术含量（笑<ul><li>打印游戏状态和数据，上色，打印格式等等啊……</li><li>用控制台主要是因为懒得写GUI，不过可以考虑做一个网页版（实际上已经做了）</li></ul></li></ul><p>好，这样就可以玩了（为了好看我还加了一个ascii的heading）：</p><p><img src="/Mine-Sweeper-Auto-Player/man-start.png" alt></p><p>等等，这篇文章就这么结束了？是不是太水了一点……</p><p>当然，我不会满足于做一个简单的扫雷小游戏。重头戏在后边——</p><h2 id="自动扫雷算法"><a href="#自动扫雷算法" class="headerlink" title="自动扫雷算法"></a>自动扫雷算法</h2><p>自动扫雷算法还是挺值得玩味的，水有一点深。</p><p>参考了一部分这篇文章的内容：<a href="https://luckytoilet.wordpress.com/2012/12/23/2125/">https://luckytoilet.wordpress.com/2012/12/23/2125/</a></p><p>大致可以把我的思路分成以下几个部分：</p><h3 id="简单策略"><a href="#简单策略" class="headerlink" title="简单策略"></a>简单策略</h3><p>针对初级的扫雷，雷的密度不大，所以经常是根据一个翻开的格子就能够知道它周围8个的情况了。</p><p>具体而言分为两种情形：</p><ul><li>周围的雷的个数==周围没翻开的格子数：没翻开的全是雷</li><li>周围的雷的个数==周围标记为雷的格子数：没翻开的全都不是雷</li></ul><p>浅显易懂的规则，编码实现也简单粗暴：遍历一遍格子，对所有翻开的格子检查一遍。</p><h3 id="复杂策略"><a href="#复杂策略" class="headerlink" title="复杂策略"></a>复杂策略</h3><p>当简单判断失效的时候，我们采用复杂一些的规则。</p><p>玩扫雷的同志们肯定会遇到一种情况，需要不止一个格子判断出是不是雷：</p><p><img src="/Mine-Sweeper-Auto-Player/27081835_lzcI.png" alt></p><p>上图里面，对红色区域和黄色区域我们很难直接去判断，但是注意旁边两个竖着的的绿色的2：</p><ul><li>因为右边有一个雷，下那个2肯定有且仅有一个雷在红色区域；</li><li>因为红色区域有雷而且右边有一个雷，上面那个2对应的雷已经满足了，所以黄色区域是安全的。</li></ul><p>上面的规则对人来说很符合逻辑，但是对程序而言，这种规则很难找到一个具体可行的规则——它太灵活啦。人的逻辑是如此复杂，以至于可以用如此简洁的形式表达这样的规则。</p><p>因此我们找一个死板一点的可替代方案——穷举。</p><p>简单来说，我们可以对上面的情形找到几个解决方案：</p><ul><li>红色区上面格子是雷；黄色区没有雷</li><li>红色区下面格子是雷；黄色区没有雷</li></ul><p>如何，是不是发现了共同之处：黄色区都没有雷。</p><p>既然如此，我们让程序去试出所有的解决方案，然后找在所有方案里都是雷/都不是雷的格子就行啦！</p><h4 id="且慢。"><a href="#且慢。" class="headerlink" title="且慢。"></a>且慢。</h4><p>虽然话是这么说没错，可是冷静下来就会意识到：这可是指数复杂度的行为啊。</p><p>对一个未翻开的格子有两种可能：是雷，或者不是雷？（Be or not to be, it’s a question…）</p><p>如果不经过剪枝，那么每一个新格子就会使得解空间翻倍——即便剪枝可能也是阶乘级别的复杂度。</p><p>即便CPU转的再快，也还是有极限的啊，这么折腾谁顶得住哇。</p><p>为了补救，我们就需要引入局部的概念——把所有待判断的未知的格子划分为几个局部，而不是对所有格子一起判断。局部是这样定义的：</p><ul><li>同在一个局部的格子的决策会互相影响。比如，相邻的格子。</li><li>不在一个局部的格子不会直接影响。</li></ul><blockquote><p>补充：出于性能的考虑，我们设置一个局部大小的上限——如果有一个局部的大小超过这个上限，那就放弃求解，不然很可能会等待计算到天荒地老……</p></blockquote><p>好，我们总结一下这一个算法：</p><ol><li>找到待判断的格子——这里我们只考虑位于已知格子边缘的那些格子，并提取出它们的约束条件（即，n个位置的格子包含m个雷），后面将通过约束条件来找出满足的解。</li><li>将所有的待判断格子分块，也就是划分局部。事实上我做的是把格子对应的约束条件分块，把所有【n个位置中包含相同格子】的约束条件划分为一块。这里使用了并查集（Union Find）算法进行归类。<ol><li>这里还遇到一个小细节，<code>set</code>是不可哈希的，所以不能作为<code>dict</code>的<code>key</code>使用，这导致我只能使用一个两个列表分别存储约束条件的键（n个格子的坐标）和值（m，雷的个数）。</li></ol></li><li>对每一个约束条件集合，找出满足的解。首先提取出这个约束条件包含的所有的格子，需要找出的解就是指“这些格子是否为雷”的结果。然后使用回溯（backtracking）的做法方便获得所有满足的解。<ol><li>中途剪枝：如果安排到一半，雷数量已经超了某个条件的总数那当然不行；而雷的数量太少，可能最后都没法满足条件也不行。</li><li>解的过滤：最后需要让所有约束条件满足的才是真正的解。</li></ol></li><li>找到所有解之中，不变的那个格子——对每个局部，把全部解（值为True/False的列表）求和之后算出是每一个格子是雷的概率，是0或者是1都可以实锤操作。</li></ol><h3 id="概率策略"><a href="#概率策略" class="headerlink" title="概率策略"></a>概率策略</h3><p>这样就能够100%完成了吗？Naive！</p><p>玩扫雷多了的人都知道，扫雷这个游戏其实很多时候还看脸。</p><p>有时候你就是会遇到无解的格子，like this：</p><p><img src="/Mine-Sweeper-Auto-Player/27081840_pqqJ.png" alt></p><p>理论上，只要地图是概率生成的，你就无法避免这种情况的发生——纯粹五五开的结果。</p><p>有时候还是在翻最后一个格子的时候遇到这种情况，你说气不气？</p><p>不过，还有时一些时候我们还是能稍微相信数学的：准确的说，是相信解的概率。</p><p>还记得我们上面获得的解集合吗？花了这么大功夫得到的东西，自然得妥善利用起来。</p><p>对上面的解集合，我们对计算出来概率是0和1的格子另眼相待；那么我们也可以稍微放宽要求，把是0的概率最大的（最可能不是雷），或者是1的概率最大的（最可能是雷）的格子也利用起来呢？</p><p>当然，实际操作的时候我倾向于对翻开前者，而不是标记后者：</p><ul><li>翻开一个格子能够获得的信息远大于标记一个格子，况且还能验证这一次的猜想。</li><li>相比之下，如果标记了格子，那么一旦预测错误，很可能会影响之后的预测。</li></ul><h3 id="补充：剩余雷数"><a href="#补充：剩余雷数" class="headerlink" title="补充：剩余雷数"></a>补充：剩余雷数</h3><p>到这里，我们还有一个信息需要利用一下：</p><ul><li>剩余的雷数。</li></ul><p>这一个数据可以帮助我们避免离谱的猜想——比如说，有时候确实存在两个雷能够满足解的情况，但是万一你只有一个雷可以分配了呢？（这个说法可能不恰当，我是说剩余的未发现的雷只剩1个了）</p><p>因此，在上文的复杂判断中，我们额外加规则来保证解的安全性（虽然只是一个很弱的必要条件，但是也是很有价值的）：</p><ol><li>每一个局部的雷数应当大于等于1。<ol><li>这是显然的，可以用反证法证明：如果有一个局部不存在雷，那么早在简单策略中就应该被排除，不会轮到复杂判断这一步。</li></ol></li><li>基于上一条，对每一个局部的总雷数不能多于：剩余总雷数-（局部的个数-1）</li></ol><h3 id="随机策略"><a href="#随机策略" class="headerlink" title="随机策略"></a>随机策略</h3><p>如果上面的策略还是不能找出合理的解，好吧，只能交给<code>random</code>了。</p><p>所谓“尽人事，听天命”也。</p><h3 id="总结：混合策略"><a href="#总结：混合策略" class="headerlink" title="总结：混合策略"></a>总结：混合策略</h3><p>我们总结一下策略的结构：</p><ol><li>简单策略：基于两个基本规则，永远是第一步</li><li>复杂策略：配合剩余雷数的附加规则使用，当简单策略不适用的时候启动 </li><li>概率策略：使用复杂策略计算出的解集算出概率，翻开概率最接近0的格子；在复杂策略失效的时候启动</li><li>随机策略：瞎蒙一个格子翻开。概率策略失效的时候启动</li></ol><p>好了，可以玩啦！</p><p>自动扫雷看着屏幕一跳一跳的有点不爽，不过格子被一个个点出来的时候真心解压啊！</p><p>上个过程图，体感雷密度较小的时候胜率还不错。</p><p>（这里设置每一步都停止0.05秒，为了方便看清做了啥，实际运行时间很短）</p><p><img src="/Mine-Sweeper-Auto-Player/play.png" alt></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>呼……看到这里，相信你也累了。</p><p>不过，收获回报的时候来啦！我们这么复杂的规则写完，没有测试结果怎么行，这不是耍流氓嘛。</p><p>这里就使用40*40的棋盘，对雷区密度为<code>[0.050, 0.075, ..., 0.250]</code>的序列，测试胜率。</p><p>针对每一个密度测试1000次，计算胜率。</p><p>过程截图：</p><p><img src="/Mine-Sweeper-Auto-Player/test.png" alt></p><ul><li>可以看到，开头的正确率还挺高，但是到了15%就开始骤降……</li><li>同时，玩一局花费的时间也增长了，这是因为高级策略介入变多，耗时增大。</li></ul><p>这个运行时间……<s>我正在跑神经网络？</s></p><p>总结果如下：</p><p>面对高密度的雷，胜率还是有点低啊——不过实际上扫雷不会遇到25%这么高的雷区密度的，相信我！</p><p><img src="/Mine-Sweeper-Auto-Player/test-plot.png" alt></p><h2 id="经典扫雷再现"><a href="#经典扫雷再现" class="headerlink" title="经典扫雷再现"></a>经典扫雷再现</h2><p>其实还有一个测试，就是按照windows原版扫雷的参数测试：</p><ul><li><strong>初级</strong>：8 × 8，10颗地雷（<a href="https://zh.wikipedia.org/wiki/Windows_98">Windows 98</a>或以前）或9 × 9，10颗地雷（<a href="https://zh.wikipedia.org/wiki/Windows_2000">Windows 2000</a>或以后）</li><li><strong>中级</strong>：16 × 16，40颗地雷</li><li><strong>高级</strong>：30 × 16，99颗地雷</li></ul><p>测试结果如下：</p><p><img src="/Mine-Sweeper-Auto-Player/classic.png" alt></p><p>emmm……基本符合上面的图像。怎么说呢，高级扫雷部分还是低了一点吧。</p><p>日后看看还有没有更强大更完备的改进算法吧。</p><h2 id="Finale"><a href="#Finale" class="headerlink" title="Finale"></a>Finale</h2><p>最后是彩蛋——死亡二选一：</p><p><img src="/Mine-Sweeper-Auto-Player/losing.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Toys </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>见习炼丹师-02：全连接神经网络</title>
      <link href="/Deep-Learning-02-FCNN/"/>
      <url>/Deep-Learning-02-FCNN/</url>
      
        <content type="html"><![CDATA[<p>本文是“见习炼丹师”系列第二篇文章，主要内容为全连接神经网络的基本理论和算法实现。</p><p>建议搭配我的另一个专栏“机器学不动了”食用。</p><p>全系列推荐结合个人实现的代码阅读：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork和pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>本文主要介绍全连接神经网络。</p><p>作为入门级别的神经网络，它的结构只有一个常见的层组成：全连接层。</p><p>每一层的神经元都和下一层网络的所有神经元互相连接，所以得名：</p><p><img src="/Deep-Learning-02-FCNN/fcnn.jpeg" alt></p><h2 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h2><p>全连接层由线性单元组成，每个线性单元/神经元本质和感知机是一样的，只不过感知机的激活函数为<strong>阶跃函数</strong>，而在神经网络中我们更喜欢使用连续而可导的函数作为激活函数。</p><p><img src="/Deep-Learning-02-FCNN/neuron.gif" alt></p><p>这里我们选用sigmoid函数：$y=sigmoid(x)=\frac{1}{1+e^{-x}}$。</p><p>还记得这个函数长什么样吗？</p><p><img src="/Deep-Learning-02-FCNN/sigmoid.jpg" alt></p><p>所以我们有：$y=sigmoid(net(x))=sigmoid(w^Tx+b)=\frac{1}{1+e^{-(w^Tx+b)}}$</p><p>sigmoid函数的导数也很特别：$y=sigmoid(x),y’=y(1-y)$，证明略。</p><h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>咱也不啰嗦了，贴一张用过好几次的图：</p><p><img src="/Deep-Learning-02-FCNN/net-half.png" alt></p><p>以第一层为例，推导反向传播公式：</p><ul><li><p>注意，此处为和图中记法一致，令$w_{ji}$表示第j个神经元对第i个输入的权重。</p></li><li><p>则$net_j=w_j*x+b=\Sigma_{i=0}^n{w_{ji}x_i}+b_j$</p></li><li><p>$\frac{\partial net_j}{\partial w_{ji}}=x_i,\frac{\partial net_j}{\partial b_j}=x_0=1$</p></li><li><p>如果已知了$\frac{\partial J}{\partial y_j}$（从后一层传下来的梯度），</p></li><li><p>那么对每一个权重的梯度为：$\frac{\partial J}{\partial w_{ji}}=\frac{\partial J}{\partial y_j}\frac{\partial y_j}{\partial net_j}\frac{\partial net_j}{\partial w_{ji}}$</p></li></ul><p>接下来把这个公式向量化：</p><ul><li>首先，视输入值维度和输出值维度分别为in_dim和out_dim，$\frac{\partial J}{\partial net_j}=\frac{\partial J}{\partial y_j}\frac{\partial y_j}{\partial net_j}$是一个out_dim* 1的向量，而$\frac{\partial net_j}{\partial w_j}=x$是一个in_dim* 1的向量；而更新需要的梯度形状和权重矩阵形状相同，是out_dim* in_dim大小的。<ul><li>所以直观上有一个感觉，把两个向量叉乘获得的矩阵就是梯度矩阵。</li></ul></li><li>事实证明，上面的感觉是正确的：$\frac{\partial J}{\partial w_{ji}}=\frac{\partial J}{\partial net_j}*\frac{\partial net_j}{\partial w_{ji}}$。</li><li>而对b的梯度，只需要前面部分即可，因为b对应的输入值为1，即$\frac{\partial J}{\partial b_j}=\frac{\partial J}{\partial y_j}\frac{\partial y_j}{\partial net_j}*1$。</li></ul><p>除了这一层本身的梯度，还应当有一个传回前一层的梯度$\frac{\partial J}{\partial x}$：</p><ul><li>$\frac{\partial J}{\partial x_i}=\frac{\partial J}{\partial y_j}\frac{\partial y_j}{\partial net_j}\frac{\partial y_j}{\partial x_i}=\frac{\partial J}{\partial y_j}\frac{\partial y_j}{\partial net_j}w_{ji}$</li><li>经过向量化，得到：$\frac{\partial J}{\partial x}=\frac{\partial J}{\partial y}\frac{\partial y}{\partial net}w$，同样也是矩阵相乘。</li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>以下代码详情见repo：<a href="https://github.com/Riroaki/LemonML/">LemonML</a></p><blockquote><p>注意：为求思路清晰和代码简洁，这里的实现默认使用输入和输出均为单个样本，而不是批量运算。</p></blockquote><p>准备工作：基本的激活函数<code>SigmoidActivation</code>类，继承自基本的<code>Activation</code>类：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SigmoidActivation</span><span class="token punctuation">(</span>Activation<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Sigmoid activation layer."""</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_arr<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        out <span class="token operator">=</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>in_arr<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> out    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> out<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        grad <span class="token operator">=</span> out <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> out<span class="token punctuation">)</span>        <span class="token keyword">return</span> grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>基本的损失函数<code>MSECriterion</code>类，继承自基本的<code>Criterion</code>类：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MSECriterion</span><span class="token punctuation">(</span>Criterion<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Mean Square Error Criterion."""</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> float<span class="token punctuation">:</span>        loss <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>y <span class="token operator">-</span> pred<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> pred<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        grad <span class="token operator">=</span> <span class="token punctuation">(</span>pred <span class="token operator">-</span> y<span class="token punctuation">)</span> <span class="token operator">*</span> y        <span class="token keyword">return</span> grad<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>首先，实现基本的全连接层：<code>FullyConnectedLayer</code>类，它继承自基本的<code>Layer</code>类：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FullyConnectedLayer</span><span class="token punctuation">(</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Fully connected layer."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span> out_dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span> activator<span class="token punctuation">:</span> Activation<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Basic parameters</span>        self<span class="token punctuation">.</span>_in_dim <span class="token operator">=</span> in_dim        self<span class="token punctuation">.</span>_out_dim <span class="token operator">=</span> out_dim        self<span class="token punctuation">.</span>_activator <span class="token operator">=</span> activator        <span class="token comment" spellcheck="true"># Weights</span>        self<span class="token punctuation">.</span>_w <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>out_dim<span class="token punctuation">,</span> in_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_b <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>out_dim<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Input, output and gradients</span>        self<span class="token punctuation">.</span>_in <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_out <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_grad_w <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_grad_b <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_arr<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_in_dim <span class="token operator">==</span> in_arr<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_in <span class="token operator">=</span> in_arr        self<span class="token punctuation">.</span>_out <span class="token operator">=</span> self<span class="token punctuation">.</span>_activator<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>            np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> in_arr<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_out    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sensitivity<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_out_dim <span class="token operator">==</span> sensitivity<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_grad_b <span class="token operator">=</span> sensitivity <span class="token operator">*</span> self<span class="token punctuation">.</span>_activator<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_out<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_grad_w <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_grad_b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_in<span class="token punctuation">)</span>        sense <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_grad_b<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">)</span>        <span class="token keyword">return</span> sense    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> learn_rate<span class="token punctuation">:</span> float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_w <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> self<span class="token punctuation">.</span>_grad_w        self<span class="token punctuation">.</span>_b <span class="token operator">-=</span> learn_rate <span class="token operator">*</span> self<span class="token punctuation">.</span>_grad_b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后基于这个类，我们可以写出一个三层的全连接网络模型（继承自<code>Module</code>类，当然这个类只是名字和pytorch的命名碰瓷一下～实际实现比较简单）：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FullyConnectNN</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Basic a simple fully connect neural network model."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dim_list<span class="token punctuation">:</span> list<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Initialize layers</span>        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>dim_list<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            in_dim <span class="token operator">=</span> dim_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            out_dim <span class="token operator">=</span> dim_list<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span>            layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>                FullyConnectedLayer<span class="token punctuation">(</span>in_dim<span class="token punctuation">,</span> out_dim<span class="token punctuation">,</span> SigmoidActivation<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_layers <span class="token operator">=</span> layers        self<span class="token punctuation">.</span>_dims <span class="token operator">=</span> dim_list        self<span class="token punctuation">.</span>_lr <span class="token operator">=</span> <span class="token number">0.03</span>    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> float<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>_dims<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">and</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> self<span class="token punctuation">.</span>_dims<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token string">'criterion'</span> <span class="token keyword">in</span> kwargs <span class="token operator">and</span> isinstance<span class="token punctuation">(</span>kwargs<span class="token punctuation">[</span><span class="token string">'criterion'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                                    Criterion<span class="token punctuation">)</span>        <span class="token keyword">assert</span> <span class="token string">'epoch'</span> <span class="token keyword">in</span> kwargs <span class="token operator">and</span> isinstance<span class="token punctuation">(</span>kwargs<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> int<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token string">'lr'</span> <span class="token keyword">in</span> kwargs <span class="token operator">and</span> isinstance<span class="token punctuation">(</span>kwargs<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> float<span class="token punctuation">)</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>_lr <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span>        criterion <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'criterion'</span><span class="token punctuation">]</span>        epoch <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token keyword">for</span> _ <span class="token keyword">in</span> range<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">for</span> i<span class="token punctuation">,</span> row <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>                pred <span class="token operator">=</span> self<span class="token punctuation">.</span>_forward<span class="token punctuation">(</span>row<span class="token punctuation">)</span>                loss <span class="token operator">+=</span> criterion<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>                grad <span class="token operator">=</span> criterion<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_backward<span class="token punctuation">(</span>grad<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> row <span class="token keyword">in</span> x<span class="token punctuation">:</span>            pred<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_forward<span class="token punctuation">(</span>row<span class="token punctuation">)</span><span class="token punctuation">)</span>        pred <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>pred<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred    <span class="token keyword">def</span> <span class="token function">_forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_arr<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>_layers<span class="token punctuation">:</span>            in_arr <span class="token operator">=</span> layer<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>in_arr<span class="token punctuation">)</span>        <span class="token keyword">return</span> in_arr    <span class="token keyword">def</span> <span class="token function">_backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sense<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>_layers<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            layer<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>sense<span class="token punctuation">)</span>            layer<span class="token punctuation">.</span>update<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_lr<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="测试：MNIST实战"><a href="#测试：MNIST实战" class="headerlink" title="测试：MNIST实战"></a>测试：MNIST实战</h2><p>待补充……</p>]]></content>
      
      
      <categories>
          
          <category> 见习炼丹师 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>见习炼丹师-01：深度学习基础</title>
      <link href="/Deep-Learning-01-Overview/"/>
      <url>/Deep-Learning-01-Overview/</url>
      
        <content type="html"><![CDATA[<p>本文是“见习炼丹师”系列第一篇文章，主要内容为深度学习的理论基础知识。</p><p>建议搭配我的另一个专栏“机器学不动了”食用。</p><p>全系列推荐结合个人实现的代码阅读：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork和pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>机器学习的专栏写了一半，感觉有必要开一个深度学习相关的专栏～</p><p>所谓深度学习，就是在普通机器学习算法的背景下，使用神经网络进行分类、回归等等工作。</p><p>这个专栏主要是督促自己把基本的几个神经网络实现一遍，并且更了解目前主流的算法。</p><h2 id="深度学习理论"><a href="#深度学习理论" class="headerlink" title="深度学习理论"></a>深度学习理论</h2><p>从之前分析感知机和神经网络关系的<a href="/Machine-Learning-07-Perceptron-and-Neural-Network">内容</a>说起。</p><p>神经网络的基本组成为三部分：</p><ul><li>网络结构（net topology）（全连接层、池化层等等组成的层次结构）</li><li>节点性质（processor characteristics）（一般节点都是线性单元，这里的性质主要是激活函数的选取）</li><li>训练法则（training rules）（一般特指后向传播的计算法则）</li></ul><p>那么本文先从这三部分讲解神经网络到底是个啥。</p><h3 id="节点性质"><a href="#节点性质" class="headerlink" title="节点性质"></a>节点性质</h3><p>首先，节点又叫做神经元，目前的神经网络中，每一个节点都是线性单元，即感知机的结构：$y=w^Tx+b$</p><p>而神经元的另一个组成部分就是激活函数，一般选择有sigmoid、tanh、ReLU等。</p><p>出于避免梯度消失的考虑，现代神经网络中一般选取ReLU作为激活函数。</p><h3 id="训练法则"><a href="#训练法则" class="headerlink" title="训练法则"></a>训练法则</h3><p>所谓训练法则，主要是指后向传播（Backpropagation）过程：根据前向传播的结果去更新网络中的权重，使得目标函数接近最小/最大值。</p><p>后向传播算法主要为一阶的梯度下降算法和二阶的Hessian方法，由于后者的计算成本较高（需要计算二阶导数），所以不采用。</p><blockquote><p>原理参考泰勒展开：$f(x) = f(x_k)+(x-x_k)f’(x_k)+\frac{1}{2!}(x-x_k)^2f’’(x_k)+o^n$</p><p>其中，梯度下降涉及一阶的余项，而Hessian涉及二阶的余项。</p></blockquote><p>且看下图（关于后向传播的具体内容，参考感知机那一篇文章）：</p><p><img src="/Deep-Learning-01-Overview/net.png" alt></p><p>此外，训练法则还包括后向传播过程的优化方式，即自适应调节学习率或者梯度的方法。</p><p>例如，Keras中就包含了以下优化算法：</p><ul><li>SGD：随机梯度下降</li><li>SGD+Momentum: 基于动量的SGD（在SGD基础上做过优化）</li><li>SGD+Nesterov+Momentum：基于动量，两步更新的SGD（在SGD+Momentum基础上做过优化）</li><li>Adagrad：自适应地为各个参数分配不同学习速率</li><li>Adadelta： 针对Adagrad问题，优化过的算法（在Adagrad基础上做过优化）</li><li>RMSprop：对于循环神经网络（RNNs）是最好的优化器（在Adadelta基础上做过优化）</li><li>Adam：对每个权值都计算自适应的学习速率（在RMSprop基础上做过优化）</li><li>Adamax：针对Adam做过优化的算法（在Adam基础上做过优化）</li></ul><h3 id="网络法则"><a href="#网络法则" class="headerlink" title="网络法则"></a>网络法则</h3><p>其实现在大部分的研究都在网络法则上作文章，因为前两部分是深度学习的基石，已经大致完善而不易改动；而网络结构千变万，到目前为止已经出现的主要结构有：</p><ul><li><p>全连接神经网络（Fully Connected Neural Network，FCNN）</p></li><li><p>卷积神经网络（Convolutional Neural Network，CNN）</p></li><li><p>循环神经网络（Recurrent Neural Network，RNN）</p></li><li><p>长短时记忆网络（Long Short Term Memory Network，LSTM）</p></li><li><p>递归神经网络（Recursive Neural Network, RNN）</p></li></ul><p>全连接神经网络是最基础的一种结构，它只有线性单元构成的全连接层组成，作为演示说明用，一般不会使用这种网络……</p><p>值得注意的是，循环神经网络和递归神经网络都简写作RNN，但是目前一般RNN特指循环神经网络。</p><p>此外，对各个领域有不同的具体网络结构，比如NLP界有Attention，Transformer等。</p><p>对于这些网络的具体理论和实现，后续文章将会深入分析。</p>]]></content>
      
      
      <categories>
          
          <category> 见习炼丹师 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一次反反爬虫——Jikipedia Crawler</title>
      <link href="/Jikipedia-Crawler/"/>
      <url>/Jikipedia-Crawler/</url>
      
        <content type="html"><![CDATA[<p>课程项目做个垂直搜索引擎，爬了一下小鸡词典——一个各种网络热词的百科，大概词条有上万个吧。</p><p>没想到这个过程还挺曲折复杂的，在这里记录一下过程用到的各种反反爬的技巧。</p><p>主要使用python的Scrapy和Selenium框架，代码在<a href="https://github.com/Riroaki/Meme-Crawler">这里</a>，里面还包括一些其他的爬虫（B站、微博等）。</p><h2 id="什么，这样一个小破站还玩反爬？"><a href="#什么，这样一个小破站还玩反爬？" class="headerlink" title="什么，这样一个小破站还玩反爬？"></a>什么，这样一个小破站还玩反爬？</h2><p>一开始我看了一下网页结构也不复杂，页面就一瀑布流，词条也很轻量级。</p><p>然后看了一下url，词条是按序号排列的：<a href="https://jikipedia.com/definition/{id}">https://jikipedia.com/definition/{id}</a></p><p>其中id目测在0-10000之间。当然超过10000的id也有，但是比较少，我们也不打算爬了。</p><p>可惜没找到请求的API，看来只能解析网页了。</p><p>呵呵，小破站估计很快就爬完了。于是我启动Scrapy就开始撸代码。</p><h3 id="前期思路"><a href="#前期思路" class="headerlink" title="前期思路"></a>前期思路</h3><ul><li>感觉网页信息挺少的，就需要获取【词条名称+词条文本+浏览点赞评论数等等】，打算先存到文本txt，再按需放到数据库。</li><li>用一个字典存已经爬取的id和对应的词，为了支持断点续爬需要用pickle存到文件，下次爬取的时候恢复。<ul><li>没想太多，id用的是线性增长，到10000的时候截止。</li></ul></li><li>对网页内容解析，正则提取就ok了 。具体规则还请看源码<code>jikipedia.py</code>。</li></ul><p>当然，这个时候<code>headers</code>信息该做的伪装都做了：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># settings.py</span>DEFAULT_REQUEST_HEADERS <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'User-Agent'</span><span class="token punctuation">:</span> <span class="token punctuation">(</span><span class="token string">'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X)'</span>                   <span class="token string">' AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0'</span>                   <span class="token string">' Mobile/15A372 Safari/604.1'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">'Token'</span><span class="token punctuation">:</span> <span class="token string">'81dca7415bd81f859fcdc968afd19be1d9015f01126142bb907181bd3dbd0098'</span><span class="token punctuation">,</span>    <span class="token string">'Accept'</span><span class="token punctuation">:</span> <span class="token string">'application/json, text/plain, */*'</span><span class="token punctuation">,</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>写的差不多了，走你。看着数据文件夹下文件数蹭蹭上涨，感觉甚是爽快——<s>仿佛自己把这个站日了一遍</s>然而我的愉悦立刻被打断了，什么，被shutdown了？？</p><p>emm，打开网页地址一看：</p><p><img src="/Jikipedia-Crawler/moss.png" alt></p><p>这么潮的吗，结果跳出一个验证码。还是拖动拼图式的……好吧我怂了。</p><h2 id="绕道通行"><a href="#绕道通行" class="headerlink" title="绕道通行"></a>绕道通行</h2><p>好吧，咱们想想解决方案：</p><p>首先，当然是：</p><h3 id="改UA"><a href="#改UA" class="headerlink" title="改UA"></a>改UA</h3><ul><li>看了一下网站的robots.txt，貌似对baidubot和googlebot是来者不拒啊。</li><li>于是就去抄了一堆这两家的UA，大致是这些：</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># settings.py</span><span class="token comment" spellcheck="true"># User-Agent to choose from</span>UA_LIST <span class="token operator">=</span> <span class="token punctuation">[</span>    <span class="token comment" spellcheck="true"># Google-bot headers</span>    <span class="token string">'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'Mozilla/5.0 (Linux; Android 6.0.1; Nexus 5X Build/MMB29P)'</span>     <span class="token string">' AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2272.96'</span>     <span class="token string">' Mobile Safari/537.36 (compatible; Googlebot/2.1;'</span>     <span class="token string">' +http://www.google.com/bot.html)'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">'Googlebot/2.1 (+http://www.google.com/bot.html)'</span><span class="token punctuation">,</span>    <span class="token string">'Mozilla/5.0 (compatible; Googlebot/2.1; http://www.google.com/bot.html)'</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># Baidu-bot headers</span>    <span class="token punctuation">(</span><span class="token string">'Mozilla/5.0 (compatible; Baiduspider/2.0;'</span>     <span class="token string">'+http://www.baidu.com/search/spider.html）'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token punctuation">(</span><span class="token string">'Mozilla/5.0 (compatible;Baiduspider-render/2.0;'</span>     <span class="token string">' +http://www.baidu.com/search/spider.html)'</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>再加一个中间件随机选取，代码大概长这样：</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># middlewares.py</span><span class="token keyword">class</span> <span class="token class-name">RandomUserAgentMiddlware</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Modify random user-agent each request."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> _<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>RandomUserAgentMiddlware<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    @classmethod    <span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> cls<span class="token punctuation">(</span>crawler<span class="token punctuation">)</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>request<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        _ <span class="token operator">=</span> spider  <span class="token comment" spellcheck="true"># Use this to silent warning</span>        <span class="token comment" spellcheck="true"># Change User-Agent</span>        <span class="token keyword">if</span> len<span class="token punctuation">(</span>UA_LIST<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>            new_ua <span class="token operator">=</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>UA_LIST<span class="token punctuation">)</span>            request<span class="token punctuation">.</span>headers<span class="token punctuation">[</span><span class="token string">'User-Agent'</span><span class="token punctuation">]</span> <span class="token operator">=</span> new_ua<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>记得把<code>settings.py</code>里面的中间件配置改掉：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># settings.py</span>DOWNLOADER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'MemeCrawler.middlewares.RandomUserAgentMiddlware'</span><span class="token punctuation">:</span> <span class="token number">543</span><span class="token punctuation">,</span>    <span class="token string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span><span class="token punctuation">:</span> None<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="下载等待"><a href="#下载等待" class="headerlink" title="下载等待"></a>下载等待</h3><p>感觉不太放心，加点等待时间吧。咱们这里看起来是固定等待，当然Scrapy内部还是采用随机等待的，这里的设置只是加个限制。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># settings.py</span><span class="token comment" spellcheck="true"># Configure a delay for requests for the same website (default: 0)</span><span class="token comment" spellcheck="true"># See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay</span><span class="token comment" spellcheck="true"># See also autothrottle settings and docs</span>DOWNLOAD_DELAY <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>好，继续。</p><p>结果这次爬了一会，连续到50个的时候就得开网页人工破解一下验证码。</p><p>我寻思效率还行吧，偶尔打断一下我看番的时间。</p><p>结果，再反复这样玩了3，4次之后，当我划开验证码跳出来的不是成功以后恢复的界面，而是……</p><p><img src="/Jikipedia-Crawler/manual.png" alt></p><p>这小破站可真会玩？？？不是，我寻思这个网站也不是很大吧……</p><p>咋办？难道还要我不停地换ip，开手机热点爬，买个ip池（算了，我穷）？</p><h2 id="道高一尺，魔高一丈？"><a href="#道高一尺，魔高一丈？" class="headerlink" title="道高一尺，魔高一丈？"></a>道高一尺，魔高一丈？</h2><p>想了一下，为啥我的请求会不给通过？</p><h3 id="随机序列id访问"><a href="#随机序列id访问" class="headerlink" title="随机序列id访问"></a>随机序列id访问</h3><p>好吧，可能是因为我请求是按照id顺序增长的，这个也很明显……</p><p>那只能随机化顺序了。于是在使用文件存储已经爬取的id的基础上，我把1-10000中剩下来待爬取的id取出做成一个随机序列，每次从序列取一个id出来。这总不能判断我是机器人了吧？</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># spiders/jikipedia.py</span><span class="token keyword">class</span> <span class="token class-name">JikiSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># ...omit other details...</span>    <span class="token keyword">def</span> <span class="token function">init_index</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Load from index file and shuffle index</span>        <span class="token keyword">if</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>ENTRY_INDEX_FILE<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">with</span> open<span class="token punctuation">(</span>ENTRY_INDEX_FILE<span class="token punctuation">,</span> <span class="token string">'rb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>                saved<span class="token punctuation">:</span> dict <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            saved <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        <span class="token comment" spellcheck="true"># Get unsaved index</span>        all_index <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>self<span class="token punctuation">.</span>max_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>        all_index<span class="token punctuation">[</span>list<span class="token punctuation">(</span>saved<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        rest_index <span class="token operator">=</span> np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>all_index <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Shuffle index</span>        np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>rest_index<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>next_sequence <span class="token operator">=</span> rest_index        self<span class="token punctuation">.</span>saved_dict <span class="token operator">=</span> saved        self<span class="token punctuation">.</span>current_index <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>    <span class="token keyword">def</span> <span class="token function">next_index</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> int<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Find index of next entry that is not saved</span>        self<span class="token punctuation">.</span>current_index <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># Return -1 if reach end of sequence</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>current_index <span class="token operator">&lt;</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>next_sequence<span class="token punctuation">)</span><span class="token punctuation">:</span>            next_id <span class="token operator">=</span> int<span class="token punctuation">(</span>self<span class="token punctuation">.</span>next_sequence<span class="token punctuation">[</span>self<span class="token punctuation">.</span>current_index<span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            next_id <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>        <span class="token keyword">return</span> next_id    <span class="token keyword">def</span> <span class="token function">close</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">,</span> reason<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Dump save record before close spider</span>        logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>reason<span class="token punctuation">)</span>        <span class="token keyword">with</span> open<span class="token punctuation">(</span>ENTRY_INDEX_FILE<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>            pickle<span class="token punctuation">.</span>dump<span class="token punctuation">(</span>self<span class="token punctuation">.</span>saved_dict<span class="token punctuation">,</span> f<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Selenium拟人操作"><a href="#Selenium拟人操作" class="headerlink" title="Selenium拟人操作"></a>Selenium拟人操作</h3><p>又想了想可能还是请求的时候只请求网页而没有顺带地加载css样式和js脚本，那么用浏览器可能好点。</p><p>这么一说，那Selenium走起啊。</p><p>在网上找了一些资料，于是加了一个下载中间件，把原来的request拦截后用Selenium获得response再返回给spider。</p><p>在这个过程，我又加了一些小trick：</p><ul><li>在网页加载中增加随机等待（sleep就行），模拟人的阅读过程</li><li>在网页加载后随机下划页面，模拟人的阅读过程</li><li>在网页加载后点击查看评论，然后再随机等待一会</li><li>在网页加载后随机点赞</li></ul><p><strong>我都点赞了！如果还觉得我是机器人，你是不是也太智能了点？</strong></p><p><img src="/Jikipedia-Crawler/1.jpg" alt></p><p>大致代码如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># middlewares.py</span><span class="token keyword">class</span> <span class="token class-name">SeleniumMiddleware</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Middleware of browsing pages by selenium."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        driver <span class="token operator">=</span> webdriver<span class="token punctuation">.</span>Chrome<span class="token punctuation">(</span>executable_path<span class="token operator">=</span>DRIVER_PATH<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Set window size and position</span>        driver<span class="token punctuation">.</span>set_window_position<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>        driver<span class="token punctuation">.</span>set_window_size<span class="token punctuation">(</span><span class="token number">1400</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Set timeout parameters</span>        driver<span class="token punctuation">.</span>set_page_load_timeout<span class="token punctuation">(</span>TIMEOUT<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>driver <span class="token operator">=</span> driver        self<span class="token punctuation">.</span>wait <span class="token operator">=</span> WebDriverWait<span class="token punctuation">(</span>driver<span class="token punctuation">,</span> timeout<span class="token operator">=</span>TIMEOUT<span class="token punctuation">,</span>                                  poll_frequency<span class="token operator">=</span>POLL_FREQUENCY<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>like_rate <span class="token operator">=</span> <span class="token number">0.4</span>  <span class="token comment" spellcheck="true"># Possibility of clicking `like` button</span>        logger<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">'Selenium driver is starting...'</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__del__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>driver <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">:</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">,</span>                        spider<span class="token punctuation">:</span> scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> HtmlResponse<span class="token punctuation">:</span>        _ <span class="token operator">=</span> spider  <span class="token comment" spellcheck="true"># Use this to silent warning</span>        <span class="token keyword">try</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>get<span class="token punctuation">(</span>request<span class="token punctuation">.</span>url<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Use random sleep</span>            sleep<span class="token punctuation">(</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> RANDOM_SLEEP_BASE<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 404 not found</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>page_source<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">'这个页面找不到了'</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>                response <span class="token operator">=</span> HtmlResponse<span class="token punctuation">(</span>url<span class="token operator">=</span>request<span class="token punctuation">.</span>url<span class="token punctuation">,</span>                                        request<span class="token operator">=</span>request<span class="token punctuation">,</span>                                        status<span class="token operator">=</span><span class="token number">404</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># 200 success</span>            <span class="token comment" spellcheck="true"># Moss CAPTCHA</span>            <span class="token keyword">elif</span> self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>page_source<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">'hello moss'</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># You can try manually do the CAPTCHA</span>                sleep<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># Jiki will be closed</span>                response <span class="token operator">=</span> HtmlResponse<span class="token punctuation">(</span>url<span class="token operator">=</span>request<span class="token punctuation">.</span>url<span class="token punctuation">,</span>                                        body<span class="token operator">=</span>self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>page_source<span class="token punctuation">,</span>                                        request<span class="token operator">=</span>request<span class="token punctuation">,</span>                                        encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">,</span>                                        status<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># Get item height</span>                card <span class="token operator">=</span> self<span class="token punctuation">.</span>wait<span class="token punctuation">.</span>until<span class="token punctuation">(</span>                    expected_conditions<span class="token punctuation">.</span>presence_of_element_located<span class="token punctuation">(</span>                        <span class="token punctuation">(</span>By<span class="token punctuation">.</span>CSS_SELECTOR<span class="token punctuation">,</span> <span class="token string">'.full-card'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                height <span class="token operator">=</span> card<span class="token punctuation">.</span>size<span class="token punctuation">[</span><span class="token string">'height'</span><span class="token punctuation">]</span>                <span class="token comment" spellcheck="true"># Scroll page</span>                scroll <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'var q=document.documentElement'</span>                          <span class="token string">'.scrollTop={};'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>height <span class="token operator">*</span> random<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>execute_script<span class="token punctuation">(</span>scroll<span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># Click comment button</span>                comment <span class="token operator">=</span> self<span class="token punctuation">.</span>wait<span class="token punctuation">.</span>until<span class="token punctuation">(</span>                    expected_conditions<span class="token punctuation">.</span>element_to_be_clickable<span class="token punctuation">(</span>                        <span class="token punctuation">(</span>By<span class="token punctuation">.</span>CSS_SELECTOR<span class="token punctuation">,</span> <span class="token string">'.comment'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                comment<span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># Randomly click like button</span>                <span class="token keyword">if</span> random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>like_rate<span class="token punctuation">:</span>                    like <span class="token operator">=</span> self<span class="token punctuation">.</span>wait<span class="token punctuation">.</span>until<span class="token punctuation">(</span>                        expected_conditions<span class="token punctuation">.</span>element_to_be_clickable<span class="token punctuation">(</span>                            <span class="token punctuation">(</span>By<span class="token punctuation">.</span>CSS_SELECTOR<span class="token punctuation">,</span> <span class="token string">'.like.button'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    like<span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true"># Form response</span>                response <span class="token operator">=</span> HtmlResponse<span class="token punctuation">(</span>url<span class="token operator">=</span>request<span class="token punctuation">.</span>url<span class="token punctuation">,</span>                                        body<span class="token operator">=</span>self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>page_source<span class="token punctuation">,</span>                                        request<span class="token operator">=</span>request<span class="token punctuation">,</span>                                        encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">,</span>                                        status<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>        <span class="token keyword">except</span> TimeoutException <span class="token keyword">as</span> e<span class="token punctuation">:</span>            logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>e<span class="token punctuation">)</span>            response <span class="token operator">=</span> HtmlResponse<span class="token punctuation">(</span>url<span class="token operator">=</span>request<span class="token punctuation">.</span>url<span class="token punctuation">,</span>                                    request<span class="token operator">=</span>request<span class="token punctuation">,</span>                                    status<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">)</span>        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>            logger<span class="token punctuation">.</span>warning<span class="token punctuation">(</span>e<span class="token punctuation">)</span>            response <span class="token operator">=</span> HtmlResponse<span class="token punctuation">(</span>url<span class="token operator">=</span>request<span class="token punctuation">.</span>url<span class="token punctuation">,</span>                                    request<span class="token operator">=</span>request<span class="token punctuation">,</span>                                    status<span class="token operator">=</span><span class="token number">404</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use random sleep after comments are shown</span>        sleep<span class="token punctuation">(</span>random<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> RANDOM_SLEEP_BASE<span class="token punctuation">)</span>        <span class="token keyword">return</span> response    @classmethod    <span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>        _ <span class="token operator">=</span> crawler  <span class="token comment" spellcheck="true"># Use this line to silent warning</span>        <span class="token keyword">return</span> cls<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>嗯，这样效果好多了。</p><p>被查封的概率小了很多，然而——效率也贼低（它竟然偶尔还说我频率过高然后要我验证……再慢就属于老年人的浏览速度了啊摔）。</p><p>好吧，至少我现在可以一边自己看着词条，一边让它一直爬了。</p><p>好像还有点快乐？哈哈哈。</p><p><img src="/Jikipedia-Crawler/success.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> Web Crawler </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scrapy </tag>
            
            <tag> selenium </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-08：K近邻</title>
      <link href="/Machine-Learning-08-K-Nearest-Neighbor/"/>
      <url>/Machine-Learning-08-K-Nearest-Neighbor/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第八篇文章，内容包含了K近邻算法的理论和实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>这一次我们介绍的是K近邻（K Nearest Neighbor）算法，它属于有监督学习中比较简单和直觉的分类算法。</p><p>古人云，“近朱者赤，近墨者黑”，又有云，“物以类聚，人以群分”，大意就是说性质相似的人和事容易聚在一块。</p><p>正是因为有相似的特征，所以被归为一类，这同样可以应用在分类的思想。</p><p>于是就有了我们的K近邻算法，它能够把新的样本迅速归类到和它最相似的那些样本里面去。</p><h2 id="K近邻理论"><a href="#K近邻理论" class="headerlink" title="K近邻理论"></a>K近邻理论</h2><p>有了原理，具体操作就是选取K个和新样本最近的样本的类别的众数作为新样本的类。</p><p>接下来我们需要细化定义这个算法：</p><ul><li>如何定义样本之间的距离远近？</li><li>如何选取K？</li></ul><p>首先很直觉地，我们选取欧式距离作为距离的衡量指标：$d(x_i,x_j)=||x_i-x_j||^2$</p><p>对于K的选取，我们既不能选太少，也不能选太多：</p><ul><li><p>少了，不足以作为参考，可能被偶然的噪声影响，样本分类受扰动影响大。</p></li><li><p>多了，徒劳增大计算量，还容易收到占比比较多的样本数量影响。</p><ul><li>当然样本不均衡的影响在哪里都有，只是这里问题比较明显</li></ul></li></ul><p>另外，应当选取K为奇数。这样一来就杜绝了模棱两可的可能性——因为一定有一个类标签是出现了最多次的。</p><p>至于具体选择就见仁见智，一般可以多次选取K，找到分类面比较光滑（对扰动影响不大）而分类准确率较高的那个值。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> scipy<span class="token punctuation">.</span>stats<span class="token keyword">from</span> <span class="token punctuation">.</span>_base <span class="token keyword">import</span> SupervisedModel<span class="token keyword">class</span> <span class="token class-name">KNearest</span><span class="token punctuation">(</span>SupervisedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""K-Nearest-Neighbor model, multi-class classifier."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>__data <span class="token operator">=</span> None        self<span class="token punctuation">.</span>__label <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>int<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Lazy training for knn, no computations</span>        self<span class="token punctuation">.</span>__data <span class="token operator">=</span> x        self<span class="token punctuation">.</span>__label <span class="token operator">=</span> label        class_count <span class="token operator">=</span> len<span class="token punctuation">(</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> class_count    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>__data <span class="token keyword">is</span> <span class="token operator">not</span> None <span class="token operator">and</span> self<span class="token punctuation">.</span>__label <span class="token keyword">is</span> <span class="token operator">not</span> None        <span class="token keyword">assert</span> <span class="token string">'k'</span> <span class="token keyword">in</span> kwargs <span class="token operator">and</span> kwargs<span class="token punctuation">[</span><span class="token string">'k'</span><span class="token punctuation">]</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>__data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        k <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'k'</span><span class="token punctuation">]</span>        pred_label <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i<span class="token punctuation">,</span> xi <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>            dist <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>self<span class="token punctuation">.</span>__data <span class="token operator">-</span> xi<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>            top_idx <span class="token operator">=</span> np<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span>dist<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span> k<span class="token punctuation">]</span>            top_label <span class="token operator">=</span> self<span class="token punctuation">.</span>__label<span class="token punctuation">[</span>top_idx<span class="token punctuation">]</span>            pred_label<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> scipy<span class="token punctuation">.</span>stats<span class="token punctuation">.</span>mode<span class="token punctuation">(</span>top_label<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use 0-1 loss</span>        loss <span class="token operator">=</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">!=</span> label<span class="token punctuation">)</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> loss <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="有效参数"><a href="#有效参数" class="headerlink" title="有效参数"></a>有效参数</h2><p>这个算法很特别，它实际上不存在“训练”的过程，只是把所有的样本，以及K作为参数。</p><p>那么算法的参数量就是$N+1$吗？好像也不是。</p><p>对于比较小的K，分类面可以极其复杂：</p><p><img src="/Machine-Learning-08-K-Nearest-Neighbor/k=1.png" alt></p><p>而比较大的K会使得分类面比较光滑：</p><p><img src="/Machine-Learning-08-K-Nearest-Neighbor/k=15.png" alt></p><p>我们知道，分类面越复杂需要描述它的参数量越大。</p><p>可以看出K的大小和模型参数的关系：K越小，参数量越多。</p><p>于是，初步判断模型的参数量（这里，我们采用一个模糊的说法，实际上应该叫做“有效参数量”）$p=\frac{N}{K}$</p><p>有效参数量（Effective number of parameters）的证明在此省略。</p><p>于是本专栏最短的一篇文章就完成了：）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-06：核方法——非线性SVM</title>
      <link href="/Machine-Learning-06-Kernel-Method/"/>
      <url>/Machine-Learning-06-Kernel-Method/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第六篇文章，内容包含了核方法的基本理论。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上一篇文章介绍SVM，并总结了线性分类器的特征。</p><p>现在我们要探究SVM的极限——是否可以对非线性但可分的数据进行分类？</p><p>（说到极限……<s>我不做人类了，JoJo！</s>）</p><p>我们看一个二次可分的问题：</p><p><img src="/Machine-Learning-06-Kernel-Method/2.png" alt></p><p>对上面这个图，形似圆环的两个类别的数据分布，显然不是线性可分的。那么所有的线性分类器（逻辑回归、SVM，以及之后介绍的Perceptron）都会失效。</p><p>如果手动做的话，我们以内圆的圆心为原点，视横坐标，纵坐标的表示为$[x_1,x_2]$，找到介于两个圆半径之间的长度$r$，那么满足$x_1^2+x_2^2-r^2\le0$为1类，反之则为2类，如此便可实现区分。</p><p>对于一个二维的平面，我们口算尚可一战；然而对问题稍做变换，情况又如何呢？</p><p>设想一个映射将原图放入三维空间：</p><p><img src="/Machine-Learning-06-Kernel-Method/3d.png" alt></p><p>这种情况下，我该如何搞定呢？或者情况不是圆……</p><p>这时候我们需要回过头把我们在二维做的事情进行泛化——<strong>找到更为一般和通用的形式</strong>，以不变应万变嘛。</p><p>注意刚才的不等式$x_1^2+x_2^2 - r^2$的形式，本质还是可以看作一个线性组合——如果令$z_0=1,z_1=x_1^2,z_2=x_2^2$那么我们还是可以用线性可分的分类器去划分，学习得到$w=[-r^2,1,1]^T$的参数使得数据可以被$w^Tz=$划分。</p><p>本质上，我们刚才做了一次坐标变换，从而把原来类的数据转换到一个空间，在那个空间里各个类的分布是线性可分的。</p><p>更为一般的形式是包含一次项和其他二次项的（毕竟你不会总是对这种圆进行分类，圆心也未必在原点）：</p><ul><li>变换前：$x=[1,x_1,x_2,…,x_n]$</li><li>变换后：$z=\begin{bmatrix} 1&amp;x_1&amp;x_2&amp;…&amp;x_n\x_1&amp;x_1x_1&amp;x_1x_2&amp;…&amp;x_1x_n\…\x_n&amp;x_nx_1&amp;x_nx_2&amp;…&amp;x_nx_n\end{bmatrix}$</li></ul><p>就获得了这样的新坐标，这个新坐标的维度为$x_ix_j,i&amp;j\in[0,n]$，所以新的坐标有$((n+1)^2-(n+1))/2=\frac{n(n+1)}{2}$个不同的维度。</p><p>然后我们将变换后的坐标变为新的$x$，这样一来训练一个$w\in \R^{n(n+1)/2\times1}$来预测，就把问题转化为线性的分类工作。</p><blockquote><p>其实在之前<a href="/Machine-Learning-03-Linear-Regression">线性回归</a>讲正则化的时候就已经做过类似的工作了，只不过那个时候的坐标变换是一个预处理的工作，即改变数据集的维度，而不是模型的工作。</p></blockquote><p>那么现在，我们如何把这种变换加入线性的分类器，让它能够自动帮我们搞定非线性的情况呢？</p><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>我们把上面这种从原feature到新feature的映射叫做核函数$\kappa$。</p><p>对上面的变换，我们有$\hat f(x)=w^Tx=\Sigma_{i=1}^n\alpha_ix_i^Tx=\Sigma_{i=1}^n\alpha_i\kappa(x_i, x)$</p><p>从而$\kappa(x_i,x)=x_i^Tx$</p><p>把它应用在SVM中，得到新的公式：</p><p>$y=sgn(w^Tx+b)=sgn(\Sigma_{i=1}^n\alpha_iy_i&lt;x_i,x&gt;+b), w=\Sigma_{i=1}^n\alpha_iy_ix_i$</p><p>在SVM中，对非支持向量的点，$\alpha_i$的值为0。</p><p>核函数更为一般的定义是：</p><blockquote><p>核函数是用于衡量两个对象$x,x’$的<strong>相似性</strong>关系的函数，通常它是：</p><ul><li>非负的，即$\kappa(x,x’)\ge0$</li><li>对称的，即$\kappa(x,x’)=\kappa(x’,x)$</li></ul></blockquote><p>通常的核函数有如下几种：</p><ul><li><code>Linear kernel</code>:$\kappa(x,x’)=x^Tx’$</li><li><code>Polynomial kernel</code>:$\kappa(x,x’)=(x^Tx’+1)^d$</li><li><code>RBF kernel</code>:$\kappa(x,x’)=exp(-\frac{||x-x’||^2}{2\sigma^2})$</li></ul><p>通过引入核函数，我们将原来的线性SVM分类器推广到了非线性的分类。</p><p>此外，还有<code>string kernel</code>和<code>graph kernel</code>，这两个核函数的作用不是把输入投影到新的维度上，而是单纯获取两个向量之间的相似程度。</p><h2 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h2><p>使用核方法意味着你已经知道了数据分布的大致情况（比如二次型），从而能够选取核的形式。</p><p>并不是一劳永逸的方法。哈哈，照这个结论，我可以说“机器学习没有银弹”咯？</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>这一章内容其实我也一知半解，可以看作是对SVM的一种补充吧（然而并没有代码实现）。</p><p>等到有更清楚的说法我会补充，有问题或者意见可以在评论区留言，共同讨论。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-07：感知机与神经网络</title>
      <link href="/Machine-Learning-07-Perceptron-and-Neural-Network/"/>
      <url>/Machine-Learning-07-Perceptron-and-Neural-Network/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第七篇文章，内容包含了感知机的详细理论和实现，以及感知机向深度学习的推广。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>这应该是我们要介绍的最后一个线性分类器了。</p><p>这玩意是60年前提出的（Frank Rosenblatt，1957），一度风靡计算机科学界。</p><p>当时人工智能的概念就开始🔥了。<s>不过这股热情迅速被1969年的一篇论文浇灭</s>。</p><p>问题在于，这玩意根本就是个线性分类器，没办法拟合非线性的函数（论文里用异或运算打脸，也就是说它连异或都不能拟合）。另外，相对当时的计算能力，这个算法还是太复杂。</p><p>于是迎来数十年的AI寒冬，直到SVM的出现带来AI的复兴——只不过Rosenblatt本人在1971年意外离世，看不到后来的故事了。</p><p>值得注意的是，1970年backpropagation的算法就已经提出了，只不过当时没有得到注意，直到1984年被重新发明。这个可是现代神经网络的基石啊。只能说，历史总是螺旋式上升、曲折发展的吧。</p><h2 id="感知机理论"><a href="#感知机理论" class="headerlink" title="感知机理论"></a>感知机理论</h2><p>感知机是一个线性二分类器。它的算法和之前的逻辑回归、SVM类似：</p><p>$f(x)=w^Tx+b$，$f(x)\ge0$表示正类，反之表示负类。</p><p>而它的学习过程，事实上有点像是个拍脑袋的算法：</p><ul><li>对于$x_i$如果预测正确，那就不做处理；</li><li>如果预测错误，那就执行$w_t=w_{t-1}+x_iy_i$的操作。</li></ul><p>我们先从直观角度理解一下这一操作的合理性：</p><ol><li>$y_i=\pm1$，如果预测错误，那么$w_{t-1}x_iy_i&lt;0$（预测和真实值异号）</li><li>假如我们重新预测这个值，会发现$w_t^Tx_i=(w_{t-1}^Tx_i+(x_iy_i)^Tx_i)=w_{t-1}^T+y_ix_i^Tx_i$</li><li>由于$x_i^Tx_i&gt;0$，这样操作相当于引入真实标签信息，使得预测结果向真实情况$y_i$靠近。</li></ol><p>那么按照之前的惯例，我们需要定义一个形式化的损失函数：</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>这就有点先射箭，后画靶子的感觉🎯。</p><p>定义损失函数：$J(w)=-\Sigma_{i\in I_M}w^Tx_iy_i$</p><p>结合梯度下降理论，我们可以得出梯度$\nabla J=\Sigma_{i\in I_M}-x_iy_i$</p><p>所以$w_t=w_{t-1}+\alpha(t)\Sigma_{i\in I_M}x_iy_i$，不过一般就让$\alpha(t)=1$。</p><p>对于$b$的梯度就使用$b_t=b_{t-1}+\Sigma_{i\in I_M}y_i$。</p><p>这样我们就得到了感知机的损失函数和参数估计方法。</p><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>感知机的参数估计就如上文所述：</p><ul><li><p>$w_t=w_{t-1}+\Sigma_{i\in I_M}x_iy_i$</p></li><li><p>$b_t=b_{t-1}+\Sigma_{i\in I_M}y_i$</p></li></ul><p>这样一来，岂不是很容易不收敛？会不会因为样本的值比较大，在收敛的边缘<s>反复横跳</s>振荡之类的……</p><p>不过已经有工作证明了它的收敛性——或者说，感知机对分类存在误差的上界。这里就直接贴图了：</p><p><img src="/Machine-Learning-07-Perceptron-and-Neural-Network/proof1.png" alt></p><p><img src="/Machine-Learning-07-Perceptron-and-Neural-Network/proof2.png" alt></p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>终于到了万众瞩目的代码实现环节。</p><p>本次代码不多说，和线性回归类似。有问题请在评论区留言。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Perceptron</span><span class="token punctuation">(</span>LinearModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Perceptron model, binary classifier."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token keyword">assert</span> np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token keyword">if</span> kwargs <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Update parameters of training</span>            self<span class="token punctuation">.</span>_update_params<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>        iters<span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token comment" spellcheck="true"># Iterates till converge or iterating times exceed bound</span>        <span class="token keyword">while</span> iters <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>_iter_bound<span class="token punctuation">:</span>            iters <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment" spellcheck="true"># Update weights using mini-batch gradient desent</span>            <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_label <span class="token keyword">in</span> batch<span class="token punctuation">(</span>x<span class="token punctuation">,</span> label<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>                pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>                loss <span class="token operator">+=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span> <span class="token operator">*</span> batch_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>                grad_w<span class="token punctuation">,</span> grad_b <span class="token operator">=</span> self<span class="token punctuation">.</span>_grad<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> pred_label<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_w <span class="token operator">-=</span> grad_w                self<span class="token punctuation">.</span>_b <span class="token operator">-=</span> grad_b            loss <span class="token operator">/=</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># Break if model converges.</span>            <span class="token keyword">if</span> loss <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>_loss_tol<span class="token punctuation">:</span>                <span class="token keyword">break</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">-</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_value</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> w<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>                       b<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_val <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b        <span class="token keyword">return</span> pred_val    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_label</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> np<span class="token punctuation">.</span>sign<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        pred_label<span class="token punctuation">[</span>pred_label <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">return</span> pred_label    @staticmethod    <span class="token keyword">def</span> <span class="token function">_loss</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        loss <span class="token operator">=</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>float<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>pred_val <span class="token operator">*</span> true_label<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> true_label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> pred_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>              true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        grad_w <span class="token operator">=</span> <span class="token operator">-</span><span class="token punctuation">(</span>true_label<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> x<span class="token punctuation">)</span><span class="token punctuation">[</span>pred_label <span class="token operator">!=</span> true_label<span class="token punctuation">]</span>        grad_b <span class="token operator">=</span> <span class="token operator">-</span>true_label<span class="token punctuation">[</span>pred_label <span class="token operator">!=</span> true_label<span class="token punctuation">]</span>        grad_w <span class="token operator">=</span> grad_w<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        grad_b <span class="token operator">=</span> grad_b<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> grad_w<span class="token punctuation">,</span> grad_b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>其实上文提到的异或运算可以用多层的感知机组合解决，如下是一种异或模型：</p><p><img src="/Machine-Learning-07-Perceptron-and-Neural-Network/xor.png" alt></p><p>在这个图中，每一个圆都是一个神经元，这里有输入神经元和感知机神经元之分。</p><p>初代的神经网络就是通过感知机组合得到的。</p><p>然而……怎么得到这么一个模型呢？总不能对每一个问题，都人工去凑出一个模型吧？</p><p>为此，我们需要引入后向传播（Backpropagation）算法。</p><h3 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h3><p>后向传播，其实就是梯度下降结合求导的链式法则，对网络的每一层进行梯度下降的参数估计。</p><p>比如说，一个神经网络的结构是$p_1(in)=out_1,p_2(out_1)=out_2,p_3(out_2)-&gt;out_3$（虽然不可能是单链条这么简单，但是足够说明问题），如果说损失函数是$J(out_3)$，那么对三个神经元（感知机）的梯度分别是：</p><ul><li>对3号感知机：$\frac{\partial J}{\partial w_3}=\frac{\partial J}{\partial out_3}\frac{\partial out_3}{\partial w_3}$</li><li>2号：$\frac{\partial J}{\partial w_2}=\frac{\partial J}{\partial out_3}\frac{\partial out_3}{\partial w_2}=\frac{\partial J}{\partial out_3}w_3\frac{\partial out_2}{\partial w_2}$</li><li>1号：$\frac{\partial J}{\partial w_1}=\frac{\partial J}{\partial out_3}\frac{\partial out_3}{\partial out_2}\frac{\partial out_2}{\partial out_1}\frac{\partial out_1}{\partial w_1}=\frac{\partial J}{\partial out_3}w_3w_2\frac{\partial out_1}{\partial w_1}$</li></ul><p>结果不一定准确，但是过程就是这个样子。</p><p>前向传播和后向传播是相对的概念，前向表示从输入层层推进到输出结果的过程，后向就是参数估计的过程。</p><p>然后，为了摆脱感知机的线性性，我们引入一个非线性的激活函数，比如$sigmoid$，$ReLU$等等。于是就变成了深度学习的模型：</p><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p>用一个图来说明深度学习的网络结构与它的后向传播算法：</p><p><img src="/Machine-Learning-07-Perceptron-and-Neural-Network/net.png" alt></p><p>比起机器学习，深度学习的一大优势在于不太需要前期的特征工程。</p><p>在机器学习的过程中，我们首先需要观察数据，选取合适的分类器（逻辑回归、SVM、贝叶斯等等），并且最重要的是：提取出一些有效分类的feature，并进行非线性的变换、降维、升维等等操作。这一个复杂的过程就是特征工程，是一个非常依赖经验和人力的活。</p><blockquote><p>在机器学习领域，想要获得一个好的结果，好的特征比模型设计、训练过程等等都要重要。</p></blockquote><p>而深度学习，你只需要把数据喂进去，选好网络结构，剩下来的事情交给模型自己去学吧——这也是为什么它的解释性差，模型靠后向传播学出来的东西能用什么逻辑解释吗？</p><h3 id="假设空间"><a href="#假设空间" class="headerlink" title="假设空间"></a>假设空间</h3><p>回到一开始的概念，深度学习最大的优势在于它广阔的假设空间。</p><p>有证明指出，仅需要像上图一样的三层网络（输入、隐含层、输出），就可以拟合<strong>任意</strong>（arbitrary）的函数。</p><p>这还是在假设使用同一个激活函数的前提下：</p><p><img src="/Machine-Learning-07-Perceptron-and-Neural-Network/proof3.png" alt></p><p>用形式化的语言描述，就是说：</p><blockquote><p>Any continuous function from input to output can be implemented in a three‐layer net, given sufficient number of hidden units $n_H$, proper nonlinearities, and weights.</p></blockquote><p>这是由A. Kolmogorov证明的，有兴趣可以自行了解～</p><h3 id="要素"><a href="#要素" class="headerlink" title="要素"></a>要素</h3><p>神经网络的三要素有：</p><ul><li>网络结构（net topology）（全连接层、池化层等等组成的层次结构）</li><li>节点性质（processor characteristics）（一般节点都是线性单元，这里的性质主要是激活函数的选取）</li><li>训练法则（training rules）（一般特指后向传播的计算法则）</li></ul><p>常见的神经网络有CNN、RNN、LSTM等。</p><p>关于神经网络的内容，在结束机器学习部分后会更新，请拭目以待。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-05：支持向量机</title>
      <link href="/Machine-Learning-05-Support-Vector-Machine/"/>
      <url>/Machine-Learning-05-Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第五篇文章，内容包含了支持向量机的详细理论和实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上一篇内容中介绍了逻辑回归的内容，并以此展示了线性分类器最基本的形式。</p><p>本文介绍的支持向量机也是一种线性分类器，只不过获得模型的方式有所不同。</p><p>作为复习，我们重新回顾一下逻辑回归的分类方式：</p><p>$y=a^Tx$，在$y&gt;0$的时候被投影到区间$(0.5,1)$，所以分类为正类；</p><p>在$y&lt;0$的时候被投影到区间$(0,0.5)$所以分类为负类。</p><p>$a$被称为分割向量，或者解向量，而每一个分类向量都对应着一个分类平面，把投影到空间中的所有点$X_i$分为两类。</p><p>下图用于解释分类平面与参数$a$（$a=[w,b]$）的关系：</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/wb.png" alt></p><p>那么，这个向量是唯一的吗？</p><p>答案是，在有限且线性可分的数据集上，一般是存在无数可行的分类面/分类向量的，如下图：</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/hyperplane.png" alt></p><p>那么很自然的我们就会有一个问题，怎样分类才是最优的呢？</p><p>这个最优的平面，理应是能够帮助我们在测试数据上有最好的分类效果的：</p><p>直觉上来看，这个分类面不应该过于靠近某一类点（或者甚至插入了同一类点之间），而是让两类点尽量离分类平面较远。因为距离近表示接近边界，那么这些点是最容易被误分类的，我们希望这样的点越少越好——也就是，希望我们的分类器的分类结果都是<strong>极端一点</strong>，<strong>模棱两可的结果尽量少一些</strong>。</p><p>换句话说，每一个点到超平面的距离都应当尽量的大，应对新数据理应有好的表现。</p><p>而事实上超平面稍微改变，影响的主要是和平面近邻的点，所以我们定义概念：</p><blockquote><p>Margin：分类平面到点集的<strong>最小</strong>距离。</p></blockquote><p>用数学化的语言描述，我们所追求的目标就是找到一个分类面可以最大化margin。</p><p>答案已经呼之欲出了，那就是今天的主角——支持向量机（Support Vector Machine）。</p><h2 id="支持向量机理论"><a href="#支持向量机理论" class="headerlink" title="支持向量机理论"></a>支持向量机理论</h2><p>首先复习一下点到平面距离公式：</p><p>对平面$C:y=a_0+a_1x_1+a_2x_2+…+a_nx_n$，任意一个点$x=[x_1,x_2,…,x_n]$到平面的距离为：</p><p>$d=\frac{|a_0+a_1x_1+a_2x_2+…+a_nx_n|}{\sqrt{a_0^2+a_1^2+a_2^2+…+a_n^2}}=\frac{|y(x)|}{||a||_2-a_0^2}$</p><p>因而，我们希望找到的由$a$决定的分类平面应当是：</p><p>$w,b=\arg \max \min \frac{|w^Tx+b|}{||w||_2}$</p><p>同时，我们希望它是分类正确的。虽然距离足够大，但是不保证分类正确那不就本末倒置了吗！</p><p>所以在这个表达式中加入分类的信息：</p><p>$w,b=\arg \max \min \frac{y_i(w^Tx_i+b)}{||w||_2}$</p><p>这里的$y_i\in {-1,1}$，表示正类和负类。由此可见SVM也是一种二分类算法。</p><p>观察这个表达式，如果分类正确，那么$w^Tx_i+b$和$y_i$应该是同号的，反之是异号。</p><p>因而这个表达式是合理的，它同时包含了“分类正确”和“点到平面距离最大”的目标。</p><h3 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h3><p>如果说到此为止，那就太简单了，然而支持向量机这名字还没出来呢！</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/%E5%A4%A9%E5%BA%95%E4%B8%8B%E5%93%AA%E6%9C%89%E8%BF%99%E6%A0%B7%E7%9A%84%E5%A5%BD%E4%BA%8B.jpg" alt></p><p>接着推呗。</p><p>回到上面这个式子：最大化$\frac{y_i(w^Tx_i+b)}{||w||_2}$，是不是单纯最大化分子&amp;最小化分母就可以了？</p><p>当然不是。因为上下都有$w$这一项。</p><p>但是注意到$a$决定了平面之后，等比例地缩放$a$不会改变平面，但是却改变了分子分母的值，对我们的计算带来困扰。我们当然希望控制的变量越少越好，希望分子和分母不要存在关联，这样我们只需要考虑一边就可以。</p><p>所以我们令$y_i(w^Tx_i+b)=1$，这样问题就转变为求$\frac{1}{||w||_2}$的最大值，也就是求$||w||_2$的最小值。</p><p>稍微修整表达，问题被改写为：</p><p>$\min\frac{1}{2}||w||_2^2$</p><p>$s.t. y_i(w^Tx_i+b)\ge1$</p><p>平方和系数$\frac{1}{2}$是为了求导方便和导数形式所确定的，不会对结果有影响。</p><h3 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h3><p>说了这么多，支持向量到底是什么？</p><p>稍微抽象化地想一想：我们的平面是被空间中的点“支撑”起来的，因为它们需要保持一定的距离。就像磁铁的两极相斥一样，平面受到了点的斥力而树立起来。</p><p>所以，这些点就叫做“支持向量”</p><p>而因为我们只考虑最近的点来计算点到平面的距离最小值，所以支持向量指的是靠近平面的那些点。</p><p>所以选取点的个数也是一个超参数，它减少了计算量（原本是对每一个点都进行计算）。</p><h2 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h2><p>说完了基本的理论部分，接下来要对算法进行批判了。</p><p>设想一下，目前的支持向量机算法是建立在“数据线性可分”的基础上，也就是说，所有的点都能够分类在平面两侧。</p><p>假如因为标注错误或者别的特殊情况，数据中有那么几个点不小心分错了，跑到平面的另一侧去了呢？</p><p>对这些点，我们的算法缺少一定的容忍性（鲁棒性），会被这些点（如果它们又刚好足够近，会被选择成为支持向量），那么就会被它们带偏——所以说，一颗老鼠屎坏了一锅粥啊。</p><p>下面就介绍一个补救措施：</p><h3 id="松弛变量"><a href="#松弛变量" class="headerlink" title="松弛变量"></a>松弛变量</h3><p>在应对近似线性可分的问题的时候，我们需要稍微放松要求，允许支持向量的点到平面的距离稍微近一点。</p><p>在公式上我们引入了一个变量，叫做松弛变量（Slack variable）：</p><p>$min\frac{1}{2}||w||<em>2^2+C\Sigma</em>{i=1}^n\epsilon_i$$</p><p>$s.t.y(w^Tx_i+b)\ge 1-\epsilon_i,$$</p><p>$\epsilon_i\ge0$</p><p>这里，$C$是用来控制原目标（最小化$w$）和新目标之间（保证尽量多的margin大于等于1）的权重的，</p><p>与这个做法相对的，我们原来的做法就叫做硬间隔（Hard margin）。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>支持向量机的理论还不止这些内容，但是目前掌握的知识足够我们实现一个简易的支持向量机了。</p><p>本次实现了简化的SVM算法，用到scipy库的优化函数取最小点来完成二次规划的部分。</p><p>代码中已经给出部分注释，如有疑问请在评论区留言。其他详情请见我的<a href="https://github.com/Riroaki/LemonML/">repo</a>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> LinearModel<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>optimize <span class="token keyword">import</span> minimize<span class="token keyword">class</span> <span class="token class-name">SVM</span><span class="token punctuation">(</span>LinearModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Support vector machine model, binary classifier."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Target and constraint functions</span>        <span class="token keyword">def</span> <span class="token function">target</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">get_func</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token keyword">lambda</span> w<span class="token punctuation">:</span> w<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x_ext<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> label<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># Target and constraint functions with slack variables</span>        <span class="token keyword">def</span> <span class="token function">target_slack</span><span class="token punctuation">(</span>w_e<span class="token punctuation">)</span><span class="token punctuation">:</span>            w <span class="token operator">=</span> w_e<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>            eps <span class="token operator">=</span> w_e<span class="token punctuation">[</span><span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>            <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">*</span> w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> c <span class="token operator">*</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>eps<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">get_func_slack_w</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token keyword">lambda</span> w_e<span class="token punctuation">:</span> w_e<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x_ext<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span> \                               <span class="token operator">*</span> label<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">+</span> w_e<span class="token punctuation">[</span>p <span class="token operator">+</span> i<span class="token punctuation">]</span>        <span class="token keyword">def</span> <span class="token function">get_func_slack_e</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token keyword">lambda</span> w_e<span class="token punctuation">:</span> w_e<span class="token punctuation">[</span>p <span class="token operator">+</span> i<span class="token punctuation">]</span>        <span class="token keyword">assert</span> np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># No slack parameters unless explicitly stated</span>        slack <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">if</span> kwargs <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Update parameters of training</span>            self<span class="token punctuation">.</span>_update_params<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Whether to use slack variables</span>            <span class="token keyword">if</span> <span class="token string">'slack'</span> <span class="token keyword">in</span> kwargs<span class="token punctuation">:</span>                <span class="token keyword">assert</span> isinstance<span class="token punctuation">(</span>kwargs<span class="token punctuation">[</span><span class="token string">'slack'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bool<span class="token punctuation">)</span>                slack <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'slack'</span><span class="token punctuation">]</span>        w_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span><span class="token punctuation">)</span>        x_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Find optimum w and b for both condition</span>        <span class="token keyword">if</span> <span class="token operator">not</span> slack<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># SVM without slack</span>            <span class="token comment" spellcheck="true"># Optimize 1/2 w^T * w</span>            <span class="token comment" spellcheck="true"># s.t. yi * (w^T * xi + b) - 1 >= 0</span>            cons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'type'</span><span class="token punctuation">:</span> <span class="token string">'ineq'</span><span class="token punctuation">,</span> <span class="token string">'fun'</span><span class="token punctuation">:</span> get_func<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">}</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># Find optimized w</span>            w_ext <span class="token operator">=</span> minimize<span class="token punctuation">(</span>target<span class="token punctuation">,</span> w_ext<span class="token punctuation">,</span> constraints<span class="token operator">=</span>cons<span class="token punctuation">)</span><span class="token punctuation">.</span>x        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># SVM with slack</span>            <span class="token comment" spellcheck="true"># Optimize 1/2 w^T * w + C * sum(eps_i)</span>            <span class="token comment" spellcheck="true"># s.t. yi * (w^T * xi + b) - 1 + eps_i >= 0, eps_i >= 0</span>            c<span class="token punctuation">,</span> w_and_eps <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>w_ext<span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            cons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>                cons<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'type'</span><span class="token punctuation">:</span> <span class="token string">'ineq'</span><span class="token punctuation">,</span> <span class="token string">'fun'</span><span class="token punctuation">:</span> get_func_slack_w<span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>                cons<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'type'</span><span class="token punctuation">:</span> <span class="token string">'ineq'</span><span class="token punctuation">,</span> <span class="token string">'fun'</span><span class="token punctuation">:</span> get_func_slack_e<span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>            cons <span class="token operator">=</span> tuple<span class="token punctuation">(</span>cons<span class="token punctuation">)</span>            w_and_eps <span class="token operator">=</span> minimize<span class="token punctuation">(</span>target_slack<span class="token punctuation">,</span> w_and_eps<span class="token punctuation">,</span> constraints<span class="token operator">=</span>cons<span class="token punctuation">)</span><span class="token punctuation">.</span>x            w_ext <span class="token operator">=</span> w_and_eps<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Update and save optimal weights &amp; bias</span>        self<span class="token punctuation">.</span>_w <span class="token operator">=</span> w_ext<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_b <span class="token operator">=</span> w_ext<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Calculate loss</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">-</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_value</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> w<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>                       b<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_val <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b        <span class="token keyword">return</span> pred_val    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_label</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> np<span class="token punctuation">.</span>sign<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        pred_label<span class="token punctuation">[</span>pred_label <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">return</span> pred_label    @staticmethod    <span class="token keyword">def</span> <span class="token function">_loss</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Hinge loss</span>        loss <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> pred_val <span class="token operator">*</span> true_label        loss<span class="token punctuation">[</span>loss <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>              true_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use scipy.optmize to find best w and b</span>        <span class="token comment" spellcheck="true"># Not grad-base method</span>        <span class="token keyword">return</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Go-beyond-SVM"><a href="#Go-beyond-SVM" class="headerlink" title="Go beyond SVM"></a>Go beyond SVM</h2><p>为了说明问题的本质，我们需要把SVM转换为更一般的形式。</p><p>优化问题往往需要把问题的限制去除或者转化为可计算的形式，所以我们对上面的问题表述再做转化：</p><p>$\epsilon_i\ge1-y_i(w^Tx_i+b)\\epsilon_i=max[1-y_i(w^Tx_i+b),0]$</p><p>所以优化问题转化为：$\min{\Sigma_{i=1}^n\max[1-y_i(w^Tx_i+b),0]+\frac{1}{2C}||w||_2^2}$</p><p>这个形式，是不是有点像正则化的线性回归？</p><p>左侧看作目标函数，右侧看作Ridge正则项，则目标函数$l(f)=\max[1-yf,0]$</p><ul><li><p>线性回归的目标：$l(f)=(y-f)^2=(1-yf)^2$</p></li><li><p>逻辑回归的目标：$l(f)=log(1+e^{-yf})$</p></li></ul><h3 id="线性分类器的一般形式"><a href="#线性分类器的一般形式" class="headerlink" title="线性分类器的一般形式"></a>线性分类器的一般形式</h3><p>由上述表示我们可以归纳出线性分类器的问题一般形式：</p><p>$\min{\Sigma_{i=1}^nl(f)+\lambda R(f)}$</p><p>其中$l(f)$为损失函数，$R(f)$为正则项。</p><p>上述模型分别对应以下：</p><ul><li>损失函数<ul><li>线性回归：Square loss</li><li>逻辑回归：Logistic loss</li><li>SVM：Hinge loss</li></ul></li><li>正则项<ul><li>Ridge：L2-regularizer</li><li>Lasso：L1-regularizer</li></ul></li></ul><p>不同损失函数关于$yf$的图像：</p><p>BTW，Hinge loss叫做合叶函数，正是因为它的形状就像打开的合叶。</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/loss.png" alt></p><h2 id="非线性SVM：核函数"><a href="#非线性SVM：核函数" class="headerlink" title="非线性SVM：核函数"></a>非线性SVM：核函数</h2><p>刚才我们解决线性问题，那么SVM的能力就仅此而已了嘛？</p><p>能不能对非线性数据也做拟合？</p><blockquote><p><strong>To be continued…</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-04：逻辑回归</title>
      <link href="/Machine-Learning-04-Logistic-Regression/"/>
      <url>/Machine-Learning-04-Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第四篇文章，内容包含了逻辑分类的详细理论和实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上回说到线性回归相关的理论，那么接下来就顺着线性回归讲解线性分类器。</p><p>线性分类器是指分类函数符合$y=w^Tx+b$形式的一系列判别模型，主要包括逻辑回归、支持向量机、感知机等，接下来会首先介绍三个线性分类器本身，再会介绍线性分类器更为一般的形式和广泛的联系。</p><h2 id="逻辑回归理论"><a href="#逻辑回归理论" class="headerlink" title="逻辑回归理论"></a>逻辑回归理论</h2><p>逻辑回归（Logistic Regression）虽然是叫回归，但是实际上是一个分类器。</p><p>逻辑回归得名于逻辑函数（Logistic Funtion），也叫做S函数（Sigmoid Function）。</p><h3 id="逻辑函数"><a href="#逻辑函数" class="headerlink" title="逻辑函数"></a>逻辑函数</h3><p>$\sigma(t)=\frac{e^t}{1+e^t}=\frac{1}{1+e^{-t}}$</p><p>这个函数具有以下特征：</p><ul><li>可以将$(-\infty,+\infty)$之间的输入值映射到$(0,1)$区间</li><li>呈现S形，单调递增：输入值为负时输出在$(0,0.5)$之间，非负输出为$[0.5,1)$。</li></ul><p>一图胜千言：</p><p><img src="/Machine-Learning-04-Logistic-Regression/logistic.png" alt></p><p>这个函数也是<a href="https://en.wikipedia.org/wiki/Logistic_distribution">逻辑分布</a>的累计分布函数。</p><p>逻辑回归使用逻辑函数估计输入参数与类变量的概率，最一般的形式为二分类：</p><ul><li>$P(y_i=1|x_i,a)=\sigma(a^Tx_i)=\frac{1}{1+e^{-a^Tx_i}}$</li><li>$P(y_i=0|x_i,a)=1-\sigma(a^Tx_i)=\frac{1}{1+e^{a^Tx_i}}$</li></ul><p>合并二式，有：$P(y_i|x_i,a)=\sigma(y_ia^Tx_i)=\frac{1}{1+e^{-y_ia^Tx_i}}$</p><p>或者：$P(y_i|x_i,a)=y_i\sigma(a^Tx_i)+(1-y_i)(1-\sigma(a^Tx_i))=y_i\frac{1}{1+e^{-a^Tx_i}}+(1-y_i)(1-\frac{1}{1+e^{-a^Tx_i}})$，</p><p>或者：$P(y_i|x_i,a)=\sigma(a^Tx_i)^{y_i}(1-\sigma(a^Tx_i))^{1-y_i}=…$</p><blockquote><p>P.S.其实合并得到的式子我感觉是凑出来的……</p></blockquote><p>这就是最基本的逻辑回归的形式。</p><p>这个式子也隐含着：判断是类1的概率和类2的概率之和为1。</p><p>另外，这两个式子还具有一个规律：</p><p>$\frac{P(y_i=1|x_i,a)}{P(y_i=0|x_i,a)}\ln\frac{\sigma(a^Tx_i)}{1-\sigma(a^Tx_i)}=a^Tx_i$，所以逻辑回归也叫做对数几率回归。</p><p>实际判断的时候，我们会把输出大于0.5的分类作为正类，输出小于0.5的值的分类作为负类，从而学习到一个参数为$w,b$的模型。</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>如何评价模型，决定了我们获取参数的方式。</p><p>在这里，对于逻辑回归这一个概率模型，我们使用极大似然估计作为目标函数：</p><p>我们有对单个输入的概率估计$P(y_i|x_i,a)$（这里使用上面的<strong>第三个</strong>式子），那么对全体的数据集$D$，总的概率估计为：</p><p>$P(D)=\prod_{i\in I}P(y_i|x_i,a)=\prod_{i\in I}(\sigma(a^Tx_i)^{y_i}(1-\sigma(a^Tx_i))^{1-y_i})$</p><p>这是一个关于$a$的函数,我们希望$P(D)$取极大，也就是估计正确的概率达到最大。</p><p>因而，需要对$P(D)$关于$a$求导。但是，连乘的形式难以求导，所以我们通过取对数把函数形式转化为连加——这在之前的内容也提及了。</p><p>$l(P(D))=\Sigma_{i\in I}y_ilog(\sigma(a^Tx_i))+(1-y_i)log(1-\sigma(a^Tx_i))$</p><p>因为这个函数是凹的，或者说是上凸的，我们再进行一次转换：$E(a)=-\frac{1}{m}l(P(D))$，可以证明，$E(a)$是一个可导的凸函数。</p><p>那么目标函数就变为$E(a)$，我们要求其最小值。</p><blockquote><p>极大似然估计（MLE）在之前的内容已经出现过，不过这里的形式与贝叶斯分类器中的极大似然估计稍有不同，因为这里的估计中，<strong>概率就是似然</strong>，极大似然估计是对概率与实际分布关系作分析；而贝叶斯分类器中，极大似然估计就是对似然与实际分布的关系作分析（当然，从另一个角度，似然就是后验概率）。</p></blockquote><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>首先对$E(a)$进行变换：</p><p>$E(a)=-\frac{1}{m}\sum_{i=1}^{m}[y_ilog(\frac{e^{a^Tx_i}}{1+e^{^{a^Tx_i}}})+(1-y_i)log(\frac{1}{1+e^{a^Tx_i}})]$</p><p>$=-\frac{1}{m}\Sigma_{i=1}^m[y_ia^Tx_i+log(1+e^{a^Tx_i})]$</p><p>接下来求梯度：</p><p>$\frac{\partial E(a)}{\partial a}=-\frac{1}{m}\Sigma_{i=1}^m[y_ix_i-\frac{e^{a^Tx_i}}{1+e^{a^Tx_i}}x_i]$</p><p>$=-\frac{1}{m}\Sigma_{i=1}^m[y_ix_i-\sigma(a^Tx_i)x_i]$</p><p>记$\sigma(a^Tx_i)$为$h_a(x_i)$，那么上式又可以表示为：$-\frac{1}{m}\Sigma_{i=1}^m[(y_i-h_a(x_i))x_i]$</p><blockquote><p>联想一下线性回归的梯度：$\frac{\partial E(a)}{\partial a}=-\frac{1}{m}\Sigma_{i=1}^m[(y_i-wx_i+b)x_i]$，</p><p>可以说形式非常相似了。</p></blockquote><p>那么和线性回归相似的，我们试着用两种方式求解：</p><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>让梯度为0，我们有：$y_i=\frac{1}{1+e^{-a^Tx_i}}$</p><p>首先令$z_i=a^Tx_i$，我们有：$y_i=\frac{1}{1+e^{-z_i}}$</p><p>所以$z_i=-log(\frac{1}{y_i}-1)=log(y_i)-log(1-y_i)$</p><p>那么我们有$a^Tx_i=log(y_i)-log(1-y_i)$</p><p>看起来是没错，那么能不能代入，通过求解方程组得到$a$呢？</p><p><strong>很遗憾，不能。</strong></p><p>我们很快就会注意到，对于$y_i=0,1$，上面这个式子是没有意义的。</p><p>那么为什么我们不能用这个方式求解呢？<strong>因为梯度是不会等于0的</strong>：$\sigma(z_i)$是不可能达到$0,1$的，所以我们至多是把梯度降低，永远不会出现$y_i=\frac{1}{1+e^{-a^Tx_i}}$的结果。</p><p>所以，正规方程解是不存在的。</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>那么很自然的，我们使用梯度下降法求解。</p><p>梯度代入即可，类似线性回归的解：</p><p>$grad = \alpha * -\frac{1}{m}\Sigma_{i=1}^m[(y_i-wx_i+b)x_i]$</p><p>$a-=grad$</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>内容相似，参见上一篇文章（<a href="/Machine-Learning-03-Linear-Regression">线性回归</a>）的有关内容。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>又到了代码实现环节。</p><p>这里实现了最基本的逻辑回归<code>LogisticRegression</code>类（不包含正则化部分）。</p><p>代码中包含部分注释，如有疑问请在评论区留言。其他详情见我的<a href="https://github.com/Riroaki/LemonML">repo</a>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> LinearModel<span class="token keyword">from</span> utils <span class="token keyword">import</span> batch<span class="token keyword">class</span> <span class="token class-name">LogisticRegression</span><span class="token punctuation">(</span>LinearModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Logistic regression model, binary classifier."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Check labels: only containing 1 and 0</span>        <span class="token keyword">assert</span> np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token keyword">if</span> kwargs <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Update parameters of training</span>            self<span class="token punctuation">.</span>_update_params<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>        iters<span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token comment" spellcheck="true"># Iterates till converge or iterating times exceed bound</span>        <span class="token keyword">while</span> iters <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>_iter_bound<span class="token punctuation">:</span>            iters <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment" spellcheck="true"># Update weights using mini-batch gradient desent</span>            <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_label <span class="token keyword">in</span> batch<span class="token punctuation">(</span>x<span class="token punctuation">,</span> label<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>                pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>                loss <span class="token operator">+=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span> <span class="token operator">*</span> batch_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                grad_w<span class="token punctuation">,</span> grad_b <span class="token operator">=</span> self<span class="token punctuation">.</span>_grad<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> pred_val<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_w <span class="token operator">-=</span> grad_w                self<span class="token punctuation">.</span>_b <span class="token operator">-=</span> grad_b            loss <span class="token operator">/=</span> n            <span class="token comment" spellcheck="true"># Break if model converges.</span>            <span class="token keyword">if</span> loss <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>_loss_tol<span class="token punctuation">:</span>                <span class="token keyword">break</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">-</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_value</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> w<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>                       b<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__sigmoid</span><span class="token punctuation">(</span>raw<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>            res <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>raw<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> res        prob <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b        pred_val <span class="token operator">=</span> __sigmoid<span class="token punctuation">(</span>prob<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_val    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_label</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> np<span class="token punctuation">.</span>sign<span class="token punctuation">(</span>pred_val <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span>        pred_label<span class="token punctuation">[</span>pred_label <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>        pred_label<span class="token punctuation">[</span>pred_label <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">return</span> pred_label    @staticmethod    <span class="token keyword">def</span> <span class="token function">_loss</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use maximum likelihood (log-likelihood loss)</span>        <span class="token comment" spellcheck="true"># loss = 1 / n * (-y * log(wx + b) - (1 - y) * log(wx + b))</span>        <span class="token comment" spellcheck="true"># Here we need to care about the log zero and overflow warning...</span>        mask_val <span class="token operator">=</span> pred_val<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        mask_val<span class="token punctuation">[</span>mask_val <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span>        mask_val<span class="token punctuation">[</span>mask_val <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span>        class1_loss <span class="token operator">=</span> <span class="token operator">-</span>true_label <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>mask_val<span class="token punctuation">)</span>        class0_loss <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> true_label<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> mask_val<span class="token punctuation">)</span>        loss <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>class0_loss <span class="token operator">+</span> class1_loss<span class="token punctuation">)</span> <span class="token operator">/</span> true_label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>              true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#  dc / dw = x * (pred_val - true_label)</span>        grad_w <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">*</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_label<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        grad_b <span class="token operator">=</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_label<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use simple gradient by multiplying learning rate and grad.</span>        grad_w <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        grad_b <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        <span class="token keyword">return</span> grad_w<span class="token punctuation">,</span> grad_b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="推广：从二分类到多分类"><a href="#推广：从二分类到多分类" class="headerlink" title="推广：从二分类到多分类"></a>推广：从二分类到多分类</h2><p>见本系列的第一篇文章（<a href="/Machine-Learning-01-Overview-2">机器学习概念</a>）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-03：线性回归</title>
      <link href="/Machine-Learning-03-Linear-Regression/"/>
      <url>/Machine-Learning-03-Linear-Regression/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第三篇文章，内容包含了线性回归的详细理论和简单线性回归的实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上回讨论了贝叶斯模型，这一模型属于生成模型（Generative Model），基于一定的假设，认为样本是由类按照一定概率模型产生的，然后根据样本学习数据。</p><p>同时我们注意到，在高斯贝叶斯分类器中，如果所有类别共享权重，那么模型就转化为线性的表达式，从而可以使用形如$y=w^Tx+b$的简洁形式建模，直接求出描述分类面的表达式（详见上次附录的证明），这就是判别模型（Discriminant Model）的思路。</p><p>那么接下来我们将深入讨论判别模型，这是直接根据数据学习得到的更加直接的规律。</p><p>线性模型就是一种判别模型，这一次我们讨论它在回归任务上的应用，也就是线性回归。</p><h2 id="线性回归理论"><a href="#线性回归理论" class="headerlink" title="线性回归理论"></a>线性回归理论</h2><p>对$x=[x_1,x_2,x_3,…,x_d]^T\in R^d$，线性函数的形式为$y=w^Tx+b$，其中$w=[w_1, w_2,…, w_d]\in R^d$，$b\in R$。</p><p>这一表达式还有一种表述，那就是将$b$这一项加入$w$中，变成：</p><p>$x=[x_1,x_2,…,x_d,1]^T\in R^{d+1},w=[w_1,w_2,…,w_d,b]^T\in R^{d+1}$</p><p>简单的模型蕴含着不可小视的力量。<strong>任意模型的表达式都可以转化为线性组合，故也可以转化为线性模型。</strong></p><p>比如，多项式可以转化为线性组合的形式：</p><p>$f(x,a)=a_0+a_1x+a_2x^2+…+a_Mx^M=\Sigma_{i=0}^Ma_ix^i$</p><p>可以转化为：$f(x,a)=A^TX$，其中：</p><p>$X=[1,x,x^2,x^3,…,x^M]^T,a=[a_0,a_1,a_2,…,a_M]^T$</p><p>基本理论部分十分简洁明了。</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>这里目标函数通常使用最小二乘误差（Mean Square Error）：</p><p>$J_n(a)=\frac{1}{N}\Sigma_{i=1}^N(y_i-a^Tx_i)^2=(y-X^Ta)^T(y-X^Ta)$</p><p>当然，使用平方残差（Residual Sum of Squares）也是可以的，区别在于比最小二乘误差少了系数$\frac{1}{N}$。</p><p>你可能会问，为什么不使用平均绝对值误差（Mean Absolute Error）：$J_n(a)=\frac{1}{N}\Sigma_{i=1}^N|y_i-a^Tx_i|$，但是它存在一些缺陷：</p><ol><li>在零点处不可导。</li><li>其导出的梯度是常数（+1或者-1），梯度求解容易导致不收敛。</li></ol><p>接下来我们抱着让目标函数最小化的目标，对它求梯度：</p><p>$\nabla J_n=-\frac{2}{N}X(y-X^Ta)$</p><p>事实上，关于最小二乘误差也可以用生成式的思路得到（对理论不感冒的可以跳过这一部分）：</p><p>然后就可以开始愉快地参数估计啦（＾∇＾）</p><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>这里我们有两种估计方法：基于正规矩阵（Normal Equation）的直接求解，或者给予梯度下降的迭代法。</p><h3 id="正规矩阵"><a href="#正规矩阵" class="headerlink" title="正规矩阵"></a>正规矩阵</h3><p>由上面的梯度，我们直接令梯度为0：</p><p>$a=(XX^T)^{-1}Xy$，这就是理论最优解——因为梯度为0，目标函数是完全凸的，所以可以认为训练数据的目标函数达到了最小值。</p><ul><li><p>如果$XX^T$非奇异，那么我们可以获得<strong>唯一解</strong>（奇异情况下使用伪逆，理论上应该是有无数组解）。</p></li><li><p>特别地，如果$X$是方阵（一般不是），那么$a=X^{-T}X^{-1}Xy=X^{-T}y$。</p></li></ul><p>好，然后我们看一下梯度求解的做法：</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>如果你还不知道梯度下降是什么的话，我在<a href="/Machine-Learning-01-Overview-2/">第一篇文章</a>里已经写过梯度下降、批量梯度下降/随机梯度下降这些概念。</p><p>通过迭代的方式更新参数，只需要把梯度乘上一个学习率$\alpha$就可以：</p><p>$grad=\alpha * -\frac{2}{N}X(y-X^Ta)$</p><p>$a-=grad$</p><p>通常，常数项$\frac{2}{N}$会省去，因为学习率是一个可以缩放的值。</p><p>好了，这里再次提出第一篇文章的问题，为什么第一种方法看起来简单直接，而且能够得到确定的“精确解”，而实际操作往往使用基于梯度的做法呢？看起来第二种做法很不精确，而且似乎未必收敛到正确的解。</p><blockquote><p>理由就是，第一个方法的实质是计算方程组的解，涉及求逆矩阵的过程，但是一来计算量大，二来难以保证矩阵非奇异或者非病态的情况下，计算过程对方程组值的扰动非常敏感，噪声带来的误差较大导致结果偏离理论解。</p></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>又到了动手实践的时间了。这一次实现了<code>LinearRegression</code>类基于<code>LinearModel</code>基类，并实现了其抽象方法。在这一份代码中不但实现了基于梯度下降的迭代方法，也实现了基于<code>normal equation</code>的直接求解法。</p><p>暂时没有实现Ridge和Lasso的正则部分，主要是希望和其他线性分类器一同构思实现，把正则化做一个更佳泛用型的模块。</p><p>代码中已经给出部分注释，如有疑问请在评论区留言。其他详情请见我的<a href="https://github.com/Riroaki/LemonML/">repo</a>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> LinearModel<span class="token keyword">from</span> utils <span class="token keyword">import</span> batch<span class="token keyword">class</span> <span class="token class-name">LinearRegression</span><span class="token punctuation">(</span>LinearModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Linear regression model."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token keyword">if</span> kwargs <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Update parameters of training</span>            self<span class="token punctuation">.</span>_update_params<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>        iters<span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token comment" spellcheck="true"># Iterates till converge or iterating times exceed bound</span>        <span class="token keyword">while</span> iters <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>_iter_bound<span class="token punctuation">:</span>            iters <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment" spellcheck="true"># Update weights using mini-batch gradient desent</span>            <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_y <span class="token keyword">in</span> batch<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>                pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>                loss <span class="token operator">+=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span> <span class="token operator">*</span> batch_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                grad_w<span class="token punctuation">,</span> grad_b <span class="token operator">=</span> self<span class="token punctuation">.</span>_grad<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> pred_val<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_w <span class="token operator">-=</span> grad_w                self<span class="token punctuation">.</span>_b <span class="token operator">-=</span> grad_b            loss <span class="token operator">/=</span> n            <span class="token comment" spellcheck="true"># Break if model converges.</span>            <span class="token keyword">if</span> loss <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>_loss_tol<span class="token punctuation">:</span>                <span class="token keyword">break</span>        <span class="token comment" spellcheck="true"># Update model with current weight and bias</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">fit_norm_eq</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Fit x using normal equation</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        x_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>        w_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>pinv<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x_ext<span class="token punctuation">.</span>T<span class="token punctuation">,</span> x_ext<span class="token punctuation">)</span><span class="token punctuation">)</span>        w_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>w_ext<span class="token punctuation">,</span> x_ext<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b <span class="token operator">=</span> w_ext<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w_ext<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Calculate training loss</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_val    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># The precision part of regression is None</span>        precision <span class="token operator">=</span> None        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_value</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> w<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>                       b<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_val <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b        <span class="token keyword">return</span> pred_val    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_label</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># NO labeling in regression.</span>        <span class="token keyword">pass</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">_loss</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> true_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use MSE loss</span>        loss <span class="token operator">=</span> float<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_val<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        loss <span class="token operator">/=</span> <span class="token number">2</span> <span class="token operator">*</span> true_val<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>              true_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use MSE loss</span>        grad_w <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">*</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_val<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        grad_b <span class="token operator">=</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_val<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use simple gradient by multiplying learning rate and grad.</span>        grad_w <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        grad_b <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        <span class="token keyword">return</span> grad_w<span class="token punctuation">,</span> grad_b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>好了，我们的模型看起来很完美不是吗？</p><p>现在我们试着用线性模型去拟合开头提到的多项式曲线$f(x,a)=a_0+a_1x+a_2x^2+…+a_Mx^M=\Sigma_{i=0}^Ma_ix^i$</p><p>假如图线长这样，$y=sin(x)$：</p><p><img src="/Machine-Learning-03-Linear-Regression/polynomial.png" alt></p><p>这种图线，0阶、1阶都是单调，2阶又不能先凹后凸，看起来都拟合不了啊。</p><p>直接上三阶试试：</p><p><img src="/Machine-Learning-03-Linear-Regression/3.png" alt></p><p>看起来还行，但是没办法做到完美贴合。</p><p>我们试试更厉害的，让M=9：</p><p><img src="/Machine-Learning-03-Linear-Regression/9.png" alt></p><p>嚯，厉害了，要不是背景把真实曲线画出来我还真就信了。</p><p>虽然做到训练误差为0，但是我们有理由相信，在真实数据上测试结果一定惨不忍睹。</p><p>结果如下：</p><p><img src="/Machine-Learning-03-Linear-Regression/results.png" alt></p><p>很明显，模型过拟合了。于是可以复习一下第一篇文章关于偏差与方差的理解：</p><blockquote><p>模型越复杂（组成模型的参数越多），方差越大，偏差越小，这是因为模型的描述能力越强；模型越简单，偏差也容易大，很可能无法拟合训练数据。</p></blockquote><p>我们还发现一个特征，那就是越高阶的那个系数绝对值越大：</p><p><img src="/Machine-Learning-03-Linear-Regression/coef.png" alt></p><p>为了避免过拟合，我们引入一种叫做正规化（Regularization）的技巧。</p><p>下面介绍两种正规化技巧，Ridge与Lasso。</p><h3 id="Ridge"><a href="#Ridge" class="headerlink" title="Ridge"></a>Ridge</h3><p>我们使用二阶残差作为目标函数，并引入一个惩罚项，变为：</p><p>$a^*=argmin\Sigma_{i=1}^N(y_i-x_i^Ta)^2+\lambda\Sigma_{j=1}^pa_j^2=(y-X^Ta)^T(y-X^Ta)+\lambda a^Ta$</p><p>计算梯度得到：$\nabla a=-2X(y-X^Ta)+2\lambda a$</p><p>用正规方程的方法，我们可以得到：$a^*=(XX^T+\lambda I)^{-1}Xy$，其中$\lambda$是一个常数。</p><p>再次求解，我们惊奇的发现，高阶的系数变小了：</p><p><img src="/Machine-Learning-03-Linear-Regression/coef1.png" alt></p><p>这就比较耐人寻味了。从理论角度分析一下这个事实：</p><p>岭回归以增大偏差为代价，换取更小的方差——这是随着$\lambda$增大，模型发生的变化。</p><p>在这个多项式中，因为很明显地看到随着模型变得复杂，模型的方差越来越大，岭回归正是降低方差的手段。</p><p><strong>不仅如此，岭回归更大的用处在于。当我们的特征存在较强的线性相关性的时候（可以说在特征方面不满秩），会导致$XX^T$的值很小，甚至趋于奇异。而岭回归会帮我们限制参数绝对值的大小，抑制相关性较强的属性系数。</strong></p><p>所以，岭回归实际上并不只是用在刚才的多项式拟合上，而是对所有存在较多线性相关属性时的通用解决方式。个人理解有点类似“降维”的操作，但是稍有不同。数学上使用术语<strong>压缩估计</strong>（shinkage）描述这一操作。</p><h3 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h3><p>和Ridge相似，唯一的不同在于惩罚项的阶数：</p><p>$a^*=argmin\Sigma_{i=1}^N(y_i-x_i^Ta)^2+\lambda\Sigma_{j=1}^p|a_j|=(y-X^Ta)^T(y-X^Ta)+\lambda ||a||_1$</p><p>它带来的影响也稍有不同，会导致模型的参数大多变成0，也就是得到<strong>稀疏化</strong>的参数。</p><p>用一幅图直观理解：</p><p><img src="/Machine-Learning-03-Linear-Regression/lasso-ridge.png" alt></p><p>红色的椭圆和蓝色的区域的切点就是目标函数的最优解，我们可以看到，如果是圆，则很容易切到圆周的任意一点，但是很难切到坐标轴上，因此没有稀疏；但是如果是菱形或者多边形，则很容易切到坐标轴上，因此很容易产生稀疏的结果。这也说明了为什么Lasso会是稀疏的。</p><h3 id="从贝叶斯角度看待正则化"><a href="#从贝叶斯角度看待正则化" class="headerlink" title="从贝叶斯角度看待正则化"></a>从贝叶斯角度看待正则化</h3><p>这一部分非常耐人寻味，它揭示了贝叶斯概率与线性模型之间存在一定的联系。</p><p>我们假定一个生成模型的输出是这样的：$y=f(x,a)+\epsilon$，其中：</p><ul><li>$f(x,a)$为判别函数</li><li>$\epsilon$为随机噪音，是我们不能直接获取的，我们假定它服从高斯分布：$\epsilon\sim N(0,\sigma)$</li></ul><p>那么根据高斯概率密度公式，$p(y|x,a,\sigma)=\frac{1}{\sigma\sqrt{2\pi}}\exp(-\frac{1}{2\sigma^2}(y-f(x,a))^2)$</p><p>而根据贝叶斯公式，我们有$p(a|D)=\frac{p(D|a)p(a)}{p(D)}$，其中：</p><ul><li>$p(D|a)=p(y|x,a,\sigma)$为似然概率</li><li>$p(a)$为先验概率</li><li>$p(a|D)$为后验概率</li></ul><ol><li><p>对于这个先验，我们选取$p(a)=N(a|0,\lambda^{-1}I)=\frac{1}{(2\pi)^{d/2}|\lambda^{-1}I|^{1/2}}\exp(-\frac{1}{2}(a-0)^T(\lambda^{-1}I)^{-1}(a-0))$</p><ol><li>取对数有：$\ln(p(a))=-\frac{\lambda}{2}a^Ta+c$，$c$为常数</li></ol></li><li><p>对似然，$p(D|a)=\prod_{i=1}^np(y_i|x_i,a,\sigma)$</p><ol><li>我们采用极大似然估计的log-likelihood方法：$\ln(p(D|a))=-\frac{1}{2\sigma^2}(y_i-f(x_i,a)^2))+c(\sigma)$</li></ol></li><li><p>而后验概率正比于先验和似然乘积：$p(a|D)\propto p(D|a)p(a)$</p><ol><li>取对数，我们有：$\ln(p(a|D))\propto\ln(p(D|a))+\ln(p(a))$，答案呼之欲出</li></ol></li></ol><p>于是，我们把两项加起来得到：$ln(p(a|D))\propto -\frac{1}{2\sigma^2}(y_i-f(x_i,a)^2)-\frac{\lambda}{2}a^Ta$</p><p>我们希望这一项最大（即最大后验概率），就是希望其相反数最小，也就是：</p><p>$a=argmin \frac{1}{2\sigma^2}(y_i-f(x_i,a)^2)+\frac{\lambda}{2}a^Ta$</p><p>看，这不就是Ridge正则化的损失函数？</p><h3 id="从量化的偏差、方差与噪音的角度看待正则化（待补充说明）"><a href="#从量化的偏差、方差与噪音的角度看待正则化（待补充说明）" class="headerlink" title="从量化的偏差、方差与噪音的角度看待正则化（待补充说明）"></a>从量化的偏差、方差与噪音的角度看待正则化（待补充说明）</h3><p>到这里我们从量化的角度来看这三个概念：</p><p>我们用一个概念来代表模型的总误差：Expected Prediction Error，EPE。</p><p>量化的计算如下（目前我对这部分理解不深，这一部分待补充）：</p><p>$EPE(f)=\int\int(y-f(x))^2p(x,y)dxdy$</p><p>$EPE=var+bias^2+noise$</p><ul><li>$bias^2=\int{E_D(f(x;D))-E(y|x)}^2p(x)dx$</li><li>$variance=\int E_D{[f(x;D)-E_D(f(x;D))]^2}p(x)dx$</li><li>$noise=\int var(y|x)p(x)dx$</li></ul><p>这里贴出不同$\lambda$参数对偏差与方差的影响：</p><p><img src="/Machine-Learning-03-Linear-Regression/-2.4.png" alt></p><p><img src="/Machine-Learning-03-Linear-Regression/-0.31.png" alt></p><p><img src="/Machine-Learning-03-Linear-Regression/2.6.png" alt></p><p>可以很明显的看出，正则项降低了图线的拟合程度，但是也降低了方差（即不同预测线的变化幅度）。</p><p>从而，我们对Bias-Variance的Trade-off有了更深的理解：</p><p><img src="/Machine-Learning-03-Linear-Regression/tradeoff.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-02：贝叶斯分类</title>
      <link href="/Machine-Learning-02-Bayes/"/>
      <url>/Machine-Learning-02-Bayes/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第二篇文章，内容包含了贝叶斯分类的详细理论和简单高斯贝叶斯分类器的实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>贝叶斯分类的核心是贝叶斯公式：</p><ul><li>$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li><li>$P(A) = \Sigma_{i=1}^n{P(B|A_i)P(A_i)}$</li><li>贝叶斯公式对多元变量同样适用，与变量是否独立也无关，是普适的公式。</li></ul><p>为了介绍这个公式，我们首先来看一道概率题：</p><blockquote><p>现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，而设定从A中抽取的概率和B中抽取的概率为1:2。</p><p>现已知从这两个容器里任意抽出了一个红球，问这个球来自容器 A 的概率是多少?</p></blockquote><p>记抽中红球的事件为$P(B)$，记从容器A抽球的概率为$P(A)$。</p><p>根据贝叶斯公式，我们有：$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$。其中$P(B|A)$表示从容器A中抽球，抽到红球的概率。</p><p>在这个公式中：</p><ul><li>$P(A|B)$是已知B发生后A的条件概率，也叫做A的后验概率（posterior probability）。</li><li>$P(B|A)$是已知A发生后B的条件概率，是B的后验概率，在这里叫A的似然概率（likelihood）。</li><li>$P(A)$是事件发生之前我们对A的经验知识，与B无关，叫做A的先验概率（prior probability）。</li><li>$P(B)$是B的先验概率，在这里叫做标准化常量（normalized constant）。</li><li>根据这个关系，后验$P(A|B)$也可以叫做<strong>标准化</strong>的似然；似然和后验是可以相互转化的。</li></ul><h2 id="贝叶斯分类理论"><a href="#贝叶斯分类理论" class="headerlink" title="贝叶斯分类理论"></a>贝叶斯分类理论</h2><p>从这个公式引申开，我们可以套用在分类理论上：</p><ul><li>我们可以类比认为每一个类对应一个容器，样本都是这个类中生成（取出）的。</li><li>分类问题可以采用这样的表述：已知一个待归类样本$X_i$的特征，那么求$X_i$属于第j个类的概率，就变成了一个后验概率。</li><li>把样本属于第j个类的概率记作事件$w_j$，这个后验概率可以表述为：$P(w_j|x=X_i)$，简记作$P(w_j|X_i)$。</li><li>那么，根据贝叶斯公式，我们有：$P(w_j|X_i)=\frac{P(X_i|w_j)P(w_j)}{P(X_i)}$。</li><li>这里的似然是$P(X_i|w_j)$，先验概率是$P(w_j)$，标准化常量为$P(X_i)$。</li></ul><p>那么，有了某个样本属于各个类别的概率，如何分类呢？</p><p>很自然的，我们选择后验概率比较大的那一个概率对应的类别作为$X_i$的分类。</p><ul><li>补充1：当我们只有先验概率的时候，我们选择先验概率较大的那一个类别作为分类。用先验概率直接估计的坏处在于”马太效应”，因为它总是把新样本归类到原本占多数的那一个类。</li><li>补充2：当采用风险矩阵（risk matrix）进行评估的时候，分类规则会更复杂一些。</li></ul><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>我们已经有了概率的公式和决策理论，如何估计概率公式中的各个概率？</p><p>答案是：从有类标签的数据（训练数据）中总结提取。</p><h3 id="先验概率的估计"><a href="#先验概率的估计" class="headerlink" title="先验概率的估计"></a>先验概率的估计</h3><p>这里的先验概率，就是在没有训练样本具体特征的值的分布情况下，某个类原始的信息。</p><p>很自然的，我们会把这个类别的样本数占全部有标签的样本的比重当作先验概率，</p><p>即：$P(w_j)=\Sigma_{i=1}^N{I(y_i=c_j)}/N$</p><h3 id="似然概率的估计"><a href="#似然概率的估计" class="headerlink" title="似然概率的估计"></a>似然概率的估计</h3><p>这里我们需要分为连续变量和离散变量两种情况讨论：</p><h4 id="连续变量"><a href="#连续变量" class="headerlink" title="连续变量"></a>连续变量</h4><p>我们有不同的假设可以做出不同的估计。常用的有高斯分布假设、二项分布假设、伯努利分布。</p><p>这里只介绍高斯分布假设对应的参数估计方法。</p><ul><li>高斯分布的具体假设：对于某一个类$c_i$，其生成的样本满足高斯分布，即：$X\sim N(\mu, \Sigma)$，其产生的每一个样本之间的概率是互相独立且同分布的（i.i.d，Independent and identically distributed）</li><li>在这里我们采用极大似然估计（Maximum Likelihood Estimation）的做法来选取参数：<ul><li>目标函数是$L(\mu_j,\Sigma_j)=\prod_{i=1}^N{P(X_i|w_j)}$。</li><li>由于不方便对目标函数求导，我们采用取对数的技巧，将连乘转化为连加：$l(\mu_j,\Sigma_j)=log(L)=\Sigma_{i=1}^NP(X_i|w_j)$</li><li>对对数似然求导，令导数为0求出$\mu,\Sigma$。由于样本是多维，求导过程比较复杂，详情参考附录。</li><li>总之，最终求出的结果和标量形式的惊人一致：<ul><li>$\hat\mu_j=\frac{1}{N_j}\Sigma_{i=0}^{N_j}X_i=\bar{x}$</li><li>$\hat\Sigma_j=\frac{1}{N_j}\Sigma_{i=1}^{N_j}(X_i-\hat\mu)(X_i-\hat\mu)^T=cov(X_i), where\ y_i = w_j$</li><li>很容易联想到标量情况下，$\hat\mu=\bar{X}, \hat\sigma=var(X)$</li></ul></li></ul></li><li>有了估计的参数以后，我们可以通过高斯概率密度公式求似然概率：$P(X; \mu, \Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(X-\mu)^T\Sigma^{-1}(X-\mu))$</li></ul><blockquote><p><strong>P.S.其实我有一点疑问🤔️，为什么这里不能使用梯度求解而是直接令梯度为0求解证明的呢，有没有大佬能够在评论区说一说自己的想法……</strong></p><p>2019.07.08更新：和评论区同学讨论了一下，这里应该是没有必要梯度求解哈哈。实在无法求出闭式解也该用EM算法（见无监督学习部分）。</p><p>PS：上文“<strong>最佳估计</strong>”不准确，已经修改。极大似然估计只是某一种估计方法而已，还有矩估计等无偏的估计方法，只是没有MLE流传广。</p></blockquote><h4 id="离散变量"><a href="#离散变量" class="headerlink" title="离散变量"></a>离散变量</h4><p>对于离散变量的估计则较为简单，我们选取以前这一特征出现过的值的分布情况作为估计，即：</p><p>$P(X_i^k|w_j)=\frac{|X_i^k|}{N_{w_j}}$</p><p>比如在某个类的性别特征中，男性出现了100次，女性出现了200次，我们就估计一个这个属性为男性的似然概率为100/300=0.33333…，对女性同理。</p><h3 id="标准化常量"><a href="#标准化常量" class="headerlink" title="标准化常量"></a>标准化常量</h3><ul><li>对于连续变量来说，理论上是通过$P(X_i)=\Sigma_{j=1}^cP(X_i|w_j)P(w_j)$求出概率密度，也就是说得算出对样本对每一个类的先验概率和似然的乘积之和，才能计算分母。</li><li>对于离散变量来说，则是求出这一个一模一样的样本在训练数据中出现的次数。<ul><li>如果过去没有出现过这一样本，我们总不能把0作为它的概率，这是不合理的（一来有可能是样本数量过少，二来人家是分母啊，怎么能是0），所以采用平滑（Smoothing）的技术，在分子分母同时加入一个与样本数有关的平滑项。具体技术在此不做详细介绍。</li></ul></li></ul><p>然而正常情况下，它并不在我们的考虑范围内：</p><p>因为，每一个类的计算概率的分母都是这一项。既然我们最终的目标是比较后验概率的大小（或者与风险矩阵关联后的大小，whatever），这一项作为相同的系数并不会产生影响：）</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>在这里因为我们并不是采取迭代优化，而是通过假设模型直接估计得出参数，所以不需要目标函数可导。</p><p>采用0-1损失函数就可以，即分类错误，结果就加一，分类正确结果就不变化的函数。</p><p>在贝叶斯中分类中，还有一个常见的评价标准（metrics）：混淆矩阵和风险矩阵，具体见上一篇<a href="/Machine-Learning-01-Overview-2">机器学习概念</a>的后半段。</p><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>朴素贝叶斯（Naive Bayes）模型是贝叶斯模型的一个变种，是简化贝叶斯参数的一种模型。</p><p>在朴素贝叶斯中，我们认为各个特征之间是独立的，互相不会影响，即：</p><p>$P(X_i|w_j)=P(X^1_i,X^2_i,X^3_i,…X_i^p|w_j)=P(X_i^1|w_j)P(X_i^2|w_j)P(X_i^3|w_j)…P(X_i^p|w_j)$</p><p>$=\prod_{k=1}^pP(X_i^k|w_j)$</p><p>将这一假设应用在高斯模型中，贝叶斯-高斯模型的$\Sigma$参数就退化为对角矩阵，因为它本质是协方差矩阵，如今各个特征之间的协方差为0，每个特征服从独立的高斯分布。</p><p>所以计算公式也变得简单，只需要单独计算每一维度的高斯分布概率再相乘就可以计算。</p><p>朴素贝叶斯分类器至今也是一个简单的流行分类器，不过在很多时候，样本的属性往往是相关的，这种情形下使用朴素贝叶斯模型就不太好。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>好了，说了这么多终于要上代码了～</p><p>这里实现了高斯贝叶斯多分类器，继承自<code>SupervisedModel</code>基类（详见本人的<a href="https://github.com/Riroaki/LemonML/">repo</a>），主要方法实现了<code>fit,predict,evaluate</code>，代码中包含一定的注释，如有疑问可以在评论区留言。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> SupervisedModel<span class="token keyword">class</span> <span class="token class-name">Bayes</span><span class="token punctuation">(</span>SupervisedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Bayes model, multi-class (or binary) classifier.    Bayes models include Gaussian, Multinomial, Bernoulli,    however here I only implemented Gaussian.    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_prior_dict <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_mean_dict <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_cov_dict <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_cov_all <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_p <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_mean_dict <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_cov_dict <span class="token keyword">is</span> None \                <span class="token operator">or</span> self<span class="token punctuation">.</span>_prior_dict <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_p <span class="token operator">!=</span> p<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>_prior_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            self<span class="token punctuation">.</span>_mean_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            self<span class="token punctuation">.</span>_cov_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            self<span class="token punctuation">.</span>_p <span class="token operator">=</span> p        <span class="token comment" spellcheck="true"># Calculate mean and co-variance matrix for each class</span>        all_class <span class="token operator">=</span> np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span>        <span class="token keyword">for</span> c <span class="token keyword">in</span> all_class<span class="token punctuation">:</span>            group <span class="token operator">=</span> x<span class="token punctuation">[</span>label <span class="token operator">==</span> c<span class="token punctuation">]</span>            mean<span class="token punctuation">,</span> cov <span class="token operator">=</span> self<span class="token punctuation">.</span>__param_gaussian<span class="token punctuation">(</span>group<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>_prior_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> group<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> n            self<span class="token punctuation">.</span>_mean_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> mean            self<span class="token punctuation">.</span>_cov_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> cov        <span class="token comment" spellcheck="true"># Calculate the whole co-variance matrix</span>        _<span class="token punctuation">,</span> cov <span class="token operator">=</span> self<span class="token punctuation">.</span>__param_gaussian<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_cov_all <span class="token operator">=</span> cov        <span class="token comment" spellcheck="true"># Calculate loss on x</span>        _<span class="token punctuation">,</span> loss <span class="token operator">=</span> self<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>x<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_cov_dict <span class="token keyword">is</span> <span class="token operator">not</span> None <span class="token operator">and</span> self<span class="token punctuation">.</span>_mean_dict <span class="token keyword">is</span> <span class="token operator">not</span> None        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_cov_all <span class="token keyword">is</span> <span class="token operator">not</span> None        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_p <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Default: non-linear classifier</span>        linear <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">if</span> <span class="token string">'linear'</span> <span class="token keyword">in</span> kwargs<span class="token punctuation">:</span>            <span class="token keyword">assert</span> isinstance<span class="token punctuation">(</span>kwargs<span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bool<span class="token punctuation">)</span>            linear <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Calculate posterior propability for each class</span>        <span class="token comment" spellcheck="true"># All class share a same co-variance matrix if linear == True</span>        prob<span class="token punctuation">,</span> label_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> c<span class="token punctuation">,</span> mean <span class="token keyword">in</span> self<span class="token punctuation">.</span>_mean_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> linear<span class="token punctuation">:</span>                cov <span class="token operator">=</span> self<span class="token punctuation">.</span>_cov_all            <span class="token keyword">else</span><span class="token punctuation">:</span>                cov <span class="token operator">=</span> self<span class="token punctuation">.</span>_cov_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span>            prior <span class="token operator">=</span> self<span class="token punctuation">.</span>_prior_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span>            current_prob <span class="token operator">=</span> self<span class="token punctuation">.</span>__posterior_gaussian<span class="token punctuation">(</span>x<span class="token punctuation">,</span> prior<span class="token punctuation">,</span> mean<span class="token punctuation">,</span> cov<span class="token punctuation">)</span>            prob<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current_prob<span class="token punctuation">)</span>            label_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>c<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Get index of class having maximum probability for each x</span>        pred_val <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>prob<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        label_list <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>label_list<span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> label_list<span class="token punctuation">[</span>pred_val<span class="token punctuation">]</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Calculate 0-1 loss</span>        loss <span class="token operator">=</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">-</span> label<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use loss to calculate precision</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> loss <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">__param_gaussian</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Estimate mean and variance."""</span>        mean <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        diff <span class="token operator">=</span> x <span class="token operator">-</span> mean        cov <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>diff<span class="token punctuation">.</span>T<span class="token punctuation">,</span> diff<span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> mean<span class="token punctuation">,</span> cov    @staticmethod    <span class="token keyword">def</span> <span class="token function">__posterior_gaussian</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> prior<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">,</span>                             mean<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> cov<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Calculate posterior probability P(wi | x)."""</span>        <span class="token comment" spellcheck="true"># Calculate likelihood probability:</span>        <span class="token comment" spellcheck="true"># P(xj | wi) ~ 1 / sqrt(det(cov))</span>        <span class="token comment" spellcheck="true"># * exp(-0.5 * (xj - mean)^T * cov^(-1) * (xi - mean))</span>        diff <span class="token operator">=</span> x <span class="token operator">-</span> mean        coef <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>det<span class="token punctuation">(</span>cov<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span>        inv <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>pinv<span class="token punctuation">(</span>cov<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Get exponent for xj (0 &lt; j &lt; n)</span>        exponents <span class="token operator">=</span> np<span class="token punctuation">.</span>apply_along_axis<span class="token punctuation">(</span>            <span class="token keyword">lambda</span> row<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>row<span class="token punctuation">,</span> inv<span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>row<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> diff<span class="token punctuation">)</span>        likelihood <span class="token operator">=</span> coef <span class="token operator">*</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> exponents<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Posterior = prior * likelihood / evidence (omitted)</span>        posterior <span class="token operator">=</span> prior <span class="token operator">*</span> likelihood        <span class="token keyword">return</span> posterior<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="拓展：分类面形状"><a href="#拓展：分类面形状" class="headerlink" title="拓展：分类面形状"></a>拓展：分类面形状</h2><p>一般而言，贝叶斯分类器并不是一个线性分类器。</p><h3 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h3><p>不同类之间的分类面是高斯面之间的相交面，和每个类的模型参数有关。</p><p>用二维特征、多分类的高斯分类面的图像说明会更清晰（概率密度表现为图像中的高度z值，不同的峰表示不同类的中心概率密度）：</p><p><img src="/Machine-Learning-02-Bayes/multiclass.png" alt></p><p>限制了二维特征与二分类的高斯分布模型之后，分类面其实有一些有趣的规律，这是由高斯分布的形状造成的。</p><p>这里以二分类、二维特征的高斯分类模型为例说明这一点，下图展示了不同的参数（$\mu,\Sigma$）带来分类面的不同：</p><p><img src="/Machine-Learning-02-Bayes/2class.png" alt></p><p>对应的概率分布三维图：</p><p><img src="/Machine-Learning-02-Bayes/2class2.png" alt></p><p>那么，高斯模型的参数又是如何影响分类面形状的呢？</p><p>可以证明有以下结论（证明见附录）：</p><ul><li><p>类的$\mu$参数不会影响分类面的性质（线性/非线性），只会改变类在高维空间的中心位置。</p></li><li><p>如果所有特征均是独立分布，且所有类共享协方差矩阵，即：$\Sigma_j=\sigma^2I$，那么分类面是线性的，且两个类之间的分类直线垂直类中心之间的连线。</p></li><li><p>如果所有特征均共享协方差矩阵，即：$\Sigma_j=\Sigma=cov(X)$，那么分类面也是线性的，但是直线会存在一定的倾斜。</p></li><li><p>如果不满足这两个条件之间的任意情况，那么分类面是圆锥曲线（抛物线、双曲线、圆……）。</p></li></ul><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li>对偶尔的数据噪声鲁棒性好，因为使用了假设模型，相当于模型的信息不完全来自数据。</li><li>但是也因为模型自带假设，在不满足假设情形的数据上拟合效果不好。</li></ul><h2 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h2><h3 id="多维高斯参数估计推导（Deriving-the-Maximum-Likelihood-Estimators）"><a href="#多维高斯参数估计推导（Deriving-the-Maximum-Likelihood-Estimators）" class="headerlink" title="多维高斯参数估计推导（Deriving the Maximum Likelihood Estimators）"></a>多维高斯参数估计推导（Deriving the Maximum Likelihood Estimators）</h3><p>来源：<a href="https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian">https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian</a></p><p>或者查看文档版本：<a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf">https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf</a></p><p><img src="/Machine-Learning-02-Bayes/appendix1.png" alt></p><p><img src="/Machine-Learning-02-Bayes/appendix2.png" alt></p><p><img src="/Machine-Learning-02-Bayes/appendix3.png" alt></p><h3 id="高斯分布参数与分类面形状证明"><a href="#高斯分布参数与分类面形状证明" class="headerlink" title="高斯分布参数与分类面形状证明"></a>高斯分布参数与分类面形状证明</h3><p>来源：<a href="https://www.byclb.com/TR/Tutorials/neural_networks/ch4_1.htm">https://www.byclb.com/TR/Tutorials/neural_networks/ch4_1.htm</a></p><p>由于证明讨论过长，篇幅所限，在此不贴出详细证明内容。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-01（上）：机器学习综述</title>
      <link href="/Machine-Learning-01-Overview/"/>
      <url>/Machine-Learning-01-Overview/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第一篇文章的上半部分，内容包含了机器学习的理论综述、算法分类。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork和pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>如今深度学习、数据挖掘、机器学习这些概念已经🔥到成为满大街都是的概念，由于其门槛低（调包）和某些fancy的功能，加上媒体的宣传和高薪的诱惑，无论计算机专业还是非计算机专业出身的人们都热衷于在其中寻找机会，我这个软工的菜🐔也不例外。当然，目前正处于新手期。</p><p>这一个系列主要记录了我在ZJU上数据挖掘课学习和梳理的机器学习的知识，并且包含一些额外的补充知识。具体内容包括了数学理论和代码实现，希望能够给入门者（包括我自己）提供一个参考。</p><p>由于本人懒癌晚期，博客将不定期更新。</p><p>读者如果有问题或者留言可以直接在相关的博文下面留言，可以共同探讨解决。</p><p>当然也可以邮件联系本人：<a href="mailto:lilq1285@163.com">lilq1285@163.com</a>，欢迎理性讨论。</p><p><strong>本系列内容属于个人原创，转载请声明出处，商业转载请联系本人，邮箱同上。</strong></p><h2 id="机器学习总览"><a href="#机器学习总览" class="headerlink" title="机器学习总览"></a>机器学习总览</h2><p>机器学习发源于统计学，主要的目标是用数学和程序语言描述事物的规律，从而为预测、决策提供参考。</p><p>以全局的视角来看机器学习这一领域的算法，主要分为有监督（Supervised）学习和无监督（Unsupervised）学习两类，此外还有半监督（Half-supervised）学习、强化（Reinforcement）学习：</p><h3 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h3><h4 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h4><ul><li>回归（Regression）通常目标是得到连续的曲线，输出是连续的值</li><li>分类（Classification）通常目标是得到决策的边界，输出的是离散的类别</li></ul><h4 id="常见算法"><a href="#常见算法" class="headerlink" title="常见算法"></a>常见算法</h4><ul><li>Linear Regression：线性回归，以线性方式组合特征拟合连续曲线</li><li>Bayes：贝叶斯分类，通过概率模型计算样本属于各个分类的后验概率，进行分类</li><li>Logistic Regression：逻辑回归，在线性回归基础上增加激活函数以进行分类</li><li>Support Vector Machine：支持向量机，选取决策面较近的点用来计算决策面的参数</li><li>K Nearest Neighbor：K近邻，寻找距离较近的K个样本的标签取众数作为样本归类</li><li>Perceptron：感知机，二分类算法，最简单的神经网络</li><li>Decision Tree：决策树，可以看作从样本数据中学习if-else语句的组合，每一个判断都是数的一个节点，实现分类</li><li>Linear Discriminant Analysis：线性判别分析，通过找到特征的线性组合以用于降维，也是一种分类算法</li></ul><h4 id="算法的分类"><a href="#算法的分类" class="headerlink" title="算法的分类"></a>算法的分类</h4><ol><li>如果从算法解决的问题分类，可以分为回归和分类两大类算法：</li></ol><ul><li>其中，线性回归为回归类的算法，其余算法均主要用于分类，当然也可以有回归的作用。因为这些分类算法大多是在连续的输出外进行处理获得类别，如逻辑回归、感知机、支持向量机等，如果用在回归上则输出的是分类前计算的结果。</li></ul><ol start="2"><li>如果从决策面的角度来看，上述的分类算法可以分为线性分类算法和非线性分类算法：</li></ol><ul><li>线性分类算法：分类面为线性/输出函数为线性形式（本质相同，采用不同的目标函数得到的模型）<ul><li>包括：逻辑回归、支持向量机、感知机、线性判别分析</li></ul></li><li>非线性算法：分类面为非线性/输出函数的形式为非线性<ul><li>包括：贝叶斯、K近邻、决策树</li></ul></li></ul><ol start="3"><li>如果从算法的实际含义角度看，上述的分类算法可以分为生成模型和判别模型：</li></ol><ul><li>生成模型：按照条件概率建立模型，基于高斯分布等假设，学习模型的参数用于分类<ul><li>包括：贝叶斯模型、线性判别分析</li></ul></li><li>判别模型：出于最大化在测试集上的表现，进行训练<ul><li>包括：大部分其他分类算法</li></ul></li></ul><p>在基本算法的基础上，现代机器学习常见的还有集成（Ensemble）学习，其核心是”三个臭屁匠，顶个诸葛亮”，并不致力于产生最强的单个分类器，而是通过把训练不同的较弱分类器，并进行集合决策以获得最好的分类效果。</p><h4 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h4><ul><li>Bagging/Bootstrap Aggregating：通过随机切分数据集，并行训练相同模型以获得更好的分类效果。<ul><li>随机森林（Random Forest）算法正是基于bagging算法实现。</li></ul></li><li>Boosting：通过训练一系列弱分类器并组合获得强分类器。</li><li>Stacking：训练一个组合不同模型的高层模型进行分类（上面两种算法对底层模型的组合方式是确定的）。</li></ul><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><h4 id="主要任务-1"><a href="#主要任务-1" class="headerlink" title="主要任务"></a>主要任务</h4><ul><li>降维（Dimensionality Reduction）指的是将样本空间从高维特征投影到较低维度的特征从而实现提高计算效率的作用。</li><li>聚类（Clustering）指的是将无标签的样本按照样本之间的距离信息等，将相近的样本归为一个簇的算法，可以理解为没有样本标签的分类算法。</li></ul><h4 id="常见算法-1"><a href="#常见算法-1" class="headerlink" title="常见算法"></a>常见算法</h4><ul><li>Principle Component Analysis：主成分分析，通过提取协方差矩阵中的特征向量作为新特征实现降维。<ul><li>与线性判别分析（LDA）相似的算法。</li></ul></li><li>K Means：K均值算法，通过抽取相近点簇的重心作为簇的代表来实现聚类。</li><li>K Medoids：K中心点算法，和K Means算法相近，不同的是选取簇中最接近重心的点作为簇的代表。</li><li>Spectral Clustering：谱聚类，通过降维方法和K Means算法实现聚类。</li><li>Gaussian Mixture Model：高斯混合模型，是基于高斯分布的假设，通过点簇的分布估计参数以实现聚类。<ul><li>K Means算法可以视为GMM的一种特殊形式。</li></ul></li><li>Matrix Factorization：矩阵分解，是一类降维算法，包括奇异值分解、矩阵非负分解和稀疏编码等算法。</li></ul><h4 id="算法的分类-1"><a href="#算法的分类-1" class="headerlink" title="算法的分类"></a>算法的分类</h4><ol><li>如果按照主要任务，可以将算法分为降维算法和聚类算法：</li></ol><ul><li>降维算法：主要包括主成分分析、矩阵分解</li><li>聚类算法：主要包括K均值、K中心点、谱聚类、高斯混合模型</li></ul><h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><p>利用少量标注样本和大量未标注样本进行机器学习的算法。</p><p>由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。</p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>没有特定的目标，强调环境的反馈作用，通过应对环境调整策略的算法。</p><p>由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。</p><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p>这一块是近十年新的方向，也是目前机器学习最火的分支，但是预计不会在近期内容中出现。</p><p>简言之，深度学习就是基于神经网络的算法，通过组合线性的神经元和非线性的激活层，以及搭建不同结构的网络，来实现回归或者预测、聚类等工作。</p><p>其”神经网络”形态的灵感得益于生物大脑的神经元连接结构，让人联想到”机器的大脑🧠”，加上诸如alphaGo等等一些新奇的成就带来的狂热使得众人为之疯狂，许多营销号和媒体甚至脑洞大开，大肆鼓吹”人工智能有害论”。</p><p>但目前而言，可解释性差、缺乏较统一的数学理论描述是其硬伤。而且也没有出现强人工智能的迹象，目前的神经网络，本质只是一种复杂的统计模型。</p><p>随着研究陷入瓶颈，这场资本与舆论的狂欢已经在逐渐冷却，未来究竟如何发展也未可知：）</p><h2 id="机器学习方法论"><a href="#机器学习方法论" class="headerlink" title="机器学习方法论"></a>机器学习方法论</h2><p><strong>机器学习的本质在于从数据或者假设中建立模型、学习参数，去拟合一个未知的函数。</strong></p><p>根据论文《A Few Useful Things to Know about Machine Learning》，机器学习的过程可以表示为：</p><p>$LEARNING = REPRESENTATION + EVALUATION + OPTIMIZATION$</p><p>也就是说，机器学习主要分为三个过程：</p><ol><li>表示（Representation）：使用计算机能够执行的语言描述算法。这个阶段确定了模型的类型，所以决定了拟合/分类函数的假设空间（Hypothesis Space）——也就是说，在这一步，模型的参数个数和模型的计算方式已经确定，比如线性模型的$y=WX+B$，那么模型无法模拟非线性的分类/回归，这是选取的模型导致的。而具体是如何线性的函数，需要在接下来的过程中确定。</li><li>评估（Evaluation）：用于评估模型的好坏。根据任务的不同（回归、分类）确定了不同的种类，同时这个评估方法应当是能够方便地找到对应的优化函数的（更明确一点，评估的函数应该是可导的）。我们训练的目标就是最小化目标函数（误差型）或者最大化目标函数（精度型）。</li><li>优化（Optimization）：评估函数就像考试，有了考试我们就可以知道自己的薄弱环节，从而确定努力的方向。而有了评估函数，就有一个对应的优化函数用于调整模型的参数。<ul><li>通常我们采用基于梯度的方法，具体会在下面梯度下降这一概念中解释。</li></ul></li></ol><p>论文中列出了一个关于这三个部分的表格，在这里贴出来：</p><p><img src="/Machine-Learning-01-Overview/table.png" alt></p><p>从表格也可以看出来，对某一种算法，并非所有的评估函数都能够使用，有些算法是绑定了评估函数的。</p><p>同时，评估函数与优化函数存在对应关系，选择某一类评估函数时，对应的优化策略也就决定了。</p><p>（接下篇：<a href="/Machine-Learning-01-Overview-2">机器学习概念</a>）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-01（下）：机器学习概念</title>
      <link href="/Machine-Learning-01-Overview-2/"/>
      <url>/Machine-Learning-01-Overview-2/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第一篇文章的下半部分，内容包含了机器学习的概念解释。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork和pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>紧接上半篇文章的算法介绍，这里主要会介绍一些基本概念。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><p>特征缩放（Feature Scaling）是一项预处理技术，它将所有的输入按照统一的标准进行处理：</p><ul><li>最大最小缩放（Min-Max Normalization）：把每一个特征的各个值按照大小缩放到$[0,1]$的区间中。</li><li>均值缩放（Mean Normalization）：把每一个特征的各个值按照大小缩放到$[-1, 1]$区间中。</li><li>标准化（Standardization）：把每一个特征缩放成平均值为0，方差为1的变量。</li><li>单位化（Scaling to Unit Length）：把每一个样本的长度（即向量的第二范数）缩放为1。</li></ul><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="泛化-、方差、偏差和噪声"><a href="#泛化-、方差、偏差和噪声" class="headerlink" title="泛化 、方差、偏差和噪声"></a>泛化 、方差、偏差和噪声</h3><p>首先，泛化（Generalization）是指模型在经过一定的数据训练之后对现实数据进行测试，我们希望模型能够最小化测试误差（testing error），而训练数据集上的误差与测试数据集上的误差就是泛化误差（Generalization Error）。</p><p>而泛化误差分为方差（Variance）与偏差（Bias），通过这两个方面可以描述模型与现实模型的误差：</p><ul><li>Bias是模型预测与真实结果的差距，可以直观理解为训练误差（training error），表现了模型的拟合能力；</li><li>Variance则是“<strong>（大小相同的）不同训练数据集</strong>训练出的模型”的训练误差之间的差异，表现了数据扰动的影响。</li></ul><p>通常来说，模型越复杂（组成模型的参数越多），方差越大，偏差越小，这是因为模型的描述能力越强；模型越简单，偏差也容易大，很可能无法拟合训练数据。</p><p>通常模型复杂程度与方差/偏差的关系：</p><p><img src="/Machine-Learning-01-Overview-2/complex.png" alt></p><p>在图中可以看到，随着模型变得复杂，训练误差/偏差变小，而方差（在图中可以看作测试误差与训练误差之间的差值）变大，训练误差在中间有一个较低值。</p><p>由此启发我们寻找一个复杂度的平衡点，使得模型具有较低的bias和variance；至于noise则是无法改变的。</p><p>此外，还有一个概念叫做噪声（noise）：噪声在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p><p>模型的误差主要来自三部分的总和。</p><h3 id="过拟合-欠拟合"><a href="#过拟合-欠拟合" class="headerlink" title="过拟合/欠拟合"></a>过拟合/欠拟合</h3><ul><li>欠拟合主要描述的是模型复杂度过低，难以拟合训练数据，此时偏差过大（上图左侧部分）</li><li>过拟合是指模型过于复杂，虽然在训练数据上能够较好拟合，但是在测试数据上误差极大，此时偏差较小而误差较大（上图右侧部分）</li></ul><p>需要注意的是，测试误差较大不能说明是过拟合还是欠拟合；需要看训练误差的大小以区分。</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="混淆矩阵与风险矩阵"><a href="#混淆矩阵与风险矩阵" class="headerlink" title="混淆矩阵与风险矩阵"></a>混淆矩阵与风险矩阵</h3><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><p>一张图说明混淆矩阵：</p><p><img src="/Machine-Learning-01-Overview-2/confuse.jpg" alt></p><p>混淆矩阵用于预测与实际的差距，对一个N元分类器而言是一个N*N的矩阵，$M[i][j]$表示了预测类为$i$，真实类为$j$的样本数。显然，正确的分类落在矩阵的主对角线上，即所有的$M[i][i]$元素。而其他项表示了分类错误的个数。</p><p>当然也有</p><p>对于二分类而言，我们会把某一个类叫做正类，另一个类叫做负类，预计某个类为正类叫做阳性，反之叫阴性，所以又产生了如下概念：</p><ul><li>预测和真实均为正类的叫做<strong>真阳性</strong>（TP，True Positive）</li><li>预测与真实均为负类的叫做<strong>真阴性</strong>（TF，True Negative）</li><li>预测为正类而真实为负类的叫做<strong>假阳性</strong>（FP，False Positive）</li><li>预测为负类而真实为正类的叫做<strong>假阴性</strong>（FN，False Negative）</li></ul><p>稍加拓展，当应用在多分类上，就是把分类错误统称为负类，分类正确的当做正类。</p><p>由此出发，我们得到新的概念作为评价指标：</p><ul><li><strong>准确率</strong>（Accuracy）：$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$</li><li><strong>精确率</strong>（Precision）：$Precision=\frac{TP}{TP+FP}$</li><li><strong>召回率</strong>（Recall）：$Recall=\frac{TP}{TP+FN}$</li></ul><p>这些概念容易搞混，另外与搜索引擎的评估也有一定关联。</p><p>那么这三个分类标准分别有什么特点和适用场景？</p><p>我们以检测疾病的分类器为例说明：</p><ol><li>Accuracy与分类目标无关，实际只看分对了没有；在类别不均衡的问题上评估粒度太大，不如后两种手段有效。因此，在没有特定要求某个类的准确率而是关注整体准确率的时候，使用这一指标。</li><li>Precision是指分类器所挑出的某个类中，真正是我们希望的那一类的概率。使用这个为指标就是期待分类器降低把没病的人当作有病的概率。</li><li>Recall是指没有被识别出是我们想要的那一类的概率。简单来说就是使用这个指标就是期待分类器不要错放过有病的人。</li></ol><h4 id="风险矩阵"><a href="#风险矩阵" class="headerlink" title="风险矩阵"></a>风险矩阵</h4><p>生活经验中，同样是分类错误，我们对于不同错误分类的容忍度往往是不同的，比如：</p><ul><li>垃圾邮件分类问题中，相比重要邮件被错误分类为垃圾邮件而进入垃圾箱，我们更情愿多收到一些被当作正常邮件的垃圾邮件。<ul><li>在这里如果正类是垃圾邮件，那么我们关注Precision多于Recall；反过来如果正常邮件是正类，那么我们更加关注Recall。</li></ul></li><li>当判决某个细胞是正常细胞还是癌细胞的时候，显然把一个正常细胞错判为癌细胞的风险要比把一个癌细胞错判为正常细胞的风险大很多，后者的错误是致命的。<ul><li>如果我们的正类是癌细胞，那么我们关注Recall多于Precision，因为我们不希望放过每一个正类。</li></ul></li></ul><p>在这些情形下，同样都是分类错误，某一种分类错误的影响更严重，所以并不是最小错误率（Minimum Probability Error）而是最小风险误差（Minimum Risk）才能够表示我们的期待，这个时候我们会把分类错误添加一个权重，使用权重来改变评判标准。</p><p>风险（Risk），可以理解为对某种错误分类情形下造成后果的严重程度，形状和混淆矩阵一致，是人为添加的半定量矩阵。</p><p>我们将风险矩阵与混淆矩阵对应位置元素相乘得到的总和就是新的目标函数值，我们的分类结果应当使得这一目标函数值达到最小。</p><p>这一改变将如何影响我们的分类策略？</p><ul><li>对一个样本$X_i$：<ul><li>对每一个类j，我们计算出$X_i$属于j类的概率分别为$P(w_j|X_i)$，并计算j类的误分类风险之和：$\Sigma_{i!=j}M[j][i]$</li><li>将类的误分类风险与概率相乘，乘积就是j类误分类的概率风险</li><li>将每一个类的误分类概率风险求出，找到概率风险最小的那一个分类作为当前的分类</li></ul></li></ul><p>按照这一方法分类计算得到的误分类概率风险是最小的。</p><h3 id="从二分类到多分类"><a href="#从二分类到多分类" class="headerlink" title="从二分类到多分类"></a>从二分类到多分类</h3><p>常见的分类器如支持向量机、感知机只能做到二分类，那么多分类问题应该如何解决？</p><p>主要有以下两种思路：</p><h4 id="Ont-versus-One：一对一"><a href="#Ont-versus-One：一对一" class="headerlink" title="Ont-versus-One：一对一"></a>Ont-versus-One：一对一</h4><p>对每一组不同的类$j_1,j_2$，我们构造一个二分类器；</p><p>然后，对每一个样本，计算所有分类器，对每一个类进行投票。</p><h4 id="One-versus-Rest：一对其余"><a href="#One-versus-Rest：一对其余" class="headerlink" title="One-versus-Rest：一对其余"></a>One-versus-Rest：一对其余</h4><p>对每一个类$j$，构造一个二分类器，区分的类是第j类和所有的其他类；</p><p>然后，对每一个样本，计算所有分类器，此时如果只有一个分类器预测为正类，那么就将其分类为正类；否则，在预测正类的类中挑选置信度最大分类器对应的类。</p><h4 id="二者的比较"><a href="#二者的比较" class="headerlink" title="二者的比较"></a>二者的比较</h4><p>OvO只需要两个类的样本，但是每个分类器需要训练$k(k-1)/2$个分类器；</p><p>OvR需要k个分类器，但是每个分类器需要训练全部样本。</p><p>综合来看，OvO训练的时间开销较小，OvR的存储开销较小。</p><h4 id="Multi-versus-Multi：多对多"><a href="#Multi-versus-Multi：多对多" class="headerlink" title="Multi-versus-Multi：多对多"></a>Multi-versus-Multi：多对多</h4><p>每次选取特定的多个类作为正类，特定的多个类作为负类进行分类，从而确定所属的类区间。</p><p>这里选取的类不能随意选取，主要有纠错输出码技术（Error-Correcting Output Codes，ECOC），在此不做展开 ，有兴趣可以参考：<a href="https://hyper.ai/wiki/4350">https://hyper.ai/wiki/4350</a></p><p>这个推广对其它二分类分类器也适用。</p><h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p>类别不平衡（class imbalance），又称为数据偏斜（class skew）。</p><p>以二分类问题为例，该问题一般指的是训练集中正负样本数比例相差过大，一般会造成：</p><ol><li><p>类别少的误判惩罚过低，导致有所偏袒，当样本不确定时倾向于把样本分类为多数类。</p></li><li><p>样本数量分布很不平衡时，特征的分布同样会不平衡。</p></li><li><p>传统的评价指标变得不可靠，例如准确率。</p></li></ol><p>而在多分类问题中，尽管原始训练集中可能不同类别训练样本数目相当，通过OvR、MvM进行拆分时也有可能会造成上述情况，所以类别不平衡问题亟待解决。</p><p>常见的解决方案有：</p><ul><li>对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当。</li><li>对较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当。</li><li>阈值调整（threshold moving），将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可。</li></ul><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证（Cross Validation）是一种避免过拟合的训练技巧。</p><p>具体思路在于将训练切分，每一次用不同的数据集来训练，优化平均的误差，从而降低不同数据集带来模型性能的变化，达到降低方差的目的。</p><p>主要有两种方法：</p><ul><li>K折验证（K-Fold）：指将数据切分为K份，每一份轮流作为验证集（Validation Set），其他数据作为训练数据，训练K轮次获得训练误差。</li><li>留一验证（Leave-One-Out）：是K=n的K折验证，通过每次取一个样本作为验证集进行交叉验证训练。</li></ul><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降（Gradient Descent）是在通常模型中通用的迭代型参数估计方法。</p><p>我们可以认为，目标函数是一个自变量为模型参数的函数，而我们希望达到它的最大/最小值。</p><p>我们知道，一个函数在最大或者最小值的位置，它的一阶梯度为全0的向量。至于它究竟是最大值还是最小值，得看其二阶导数，或者进行测试局部变化来验证。</p><p>当然，通常我们会希望目标函数是纯凸/纯凹的，因为这样它的驻点（极大极小值点）只有一个，一旦找到极值点就能够确定它是最优的。在这一理论的驱动下，诞生了凸优化这一学科，目标就是把各种非凸问题转化成凸的问题。</p><p>因此我们会对它进行求导，寻找一阶导数为0的点对应的自变量，也就是模型参数的值。</p><p>此时，我们可以直接利用等式的梯度为0求解出参数，也可以采用迭代求解的梯度下降方法。</p><p>前者看起来不是更直接而且精确嘛？但是事实上我们大多采用的是后者。理由就是，第一个方法的实质是计算方程组的解，涉及求逆矩阵的过程，但是一来计算量大，二来难以保证矩阵非奇异或者非病态的情况下，计算过程对方程组值的扰动非常敏感，噪声带来的误差较大导致结果偏离理论解。</p><p>那么，后者是如何操作的？</p><p>在每次训练时，减去梯度值和学习率的乘积。对于一个局部凸的部分，我们可以看到在减去梯度之后我们的参数坐标会向极值点（最低点）靠近，且梯度绝对值越大，下降越快。</p><p>理论依据：梯度的反方向就是函数局部值下降最快的方向。</p><p><img src="/Machine-Learning-01-Overview-2/gradient.png" alt></p><p>为了快速收敛、避免震荡的目的，也出现了很多学习率优化算法，如自适应性优化（Adam）、Adagrad和随机梯度下降（SGD）、Momentum等策略，这一块暂时不做介绍。</p><h3 id="批量梯度下降-随机梯度下降"><a href="#批量梯度下降-随机梯度下降" class="headerlink" title="批量梯度下降/随机梯度下降"></a>批量梯度下降/随机梯度下降</h3><p>这是梯度下降的两种操作方式。</p><ul><li>随机梯度下降（Stochastic Gradient Descent）是指，对每一个训练的样本都计算一次梯度并且用梯度执行更新参数的操作。这种方法的好处是更新次数快，且存在一定的随机性不会陷入局部极小值；但是也因为随机性强，往往梯度的波动大，某一两个样本带来的参数变化太大，更新不稳定，甚至导致不收敛 。</li><li>批量梯度下降（Batch Gradient Descent）是指，每次对所有训练样本进行计算梯度并且只用所有梯度的平均值进行一次更新。这种方法的好处自然就是稳定更新；但是其更新太慢，在一定的时间里难以达到收敛，而且也容易陷入局部最小值，最终在较小的梯度下停止更新。</li></ul><p>一般来说现有的技巧在于折衷两种方案，进行小批量的梯度下降，并且打乱样本以获取随机性。</p><p>这样做的好处在于：</p><ol><li>利用了随机梯度下降的随机性，一般不会陷入局部极小值。</li><li>更新速度适中，保持较好的稳定性不会震荡，同时也能够较快达到收敛。</li><li>最重要的是，方便底层GPU优化。因为梯度计算的底层操作是矩阵运算，而GPU由于多核计算能够并行地计算某一行的计算结果，从而加速梯度更新过程。所以一般而言，小批量梯度下降的效率比随机梯度下降更高。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Restart</title>
      <link href="/Restart/"/>
      <url>/Restart/</url>
      
        <content type="html"><![CDATA[<p>今天把博客文章全都清空了。</p><p>主要是之前的文章太乱，缺乏整理；加上近期学了很多东西之后，回头看过去的内容觉得有些浅薄，决心从头开始写。</p><p>今后会在这里写一些机器学习，以及数据处理的东西。</p><p>当然还有一些工程向的内容，总之我会更加深思熟虑地推送文章。</p><p>（是不是也考虑一下换主题呢……哈哈还是算了估计又要挑很久）</p>]]></content>
      
      
      
        <tags>
            
            <tag> Hello, world! </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
