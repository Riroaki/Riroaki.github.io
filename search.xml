<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学不动了-06：核方法——非线性SVM</title>
      <link href="/Machine-Learning-06-Kernel-Method/"/>
      <url>/Machine-Learning-06-Kernel-Method/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第六篇文章，内容包含了核方法的基本理论。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上一篇文章介绍SVM，并总结了线性分类器的特征。</p><p>现在我们要探究SVM的极限——是否可以对非线性但可分的数据进行分类？</p><p>（说到极限……<s>我不做人类了，JoJo！</s>）</p><p>我们看一个二次可分的问题：</p><p><img src="/Machine-Learning-06-Kernel-Method/2.png" alt></p><p>对上面这个图，形似圆环的两个类别的数据分布，显然不是线性可分的。那么所有的线性分类器（逻辑回归、SVM，以及之后介绍的Perceptron）都会失效。</p><p>如果手动做的话，我们以内圆的圆心为原点，视横坐标，纵坐标的表示为$[x_1,x_2]$，找到介于两个圆半径之间的长度$r$，那么满足$x_1^2+x_2^2-r^2\le0$为1类，反之则为2类，如此便可实现区分。</p><p>对于一个二维的平面，我们口算尚可一战；然而对问题稍做变换，情况又如何呢？</p><p>设想一个映射将原图放入三维空间：</p><p><img src="/Machine-Learning-06-Kernel-Method/3d.png" alt></p><p>这种情况下，我该如何搞定呢？或者情况不是圆……</p><p>这时候我们需要回过头把我们在二维做的事情进行泛化——<strong>找到更为一般和通用的形式</strong>，以不变应万变嘛。</p><p>注意刚才的不等式$x_1^2+x_2^2 - r^2$的形式，本质还是可以看作一个线性组合——如果令$z_0=1,z_1=x_1^2,z_2=x_2^2$那么我们还是可以用线性可分的分类器去划分，学习得到$w=[-r^2,1,1]^T$的参数使得数据可以被$w^Tz=$划分。</p><p>本质上，我们刚才做了一次坐标变换，从而把原来类的数据转换到一个空间，在那个空间里各个类的分布是线性可分的。</p><p>更为一般的形式是包含一次项和其他二次项的（毕竟你不会总是对这种圆进行分类，圆心也未必在原点）：</p><ul><li>变换前：$x=[1,x_1,x_2,…,x_n]$</li><li>变换后：$z=\begin{bmatrix} 1&amp;x_1&amp;x_2&amp;…&amp;x_n\x_1&amp;x_1x_1&amp;x_1x_2&amp;…&amp;x_1x_n\…\x_n&amp;x_nx_1&amp;x_nx_2&amp;…&amp;x_nx_n\end{bmatrix}$</li></ul><p>就获得了这样的新坐标，这个新坐标的维度为$x_ix_j,i&amp;j\in[0,n]$，所以新的坐标有$((n+1)^2-(n+1))/2=\frac{n(n+1)}{2}$个不同的维度。</p><p>然后我们将变换后的坐标变为新的$x$，这样一来训练一个$w\in \R^{n(n+1)/2\times1}$来预测，就把问题转化为线性的分类工作。</p><blockquote><p>其实在之前<a href="/Machine-Learning-03-Linear-Regression">线性回归</a>讲正则化的时候就已经做过类似的工作了，只不过那个时候的坐标变换是一个预处理的工作，即改变数据集的维度，而不是模型的工作。</p></blockquote><p>那么现在，我们如何把这种变换加入线性的分类器，让它能够自动帮我们搞定非线性的情况呢？</p><h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>我们把上面这种从原feature到新feature的映射叫做核函数$\kappa$。</p><p>对上面的变换，我们有$\hat f(x)=w^Tx=\Sigma_{i=1}^n\alpha_ix_i^Tx=\Sigma_{i=1}^n\alpha_i\kappa(x_i, x)$</p><p>从而$\kappa(x_i,x)=x_i^Tx$</p><p>把它应用在SVM中，得到新的公式：</p><p>$y=sgn(w^Tx+b)=sgn(\Sigma_{i=1}^n\alpha_iy_i&lt;x_i,x&gt;+b), w=\Sigma_{i=1}^n\alpha_iy_ix_i$</p><p>在SVM中，对非支持向量的点，$\alpha_i$的值为0。</p><p>核函数更为一般的定义是：</p><blockquote><p>核函数是用于衡量两个对象$x,x’$的<strong>相似性</strong>关系的函数，通常它是：</p><ul><li>非负的，即$\kappa(x,x’)\ge0$</li><li>对称的，即$\kappa(x,x’)=\kappa(x’,x)$</li></ul></blockquote><p>通常的核函数有如下几种：</p><ul><li><code>Linear kernel</code>:$\kappa(x,x’)=x^Tx’$</li><li><code>Polynomial kernel</code>:$\kappa(x,x’)=(x^Tx’+1)^d$</li><li><code>RBF kernel</code>:$\kappa(x,x’)=exp(-\frac{||x-x’||^2}{2\sigma^2})$</li></ul><p>通过引入核函数，我们将原来的线性SVM分类器推广到了非线性的分类。</p><p>此外，还有<code>string kernel</code>和<code>graph kernel</code>，这两个核函数的作用不是把输入投影到新的维度上，而是单纯获取两个向量之间的相似程度。</p><h2 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h2><p>使用核方法意味着你已经知道了数据分布的大致情况（比如二次型），从而能够选取核的形式。</p><p>并不是一劳永逸的方法。哈哈，照这个结论，我可以说“机器学习没有银弹”咯？</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>这一章内容其实我也一知半解，可以看作是对SVM的一种补充吧（然而并没有代码实现）。</p><p>等到有更清楚的说法我会补充，有问题或者意见可以在评论区留言，共同讨论。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-05：支持向量机</title>
      <link href="/Machine-Learning-05-Support-Vector-Machine/"/>
      <url>/Machine-Learning-05-Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第五篇文章，内容包含了支持向量机的详细理论和实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上一篇内容中介绍了逻辑回归的内容，并以此展示了线性分类器最基本的形式。</p><p>本文介绍的支持向量机也是一种线性分类器，只不过获得模型的方式有所不同。</p><p>作为复习，我们重新回顾一下逻辑回归的分类方式：</p><p>$y=a^Tx$，在$y&gt;0$的时候被投影到区间$(0.5,1)$，所以分类为正类；</p><p>在$y&lt;0$的时候被投影到区间$(0,0.5)$所以分类为负类。</p><p>$a$被称为分割向量，或者解向量，而每一个分类向量都对应着一个分类平面，把投影到空间中的所有点$X_i$分为两类。</p><p>下图用于解释分类平面与参数$a$（$a=[w,b]$）的关系：</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/wb.png" alt></p><p>那么，这个向量是唯一的吗？</p><p>答案是，在有限且线性可分的数据集上，一般是存在无数可行的分类面/分类向量的，如下图：</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/hyperplane.png" alt></p><p>那么很自然的我们就会有一个问题，怎样分类才是最优的呢？</p><p>这个最优的平面，理应是能够帮助我们在测试数据上有最好的分类效果的：</p><p>直觉上来看，这个分类面不应该过于靠近某一类点（或者甚至插入了同一类点之间），而是让两类点尽量离分类平面较远。因为距离近表示接近边界，那么这些点是最容易被误分类的，我们希望这样的点越少越好——也就是，希望我们的分类器的分类结果都是<strong>极端一点</strong>，<strong>模棱两可的结果尽量少一些</strong>。</p><p>换句话说，每一个点到超平面的距离都应当尽量的大，应对新数据理应有好的表现。</p><p>而事实上超平面稍微改变，影响的主要是和平面近邻的点，所以我们定义概念：</p><blockquote><p>Margin：分类平面到点集的<strong>最小</strong>距离。</p></blockquote><p>用数学化的语言描述，我们所追求的目标就是找到一个分类面可以最大化margin。</p><p>答案已经呼之欲出了，那就是今天的主角——支持向量机（Support Vector Machine）。</p><h2 id="支持向量机理论"><a href="#支持向量机理论" class="headerlink" title="支持向量机理论"></a>支持向量机理论</h2><p>首先复习一下点到平面距离公式：</p><p>对平面$C:y=a_0+a_1x_1+a_2x_2+…+a_nx_n$，任意一个点$x=[x_1,x_2,…,x_n]$到平面的距离为：</p><p>$d=\frac{|a_0+a_1x_1+a_2x_2+…+a_nx_n|}{\sqrt{a_0^2+a_1^2+a_2^2+…+a_n^2}}=\frac{|y(x)|}{||a||_2-a_0^2}$</p><p>因而，我们希望找到的由$a$决定的分类平面应当是：</p><p>$w,b=\arg \max \min \frac{|w^Tx+b|}{||w||_2}$</p><p>同时，我们希望它是分类正确的。虽然距离足够大，但是不保证分类正确那不就本末倒置了吗！</p><p>所以在这个表达式中加入分类的信息：</p><p>$w,b=\arg \max \min \frac{y_i(w^Tx_i+b)}{||w||_2}$</p><p>这里的$y_i\in {-1,1}$，表示正类和负类。由此可见SVM也是一种二分类算法。</p><p>观察这个表达式，如果分类正确，那么$w^Tx_i+b$和$y_i$应该是同号的，反之是异号。</p><p>因而这个表达式是合理的，它同时包含了“分类正确”和“点到平面距离最大”的目标。</p><h3 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h3><p>如果说到此为止，那就太简单了，然而支持向量机这名字还没出来呢！</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/%E5%A4%A9%E5%BA%95%E4%B8%8B%E5%93%AA%E6%9C%89%E8%BF%99%E6%A0%B7%E7%9A%84%E5%A5%BD%E4%BA%8B.jpg" alt></p><p>接着推呗。</p><p>回到上面这个式子：最大化$\frac{y_i(w^Tx_i+b)}{||w||_2}$，是不是单纯最大化分子&amp;最小化分母就可以了？</p><p>当然不是。因为上下都有$w$这一项。</p><p>但是注意到$a$决定了平面之后，等比例地缩放$a$不会改变平面，但是却改变了分子分母的值，对我们的计算带来困扰。我们当然希望控制的变量越少越好，希望分子和分母不要存在关联，这样我们只需要考虑一边就可以。</p><p>所以我们令$y_i(w^Tx_i+b)=1$，这样问题就转变为求$\frac{1}{||w||_2}$的最大值，也就是求$||w||_2$的最小值。</p><p>稍微修整表达，问题被改写为：</p><p>$\min\frac{1}{2}||w||_2^2$</p><p>$s.t. y_i(w^Tx_i+b)\ge1$</p><p>平方和系数$\frac{1}{2}$是为了求导方便和导数形式所确定的，不会对结果有影响。</p><h3 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h3><p>说了这么多，支持向量到底是什么？</p><p>稍微抽象化地想一想：我们的平面是被空间中的点“支撑”起来的，因为它们需要保持一定的距离。就像磁铁的两极相斥一样，平面受到了点的斥力而树立起来。</p><p>所以，这些点就叫做“支持向量”</p><p>而因为我们只考虑最近的点来计算点到平面的距离最小值，所以支持向量指的是靠近平面的那些点。</p><p>所以选取点的个数也是一个超参数，它减少了计算量（原本是对每一个点都进行计算）。</p><h2 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h2><p>说完了基本的理论部分，接下来要对算法进行批判了。</p><p>设想一下，目前的支持向量机算法是建立在“数据线性可分”的基础上，也就是说，所有的点都能够分类在平面两侧。</p><p>假如因为标注错误或者别的特殊情况，数据中有那么几个点不小心分错了，跑到平面的另一侧去了呢？</p><p>对这些点，我们的算法缺少一定的容忍性（鲁棒性），会被这些点（如果它们又刚好足够近，会被选择成为支持向量），那么就会被它们带偏——所以说，一颗老鼠屎坏了一锅粥啊。</p><p>下面就介绍一个补救措施：</p><h3 id="松弛变量"><a href="#松弛变量" class="headerlink" title="松弛变量"></a>松弛变量</h3><p>在应对近似线性可分的问题的时候，我们需要稍微放松要求，允许支持向量的点到平面的距离稍微近一点。</p><p>在公式上我们引入了一个变量，叫做松弛变量（Slack variable）：</p><p>$min\frac{1}{2}||w||<em>2^2+C\Sigma</em>{i=1}^n\epsilon_i$$</p><p>$s.t.y(w^Tx_i+b)\ge 1-\epsilon_i,$$</p><p>$\epsilon_i\ge0$</p><p>这里，$C$是用来控制原目标（最小化$w$）和新目标之间（保证尽量多的margin大于等于1）的权重的，</p><p>与这个做法相对的，我们原来的做法就叫做硬间隔（Hard margin）。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>支持向量机的理论还不止这些内容，但是目前掌握的知识足够我们实现一个简易的支持向量机了。</p><p>本次实现了简化的SVM算法，用到scipy库的优化函数取最小点来完成二次规划的部分。</p><p>代码中已经给出部分注释，如有疑问请在评论区留言。其他详情请见我的<a href="https://github.com/Riroaki/LemonML/">repo</a>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> LinearModel<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>optimize <span class="token keyword">import</span> minimize<span class="token keyword">class</span> <span class="token class-name">SVM</span><span class="token punctuation">(</span>LinearModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Support vector machine model, binary classifier."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Target and constraint functions</span>        <span class="token keyword">def</span> <span class="token function">target</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">get_func</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token keyword">lambda</span> w<span class="token punctuation">:</span> w<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x_ext<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> label<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># Target and constraint functions with slack variables</span>        <span class="token keyword">def</span> <span class="token function">target_slack</span><span class="token punctuation">(</span>w_e<span class="token punctuation">)</span><span class="token punctuation">:</span>            w <span class="token operator">=</span> w_e<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>            eps <span class="token operator">=</span> w_e<span class="token punctuation">[</span><span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token punctuation">]</span>            <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">*</span> w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>w<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> c <span class="token operator">*</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>eps<span class="token punctuation">)</span>        <span class="token keyword">def</span> <span class="token function">get_func_slack_w</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token keyword">lambda</span> w_e<span class="token punctuation">:</span> w_e<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>x_ext<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span> \                               <span class="token operator">*</span> label<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token operator">+</span> w_e<span class="token punctuation">[</span>p <span class="token operator">+</span> i<span class="token punctuation">]</span>        <span class="token keyword">def</span> <span class="token function">get_func_slack_e</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token keyword">lambda</span> w_e<span class="token punctuation">:</span> w_e<span class="token punctuation">[</span>p <span class="token operator">+</span> i<span class="token punctuation">]</span>        <span class="token keyword">assert</span> np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># No slack parameters unless explicitly stated</span>        slack <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">if</span> kwargs <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Update parameters of training</span>            self<span class="token punctuation">.</span>_update_params<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Whether to use slack variables</span>            <span class="token keyword">if</span> <span class="token string">'slack'</span> <span class="token keyword">in</span> kwargs<span class="token punctuation">:</span>                <span class="token keyword">assert</span> isinstance<span class="token punctuation">(</span>kwargs<span class="token punctuation">[</span><span class="token string">'slack'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bool<span class="token punctuation">)</span>                slack <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'slack'</span><span class="token punctuation">]</span>        w_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span><span class="token punctuation">)</span>        x_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Find optimum w and b for both condition</span>        <span class="token keyword">if</span> <span class="token operator">not</span> slack<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># SVM without slack</span>            <span class="token comment" spellcheck="true"># Optimize 1/2 w^T * w</span>            <span class="token comment" spellcheck="true"># s.t. yi * (w^T * xi + b) - 1 >= 0</span>            cons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'type'</span><span class="token punctuation">:</span> <span class="token string">'ineq'</span><span class="token punctuation">,</span> <span class="token string">'fun'</span><span class="token punctuation">:</span> get_func<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">}</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">]</span>            <span class="token comment" spellcheck="true"># Find optimized w</span>            w_ext <span class="token operator">=</span> minimize<span class="token punctuation">(</span>target<span class="token punctuation">,</span> w_ext<span class="token punctuation">,</span> constraints<span class="token operator">=</span>cons<span class="token punctuation">)</span><span class="token punctuation">.</span>x        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># SVM with slack</span>            <span class="token comment" spellcheck="true"># Optimize 1/2 w^T * w + C * sum(eps_i)</span>            <span class="token comment" spellcheck="true"># s.t. yi * (w^T * xi + b) - 1 + eps_i >= 0, eps_i >= 0</span>            c<span class="token punctuation">,</span> w_and_eps <span class="token operator">=</span> <span class="token number">1000</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>w_ext<span class="token punctuation">,</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            cons <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">for</span> idx <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>                cons<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'type'</span><span class="token punctuation">:</span> <span class="token string">'ineq'</span><span class="token punctuation">,</span> <span class="token string">'fun'</span><span class="token punctuation">:</span> get_func_slack_w<span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>                cons<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">'type'</span><span class="token punctuation">:</span> <span class="token string">'ineq'</span><span class="token punctuation">,</span> <span class="token string">'fun'</span><span class="token punctuation">:</span> get_func_slack_e<span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span>            cons <span class="token operator">=</span> tuple<span class="token punctuation">(</span>cons<span class="token punctuation">)</span>            w_and_eps <span class="token operator">=</span> minimize<span class="token punctuation">(</span>target_slack<span class="token punctuation">,</span> w_and_eps<span class="token punctuation">,</span> constraints<span class="token operator">=</span>cons<span class="token punctuation">)</span><span class="token punctuation">.</span>x            w_ext <span class="token operator">=</span> w_and_eps<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>p <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Update and save optimal weights &amp; bias</span>        self<span class="token punctuation">.</span>_w <span class="token operator">=</span> w_ext<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>_b <span class="token operator">=</span> w_ext<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Calculate loss</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">-</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_value</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> w<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>                       b<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_val <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b        <span class="token keyword">return</span> pred_val    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_label</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> np<span class="token punctuation">.</span>sign<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        pred_label<span class="token punctuation">[</span>pred_label <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>        <span class="token keyword">return</span> pred_label    @staticmethod    <span class="token keyword">def</span> <span class="token function">_loss</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Hinge loss</span>        loss <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> pred_val <span class="token operator">*</span> true_label        loss<span class="token punctuation">[</span>loss <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        loss <span class="token operator">=</span> loss<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>              true_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use scipy.optmize to find best w and b</span>        <span class="token comment" spellcheck="true"># Not grad-base method</span>        <span class="token keyword">return</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Go-beyond-SVM"><a href="#Go-beyond-SVM" class="headerlink" title="Go beyond SVM"></a>Go beyond SVM</h2><p>为了说明问题的本质，我们需要把SVM转换为更一般的形式。</p><p>优化问题往往需要把问题的限制去除或者转化为可计算的形式，所以我们对上面的问题表述再做转化：</p><p>$\epsilon_i\ge1-y_i(w^Tx_i+b)\\epsilon_i=max[1-y_i(w^Tx_i+b),0]$</p><p>所以优化问题转化为：$\min{\Sigma_{i=1}^n\max[1-y_i(w^Tx_i+b),0]+\frac{1}{2C}||w||_2^2}$</p><p>这个形式，是不是有点像正则化的线性回归？</p><p>左侧看作目标函数，右侧看作Ridge正则项，则目标函数$l(f)=\max[1-yf,0]$</p><ul><li><p>线性回归的目标：$l(f)=(y-f)^2=(1-yf)^2$</p></li><li><p>逻辑回归的目标：$l(f)=log(1+e^{-yf})$</p></li></ul><h3 id="线性分类器的一般形式"><a href="#线性分类器的一般形式" class="headerlink" title="线性分类器的一般形式"></a>线性分类器的一般形式</h3><p>由上述表示我们可以归纳出线性分类器的问题一般形式：</p><p>$\min{\Sigma_{i=1}^nl(f)+\lambda R(f)}$</p><p>其中$l(f)$为损失函数，$R(f)$为正则项。</p><p>上述模型分别对应以下：</p><ul><li>损失函数<ul><li>线性回归：Square loss</li><li>逻辑回归：Logistic loss</li><li>SVM：Hinge loss</li></ul></li><li>正则项<ul><li>Ridge：L2-regularizer</li><li>Lasso：L1-regularizer</li></ul></li></ul><p>不同损失函数关于$yf$的图像：</p><p>BTW，Hinge loss叫做合叶函数，正是因为它的形状就像打开的合叶。</p><p><img src="/Machine-Learning-05-Support-Vector-Machine/loss.png" alt></p><h2 id="非线性SVM：核函数"><a href="#非线性SVM：核函数" class="headerlink" title="非线性SVM：核函数"></a>非线性SVM：核函数</h2><p>刚才我们解决线性问题，那么SVM的能力就仅此而已了嘛？</p><p>能不能对非线性数据也做拟合？</p><blockquote><p><strong>To be continued…</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-04：逻辑回归</title>
      <link href="/Machine-Learning-04-Logistic-Regression/"/>
      <url>/Machine-Learning-04-Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第四篇文章，内容包含了逻辑分类的详细理论和实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上回说到线性回归相关的理论，那么接下来就顺着线性回归讲解线性分类器。</p><p>线性分类器是指分类函数符合$y=w^Tx+b$形式的一系列判别模型，主要包括逻辑回归、支持向量机、感知机等，接下来会首先介绍三个线性分类器本身，再会介绍线性分类器更为一般的形式和广泛的联系。</p><h2 id="逻辑回归理论"><a href="#逻辑回归理论" class="headerlink" title="逻辑回归理论"></a>逻辑回归理论</h2><p>逻辑回归（Logistic Regression）虽然是叫回归，但是实际上是一个分类器。</p><p>逻辑回归得名于逻辑函数（Logistic Funtion），也叫做S函数（Sigmoid Function）。</p><h3 id="逻辑函数"><a href="#逻辑函数" class="headerlink" title="逻辑函数"></a>逻辑函数</h3><p>$\sigma(t)=\frac{e^t}{1+e^t}=\frac{1}{1+e^{-t}}$</p><p>这个函数具有以下特征：</p><ul><li>可以将$(-\infty,+\infty)$之间的输入值映射到$(0,1)$区间</li><li>呈现S形，单调递增：输入值为负时输出在$(0,0.5)$之间，非负输出为$[0.5,1)$。</li></ul><p>一图胜千言：</p><p><img src="/Machine-Learning-04-Logistic-Regression/logistic.png" alt></p><p>这个函数也是<a href="https://en.wikipedia.org/wiki/Logistic_distribution">逻辑分布</a>的累计分布函数。</p><p>逻辑回归使用逻辑函数估计输入参数与类变量的概率，最一般的形式为二分类：</p><ul><li>$P(y_i=1|x_i,a)=\sigma(a^Tx_i)=\frac{1}{1+e^{-a^Tx_i}}$</li><li>$P(y_i=-1|x_i,a)=1-\sigma(a^Tx_i)=\frac{1}{1+e^{a^Tx_i}}$</li></ul><p>合并二式，有：$P(y_i|x_i,a)=\sigma(y_ia^Tx_i)=\frac{1}{1+e^{-y_ia^Tx_i}}$</p><p>或者：$P(y_i|x_i,a)=y_i\sigma(a^Tx_i)+(1-y_i)(1-\sigma(a^Tx_i))=y_i\frac{1}{1+e^{-a^Tx_i}}+(1-y_i)(1-\frac{1}{1+e^{-a^Tx_i}})$，</p><p>或者：$P(y_i|x_i,a)=\sigma(a^Tx_i)^{y_i}(1-\sigma(a^Tx_i))^{1-y_i}=…$</p><blockquote><p>P.S.其实合并得到的式子我感觉是凑出来的……</p></blockquote><p>这就是最基本的逻辑回归的形式。</p><p>这个式子也隐含着：判断是类1的概率和类2的概率之和为1。</p><p>另外，这两个式子还具有一个规律：</p><p>$\frac{P(y_i=1|x_i,a)}{P(y_i=-1|x_i,a)}\ln\frac{\sigma(a^Tx_i)}{1-\sigma(a^Tx_i)}=a^Tx_i$，所以逻辑回归也叫做对数几率回归。</p><p>实际判断的时候，我们会把输出大于0.5的分类作为正类，输出小于0.5的值的分类作为负类，从而学习到一个参数为$w,b$的模型。</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>如何评价模型，决定了我们获取参数的方式。</p><p>在这里，对于逻辑回归这一个概率模型，我们使用极大似然估计作为目标函数：</p><p>我们有对单个输入的概率估计$P(y_i=\pm1|x_i,a)$（这里使用上面的<strong>第三个</strong>式子），那么对全体的数据集$D$，总的概率估计为：</p><p>$P(D)=\prod_{i\in I}P(y_i|x_i,a)=\prod_{i\in I}(\sigma(a^Tx_i)^{y_i}(1-\sigma(a^Tx_i))^{1-y_i})$</p><p>这是一个关于$a$的函数,我们希望$P(D)$取极大，也就是估计正确的概率达到最大。</p><p>因而，需要对$P(D)$关于$a$求导。但是，连乘的形式难以求导，所以我们通过取对数把函数形式转化为连加——这在之前的内容也提及了。</p><p>$l(P(D))=\Sigma_{i\in I}y_ilog(\sigma(a^Tx_i))+(1-y_i)log(1-\sigma(a^Tx_i))$</p><p>因为这个函数是凹的，或者说是上凸的，我们再进行一次转换：$E(a)=-\frac{1}{m}l(P(D))$，可以证明，$E(a)$是一个可导的凸函数。</p><p>那么目标函数就变为$E(a)$，我们要求其最小值。</p><blockquote><p>极大似然估计（MLE）在之前的内容已经出现过，不过这里的形式与贝叶斯分类器中的极大似然估计稍有不同，因为这里的估计中，<strong>概率就是似然</strong>，极大似然估计是对概率与实际分布关系作分析；而贝叶斯分类器中，极大似然估计就是对似然与实际分布的关系作分析（当然，从另一个角度，似然就是后验概率）。</p></blockquote><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>首先对$E(a)$进行变换：</p><p>$E(a)=-\frac{1}{m}\sum_{i=1}^{m}[y_ilog(\frac{e^{a^Tx_i}}{1+e^{^{a^Tx_i}}})+(1-y_i)log(\frac{1}{1+e^{a^Tx_i}})]$</p><p>$=-\frac{1}{m}\Sigma_{i=1}^m[y_ia^Tx_i+log(1+e^{a^Tx_i})]$</p><p>接下来求梯度：</p><p>$\frac{\partial E(a)}{\partial a}=-\frac{1}{m}\Sigma_{i=1}^m[y_ix_i-\frac{e^{a^Tx_i}}{1+e^{a^Tx_i}}x_i]$</p><p>$=-\frac{1}{m}\Sigma_{i=1}^m[y_ix_i-\sigma(a^Tx_i)x_i]$</p><p>记$\sigma(a^Tx_i)$为$h_a(x_i)$，那么上式又可以表示为：$-\frac{1}{m}\Sigma_{i=1}^m[(y_i-h_a(x_i))x_i]$</p><blockquote><p>联想一下线性回归的梯度：$\frac{\partial E(a)}{\partial a}=-\frac{1}{m}\Sigma_{i=1}^m[(y_i-wx_i+b)x_i]$，</p><p>可以说形式非常相似了。</p></blockquote><p>那么和线性回归相似的，我们试着用两种方式求解：</p><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>让梯度为0，我们有：$y_i=\frac{1}{1+e^{-a^Tx_i}}$</p><p>首先令$z_i=a^Tx_i$，我们有：$y_i=\frac{1}{1+e^{-z_i}}$</p><p>所以$z_i=-log(\frac{1}{y_i}-1)=log(y_i)-log(1-y_i)$</p><p>那么我们有$a^Tx_i=log(y_i)-log(1-y_i)$</p><p>看起来是没错，那么能不能代入，通过求解方程组得到$a$呢？</p><p><strong>很遗憾，不能。</strong></p><p>我们很快就会注意到，对于$y_i=0,1$，上面这个式子是没有意义的。</p><p>那么为什么我们不能用这个方式求解呢？<strong>因为梯度是不会等于0的</strong>：$\sigma(z_i)$是不可能达到$0,1$的，所以我们至多是把梯度降低，永远不会出现$y_i=\frac{1}{1+e^{-a^Tx_i}}$的结果。</p><p>所以，正规方程解是不存在的。</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>那么很自然的，我们使用梯度下降法求解。</p><p>梯度代入即可，类似线性回归的解：</p><p>$grad = \alpha * -\frac{1}{m}\Sigma_{i=1}^m[(y_i-wx_i+b)x_i]$</p><p>$a-=grad$</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>内容相似，参见上一篇文章（<a href="/Machine-Learning-03-Linear-Regression">线性回归</a>）的有关内容。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>又到了代码实现环节。</p><p>这里实现了最基本的逻辑回归<code>LogisticRegression</code>类（不包含正则化部分）。</p><p>代码中包含部分注释，如有疑问请在评论区留言。其他详情见我的<a href="https://github.com/Riroaki/LemonML">repo</a>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> LinearModel<span class="token keyword">from</span> utils <span class="token keyword">import</span> batch<span class="token keyword">class</span> <span class="token class-name">LogisticRegression</span><span class="token punctuation">(</span>LinearModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Logistic regression model, binary classifier."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Check labels: only containing 1 and 0</span>        <span class="token keyword">assert</span> np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token keyword">if</span> kwargs <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Update parameters of training</span>            self<span class="token punctuation">.</span>_update_params<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>        iters<span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token comment" spellcheck="true"># Iterates till converge or iterating times exceed bound</span>        <span class="token keyword">while</span> iters <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>_iter_bound<span class="token punctuation">:</span>            iters <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment" spellcheck="true"># Update weights using mini-batch gradient desent</span>            <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_label <span class="token keyword">in</span> batch<span class="token punctuation">(</span>x<span class="token punctuation">,</span> label<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>                pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>                loss <span class="token operator">+=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span> <span class="token operator">*</span> batch_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                grad_w<span class="token punctuation">,</span> grad_b <span class="token operator">=</span> self<span class="token punctuation">.</span>_grad<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> pred_val<span class="token punctuation">,</span> batch_label<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_w <span class="token operator">-=</span> grad_w                self<span class="token punctuation">.</span>_b <span class="token operator">-=</span> grad_b            loss <span class="token operator">/=</span> n            <span class="token comment" spellcheck="true"># Break if model converges.</span>            <span class="token keyword">if</span> loss <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>_loss_tol<span class="token punctuation">:</span>                <span class="token keyword">break</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_label<span class="token punctuation">(</span>pred_val<span class="token punctuation">)</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">-</span> label<span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_value</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> w<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>                       b<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">def</span> <span class="token function">__sigmoid</span><span class="token punctuation">(</span>raw<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>            res <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>raw<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token keyword">return</span> res        prob <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b        pred_val <span class="token operator">=</span> __sigmoid<span class="token punctuation">(</span>prob<span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_val    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_label</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> np<span class="token punctuation">.</span>sign<span class="token punctuation">(</span>pred_val <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span>        pred_label<span class="token punctuation">[</span>pred_label <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>        pred_label<span class="token punctuation">[</span>pred_label <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">return</span> pred_label    @staticmethod    <span class="token keyword">def</span> <span class="token function">_loss</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use maximum likelihood (log-likelihood loss)</span>        <span class="token comment" spellcheck="true"># loss = 1 / n * (-y * log(wx + b) - (1 - y) * log(wx + b))</span>        <span class="token comment" spellcheck="true"># Here we need to care about the log zero and overflow warning...</span>        mask_val <span class="token operator">=</span> pred_val<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        mask_val<span class="token punctuation">[</span>mask_val <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span>        mask_val<span class="token punctuation">[</span>mask_val <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span>        class1_loss <span class="token operator">=</span> <span class="token operator">-</span>true_label <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>mask_val<span class="token punctuation">)</span>        class0_loss <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> true_label<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> mask_val<span class="token punctuation">)</span>        loss <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>class0_loss <span class="token operator">+</span> class1_loss<span class="token punctuation">)</span> <span class="token operator">/</span> true_label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>              true_label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true">#  dc / dw = x * (pred_val - true_label)</span>        grad_w <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">*</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_label<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        grad_b <span class="token operator">=</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_label<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use simple gradient by multiplying learning rate and grad.</span>        grad_w <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        grad_b <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        <span class="token keyword">return</span> grad_w<span class="token punctuation">,</span> grad_b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="推广：从二分类到多分类"><a href="#推广：从二分类到多分类" class="headerlink" title="推广：从二分类到多分类"></a>推广：从二分类到多分类</h2><p>见本系列的第一篇文章（<a href="/Machine-Learning-01-Overview-2">机器学习概念</a>）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-03：线性回归</title>
      <link href="/Machine-Learning-03-Linear-Regression/"/>
      <url>/Machine-Learning-03-Linear-Regression/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第三篇文章，内容包含了线性回归的详细理论和简单线性回归的实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>上回讨论了贝叶斯模型，这一模型属于生成模型（Generative Model），基于一定的假设，认为样本是由类按照一定概率模型产生的，然后根据样本学习数据。</p><p>同时我们注意到，在高斯贝叶斯分类器中，如果所有类别共享权重，那么模型就转化为线性的表达式，从而可以使用形如$y=w^Tx+b$的简洁形式建模，直接求出描述分类面的表达式（详见上次附录的证明），这就是判别模型（Discriminant Model）的思路。</p><p>那么接下来我们将深入讨论判别模型，这是直接根据数据学习得到的更加直接的规律。</p><p>线性模型就是一种判别模型，这一次我们讨论它在回归任务上的应用，也就是线性回归。</p><h2 id="线性回归理论"><a href="#线性回归理论" class="headerlink" title="线性回归理论"></a>线性回归理论</h2><p>对$x=[x_1,x_2,x_3,…,x_d]^T\in R^d$，线性函数的形式为$y=w^Tx+b$，其中$w=[w_1, w_2,…, w_d]\in R^d$，$b\in R$。</p><p>这一表达式还有一种表述，那就是将$b$这一项加入$w$中，变成：</p><p>$x=[x_1,x_2,…,x_d,1]^T\in R^{d+1},w=[w_1,w_2,…,w_d,b]^T\in R^{d+1}$</p><p>简单的模型蕴含着不可小视的力量。<strong>任意模型的表达式都可以转化为线性组合，故也可以转化为线性模型。</strong></p><p>比如，多项式可以转化为线性组合的形式：</p><p>$f(x,a)=a_0+a_1x+a_2x^2+…+a_Mx^M=\Sigma_{i=0}^Ma_ix^i$</p><p>可以转化为：$f(x,a)=A^TX$，其中：</p><p>$X=[1,x,x^2,x^3,…,x^M]^T,a=[a_0,a_1,a_2,…,a_M]^T$</p><p>基本理论部分十分简洁明了。</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>这里目标函数通常使用最小二乘误差（Mean Square Error）：</p><p>$J_n(a)=\frac{1}{N}\Sigma_{i=1}^N(y_i-a^Tx_i)^2=(y-X^Ta)^T(y-X^Ta)$</p><p>当然，使用平方残差（Residual Sum of Squares）也是可以的，区别在于比最小二乘误差少了系数$\frac{1}{N}$。</p><p>你可能会问，为什么不使用平均绝对值误差（Mean Absolute Error）：$J_n(a)=\frac{1}{N}\Sigma_{i=1}^N|y_i-a^Tx_i|$，但是它存在一些缺陷：</p><ol><li>在零点处不可导。</li><li>其导出的梯度是常数（+1或者-1），梯度求解容易导致不收敛。</li></ol><p>接下来我们抱着让目标函数最小化的目标，对它求梯度：</p><p>$\nabla J_n=-\frac{2}{N}X(y-X^Ta)$</p><p>事实上，关于最小二乘误差也可以用生成式的思路得到（对理论不感冒的可以跳过这一部分）：</p><p>然后就可以开始愉快地参数估计啦（＾∇＾）</p><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>这里我们有两种估计方法：基于正规矩阵（Normal Equation）的直接求解，或者给予梯度下降的迭代法。</p><h3 id="正规矩阵"><a href="#正规矩阵" class="headerlink" title="正规矩阵"></a>正规矩阵</h3><p>由上面的梯度，我们直接令梯度为0：</p><p>$a=(XX^T)^{-1}Xy$，这就是理论最优解——因为梯度为0，目标函数是完全凸的，所以可以认为训练数据的目标函数达到了最小值。</p><ul><li><p>如果$XX^T$非奇异，那么我们可以获得<strong>唯一解</strong>（奇异情况下使用伪逆，理论上应该是有无数组解）。</p></li><li><p>特别地，如果$X$是方阵（一般不是），那么$a=X^{-T}X^{-1}Xy=X^{-T}y$。</p></li></ul><p>好，然后我们看一下梯度求解的做法：</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>如果你还不知道梯度下降是什么的话，我在<a href="/Machine-Learning-01-Overview-2/">第一篇文章</a>里已经写过梯度下降、批量梯度下降/随机梯度下降这些概念。</p><p>通过迭代的方式更新参数，只需要把梯度乘上一个学习率$\alpha$就可以：</p><p>$grad=\alpha * -\frac{2}{N}X(y-X^Ta)$</p><p>$a-=grad$</p><p>通常，常数项$\frac{2}{N}$会省去，因为学习率是一个可以缩放的值。</p><p>好了，这里再次提出第一篇文章的问题，为什么第一种方法看起来简单直接，而且能够得到确定的“精确解”，而实际操作往往使用基于梯度的做法呢？看起来第二种做法很不精确，而且似乎未必收敛到正确的解。</p><blockquote><p>理由就是，第一个方法的实质是计算方程组的解，涉及求逆矩阵的过程，但是一来计算量大，二来难以保证矩阵非奇异或者非病态的情况下，计算过程对方程组值的扰动非常敏感，噪声带来的误差较大导致结果偏离理论解。</p></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>又到了动手实践的时间了。这一次实现了<code>LinearRegression</code>类基于<code>LinearModel</code>基类，并实现了其抽象方法。在这一份代码中不但实现了基于梯度下降的迭代方法，也实现了基于<code>normal equation</code>的直接求解法。</p><p>暂时没有实现Ridge和Lasso的正则部分，主要是希望和其他线性分类器一同构思实现，把正则化做一个更佳泛用型的模块。</p><p>代码中已经给出部分注释，如有疑问请在评论区留言。其他详情请见我的<a href="https://github.com/Riroaki/LemonML/">repo</a>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> LinearModel<span class="token keyword">from</span> utils <span class="token keyword">import</span> batch<span class="token keyword">class</span> <span class="token class-name">LinearRegression</span><span class="token punctuation">(</span>LinearModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Linear regression model."""</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        <span class="token keyword">if</span> kwargs <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Update parameters of training</span>            self<span class="token punctuation">.</span>_update_params<span class="token punctuation">(</span>kwargs<span class="token punctuation">)</span>        iters<span class="token punctuation">,</span> loss <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span>        <span class="token comment" spellcheck="true"># Iterates till converge or iterating times exceed bound</span>        <span class="token keyword">while</span> iters <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>_iter_bound<span class="token punctuation">:</span>            iters <span class="token operator">+=</span> <span class="token number">1</span>            <span class="token comment" spellcheck="true"># Update weights using mini-batch gradient desent</span>            <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_y <span class="token keyword">in</span> batch<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>                pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>                loss <span class="token operator">+=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span> <span class="token operator">*</span> batch_x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>                grad_w<span class="token punctuation">,</span> grad_b <span class="token operator">=</span> self<span class="token punctuation">.</span>_grad<span class="token punctuation">(</span>batch_x<span class="token punctuation">,</span> pred_val<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span>                self<span class="token punctuation">.</span>_w <span class="token operator">-=</span> grad_w                self<span class="token punctuation">.</span>_b <span class="token operator">-=</span> grad_b            loss <span class="token operator">/=</span> n            <span class="token comment" spellcheck="true"># Break if model converges.</span>            <span class="token keyword">if</span> loss <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>_loss_tol<span class="token punctuation">:</span>                <span class="token keyword">break</span>        <span class="token comment" spellcheck="true"># Update model with current weight and bias</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">fit_norm_eq</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Fit x using normal equation</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_w <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_b <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">!=</span> p<span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Initialize weights using random values</span>            self<span class="token punctuation">.</span>_init_model<span class="token punctuation">(</span>p<span class="token punctuation">)</span>        x_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>        w_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>pinv<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x_ext<span class="token punctuation">.</span>T<span class="token punctuation">,</span> x_ext<span class="token punctuation">)</span><span class="token punctuation">)</span>        w_ext <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>w_ext<span class="token punctuation">,</span> x_ext<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b <span class="token operator">=</span> w_ext<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w_ext<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Calculate training loss</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_w<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_b<span class="token punctuation">)</span>        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_update_model<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> pred_val    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> y<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">assert</span> <span class="token operator">not</span> np<span class="token punctuation">.</span>isinf<span class="token punctuation">(</span>self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'loss'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        pred_val <span class="token operator">=</span> self<span class="token punctuation">.</span>_predict_value<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'w'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                                       self<span class="token punctuation">.</span>_optimum<span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># The precision part of regression is None</span>        precision <span class="token operator">=</span> None        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>_loss<span class="token punctuation">(</span>pred_val<span class="token punctuation">,</span> y<span class="token punctuation">)</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_value</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> w<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>                       b<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        pred_val <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b        <span class="token keyword">return</span> pred_val    @staticmethod    <span class="token keyword">def</span> <span class="token function">_predict_label</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># NO labeling in regression.</span>        <span class="token keyword">pass</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">_loss</span><span class="token punctuation">(</span>pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> true_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use MSE loss</span>        loss <span class="token operator">=</span> float<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_val<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        loss <span class="token operator">/=</span> <span class="token number">2</span> <span class="token operator">*</span> true_val<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">_grad</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> pred_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span>              true_val<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Use MSE loss</span>        grad_w <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">*</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_val<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        grad_b <span class="token operator">=</span> <span class="token punctuation">(</span>pred_val <span class="token operator">-</span> true_val<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use simple gradient by multiplying learning rate and grad.</span>        grad_w <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        grad_b <span class="token operator">*=</span> self<span class="token punctuation">.</span>_learn_rate        <span class="token keyword">return</span> grad_w<span class="token punctuation">,</span> grad_b<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="正规化"><a href="#正规化" class="headerlink" title="正规化"></a>正规化</h2><p>好了，我们的模型看起来很完美不是吗？</p><p>现在我们试着用线性模型去拟合开头提到的多项式曲线$f(x,a)=a_0+a_1x+a_2x^2+…+a_Mx^M=\Sigma_{i=0}^Ma_ix^i$</p><p>假如图线长这样，$y=sin(x)$：</p><p><img src="/Machine-Learning-03-Linear-Regression/polynomial.png" alt></p><p>这种图线，0阶、1阶都是单调，2阶又不能先凹后凸，看起来都拟合不了啊。</p><p>直接上三阶试试：</p><p><img src="/Machine-Learning-03-Linear-Regression/3.png" alt></p><p>看起来还行，但是没办法做到完美贴合。</p><p>我们试试更厉害的，让M=9：</p><p><img src="/Machine-Learning-03-Linear-Regression/9.png" alt></p><p>嚯，厉害了，要不是背景把真实曲线画出来我还真就信了。</p><p>虽然做到训练误差为0，但是我们有理由相信，在真实数据上测试结果一定惨不忍睹。</p><p>结果如下：</p><p><img src="/Machine-Learning-03-Linear-Regression/results.png" alt></p><p>很明显，模型过拟合了。于是可以复习一下第一篇文章关于偏差与方差的理解：</p><blockquote><p>模型越复杂（组成模型的参数越多），方差越大，偏差越小，这是因为模型的描述能力越强；模型越简单，偏差也容易大，很可能无法拟合训练数据。</p></blockquote><p>我们还发现一个特征，那就是越高阶的那个系数绝对值越大：</p><p><img src="/Machine-Learning-03-Linear-Regression/coef.png" alt></p><p>为了避免过拟合，我们引入一种叫做正规化（Regularization）的技巧。</p><p>下面介绍两种正规化技巧，Ridge与Lasso。</p><h3 id="Ridge"><a href="#Ridge" class="headerlink" title="Ridge"></a>Ridge</h3><p>我们使用二阶残差作为目标函数，并引入一个惩罚项，变为：</p><p>$a^*=argmin\Sigma_{i=1}^N(y_i-x_i^Ta)^2+\lambda\Sigma_{j=1}^pa_j^2=(y-X^Ta)^T(y-X^Ta)+\lambda a^Ta$</p><p>计算梯度得到：$\nabla a=-2X(y-X^Ta)+2\lambda a$</p><p>用正规方程的方法，我们可以得到：$a^*=(XX^T+\lambda I)^{-1}Xy$，其中$\lambda$是一个常数。</p><p>再次求解，我们惊奇的发现，高阶的系数变小了：</p><p><img src="/Machine-Learning-03-Linear-Regression/coef1.png" alt></p><p>这就比较耐人寻味了。从理论角度分析一下这个事实：</p><p>岭回归以增大偏差为代价，换取更小的方差——这是随着$\lambda$增大，模型发生的变化。</p><p>在这个多项式中，因为很明显地看到随着模型变得复杂，模型的方差越来越大，岭回归正是降低方差的手段。</p><p><strong>不仅如此，岭回归更大的用处在于。当我们的特征存在较强的线性相关性的时候（可以说在特征方面不满秩），会导致$XX^T$的值很小，甚至趋于奇异。而岭回归会帮我们限制参数绝对值的大小，抑制相关性较强的属性系数。</strong></p><p>所以，岭回归实际上并不只是用在刚才的多项式拟合上，而是对所有存在较多线性相关属性时的通用解决方式。个人理解有点类似“降维”的操作，但是稍有不同。数学上使用术语<strong>压缩估计</strong>（shinkage）描述这一操作。</p><h3 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h3><p>和Ridge相似，唯一的不同在于惩罚项的阶数：</p><p>$a^*=argmin\Sigma_{i=1}^N(y_i-x_i^Ta)^2+\lambda\Sigma_{j=1}^p|a_j|=(y-X^Ta)^T(y-X^Ta)+\lambda ||a||_1$</p><p>它带来的影响也稍有不同，会导致模型的参数大多变成0，也就是得到<strong>稀疏化</strong>的参数。</p><p>用一幅图直观理解：</p><p><img src="/Machine-Learning-03-Linear-Regression/lasso-ridge.png" alt></p><p>红色的椭圆和蓝色的区域的切点就是目标函数的最优解，我们可以看到，如果是圆，则很容易切到圆周的任意一点，但是很难切到坐标轴上，因此没有稀疏；但是如果是菱形或者多边形，则很容易切到坐标轴上，因此很容易产生稀疏的结果。这也说明了为什么Lasso会是稀疏的。</p><h3 id="从贝叶斯角度看待正则化"><a href="#从贝叶斯角度看待正则化" class="headerlink" title="从贝叶斯角度看待正则化"></a>从贝叶斯角度看待正则化</h3><p>这一部分非常耐人寻味，它揭示了贝叶斯概率与线性模型之间存在一定的联系。</p><p>我们假定一个生成模型的输出是这样的：$y=f(x,a)+\epsilon$，其中：</p><ul><li>$f(x,a)$为判别函数</li><li>$\epsilon$为随机噪音，是我们不能直接获取的，我们假定它服从高斯分布：$\epsilon\sim N(0,\sigma)$</li></ul><p>那么根据高斯概率密度公式，$p(y|x,a,\sigma)=\frac{1}{\sigma\sqrt{2\pi}}\exp(-\frac{1}{2\sigma^2}(y-f(x,a))^2)$</p><p>而根据贝叶斯公式，我们有$p(a|D)=\frac{p(D|a)p(a)}{p(D)}$，其中：</p><ul><li>$p(D|a)=p(y|x,a,\sigma)$为似然概率</li><li>$p(a)$为先验概率</li><li>$p(a|D)$为后验概率</li></ul><ol><li><p>对于这个先验，我们选取$p(a)=N(a|0,\lambda^{-1}I)=\frac{1}{(2\pi)^{d/2}|\lambda^{-1}I|^{1/2}}\exp(-\frac{1}{2}(a-0)^T(\lambda^{-1}I)^{-1}(a-0))$</p><ol><li>取对数有：$\ln(p(a))=-\frac{\lambda}{2}a^Ta+c$，$c$为常数</li></ol></li><li><p>对似然，$p(D|a)=\prod_{i=1}^np(y_i|x_i,a,\sigma)$</p><ol><li>我们采用极大似然估计的log-likelihood方法：$\ln(p(D|a))=-\frac{1}{2\sigma^2}(y_i-f(x_i,a)^2))+c(\sigma)$</li></ol></li><li><p>而后验概率正比于先验和似然乘积：$p(a|D)\propto p(D|a)p(a)$</p><ol><li>取对数，我们有：$\ln(p(a|D))\propto\ln(p(D|a))+\ln(p(a))$，答案呼之欲出</li></ol></li></ol><p>于是，我们把两项加起来得到：$ln(p(a|D))\propto -\frac{1}{2\sigma^2}(y_i-f(x_i,a)^2)-\frac{\lambda}{2}a^Ta$</p><p>我们希望这一项最大（即最大后验概率），就是希望其相反数最小，也就是：</p><p>$a=argmin \frac{1}{2\sigma^2}(y_i-f(x_i,a)^2)+\frac{\lambda}{2}a^Ta$</p><p>看，这不就是Ridge正则化的损失函数？</p><h3 id="从量化的偏差、方差与噪音的角度看待正则化（待补充说明）"><a href="#从量化的偏差、方差与噪音的角度看待正则化（待补充说明）" class="headerlink" title="从量化的偏差、方差与噪音的角度看待正则化（待补充说明）"></a>从量化的偏差、方差与噪音的角度看待正则化（待补充说明）</h3><p>到这里我们从量化的角度来看这三个概念：</p><p>我们用一个概念来代表模型的总误差：Expected Prediction Error，EPE。</p><p>量化的计算如下（目前我对这部分理解不深，这一部分待补充）：</p><p>$EPE(f)=\int\int(y-f(x))^2p(x,y)dxdy$</p><p>$EPE=var+bias^2+noise$</p><ul><li>$bias^2=\int{E_D(f(x;D))-E(y|x)}^2p(x)dx$</li><li>$variance=\int E_D{[f(x;D)-E_D(f(x;D))]^2}p(x)dx$</li><li>$noise=\int var(y|x)p(x)dx$</li></ul><p>这里贴出不同$\lambda$参数对偏差与方差的影响：</p><p><img src="/Machine-Learning-03-Linear-Regression/-2.4.png" alt></p><p><img src="/Machine-Learning-03-Linear-Regression/-0.31.png" alt></p><p><img src="/Machine-Learning-03-Linear-Regression/2.6.png" alt></p><p>可以很明显的看出，正则项降低了图线的拟合程度，但是也降低了方差（即不同预测线的变化幅度）。</p><p>从而，我们对Bias-Variance的Trade-off有了更深的理解：</p><p><img src="/Machine-Learning-03-Linear-Regression/tradeoff.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-02：贝叶斯分类</title>
      <link href="/Machine-Learning-02-Bayes/"/>
      <url>/Machine-Learning-02-Bayes/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第二篇文章，内容包含了贝叶斯分类的详细理论和简单高斯贝叶斯分类器的实现。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork与pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>贝叶斯分类的核心是贝叶斯公式：</p><ul><li>$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$</li><li>$P(A) = \Sigma_{i=1}^n{P(B|A_i)P(A_i)}$</li><li>贝叶斯公式对多元变量同样适用，与变量是否独立也无关，是普适的公式。</li></ul><p>为了介绍这个公式，我们首先来看一道概率题：</p><blockquote><p>现分别有 A、B 两个容器，在容器 A 里分别有 7 个红球和 3 个白球，在容器 B 里有 1 个红球和 9 个白球，而设定从A中抽取的概率和B中抽取的概率为1:2。</p><p>现已知从这两个容器里任意抽出了一个红球，问这个球来自容器 A 的概率是多少?</p></blockquote><p>记抽中红球的事件为$P(B)$，记从容器A抽球的概率为$P(A)$。</p><p>根据贝叶斯公式，我们有：$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$。其中$P(B|A)$表示从容器A中抽球，抽到红球的概率。</p><p>在这个公式中：</p><ul><li>$P(A|B)$是已知B发生后A的条件概率，也叫做A的后验概率（posterior probability）。</li><li>$P(B|A)$是已知A发生后B的条件概率，是B的后验概率，在这里叫A的似然概率（likelihood）。</li><li>$P(A)$是事件发生之前我们对A的经验知识，与B无关，叫做A的先验概率（prior probability）。</li><li>$P(B)$是B的先验概率，在这里叫做标准化常量（normalized constant）。</li><li>根据这个关系，后验$P(A|B)$也可以叫做<strong>标准化</strong>的似然；似然和后验是可以相互转化的。</li></ul><h2 id="贝叶斯分类理论"><a href="#贝叶斯分类理论" class="headerlink" title="贝叶斯分类理论"></a>贝叶斯分类理论</h2><p>从这个公式引申开，我们可以套用在分类理论上：</p><ul><li>我们可以类比认为每一个类对应一个容器，样本都是这个类中生成（取出）的。</li><li>分类问题可以采用这样的表述：已知一个待归类样本$X_i$的特征，那么求$X_i$属于第j个类的概率，就变成了一个后验概率。</li><li>把样本属于第j个类的概率记作事件$w_j$，这个后验概率可以表述为：$P(w_j|x=X_i)$，简记作$P(w_j|X_i)$。</li><li>那么，根据贝叶斯公式，我们有：$P(w_j|X_i)=\frac{P(X_i|w_j)P(w_j)}{P(X_i)}$。</li><li>这里的似然是$P(X_i|w_j)$，先验概率是$P(w_j)$，标准化常量为$P(X_i)$。</li></ul><p>那么，有了某个样本属于各个类别的概率，如何分类呢？</p><p>很自然的，我们选择后验概率比较大的那一个概率对应的类别作为$X_i$的分类。</p><ul><li>补充1：当我们只有先验概率的时候，我们选择先验概率较大的那一个类别作为分类。用先验概率直接估计的坏处在于”马太效应”，因为它总是把新样本归类到原本占多数的那一个类。</li><li>补充2：当采用风险矩阵（risk matrix）进行评估的时候，分类规则会更复杂一些。</li></ul><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><p>我们已经有了概率的公式和决策理论，如何估计概率公式中的各个概率？</p><p>答案是：从有类标签的数据（训练数据）中总结提取。</p><h3 id="先验概率的估计"><a href="#先验概率的估计" class="headerlink" title="先验概率的估计"></a>先验概率的估计</h3><p>这里的先验概率，就是在没有训练样本具体特征的值的分布情况下，某个类原始的信息。</p><p>很自然的，我们会把这个类别的样本数占全部有标签的样本的比重当作先验概率，</p><p>即：$P(w_j)=\Sigma_{i=1}^N{I(y_i=c_j)}/N$</p><h3 id="似然概率的估计"><a href="#似然概率的估计" class="headerlink" title="似然概率的估计"></a>似然概率的估计</h3><p>这里我们需要分为连续变量和离散变量两种情况讨论：</p><h4 id="连续变量"><a href="#连续变量" class="headerlink" title="连续变量"></a>连续变量</h4><p>我们有不同的假设可以做出不同的估计。常用的有高斯分布假设、二项分布假设、伯努利分布。</p><p>这里只介绍高斯分布假设对应的参数估计方法。</p><ul><li>高斯分布的具体假设：对于某一个类$c_i$，其生成的样本满足高斯分布，即：$X\sim N(\mu, \Sigma)$，其产生的每一个样本之间的概率是互相独立且同分布的（i.i.d，Independent and identically distributed）</li><li>在这里我们采用极大似然估计（Maximum Likelihood Estimation）的做法来选取最佳参数：<ul><li>目标函数是$L(\mu_j,\Sigma_j)=\prod_{i=1}^N{P(X_i|w_j)}$。</li><li>由于不方便对目标函数求导，我们采用取对数的技巧，将连乘转化为连加：$l(\mu_j,\Sigma_j)=log(L)=\Sigma_{i=1}^NP(X_i|w_j)$</li><li>对对数似然求导，令导数为0求出$\mu,\Sigma$。由于样本是多维，求导过程比较复杂，详情参考附录。</li><li>总之，最终求出的结果和标量形式的惊人一致：<ul><li>$\hat\mu_j=\frac{1}{N_j}\Sigma_{i=0}^{N_j}X_i=\bar{x}$</li><li>$\hat\Sigma=\frac{1}{N_j}\Sigma_{i=1}^{N_j}(X_i-\hat\mu)(X_i-\hat\mu)^T=cov(X_i), where\ y_i = w_j$</li><li>很容易联想到标量情况下，$\hat\mu=\bar{X}, \hat\sigma=var(X)$</li></ul></li></ul></li><li>有了估计的参数以后，我们可以通过高斯概率密度公式求似然概率：$P(X; \mu, \Sigma)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(X-\mu)^T\Sigma^{-1}(X-\mu))$</li></ul><blockquote><p><strong>P.S.其实我有一点疑问🤔️，为什么这里不能使用梯度求解而是直接令梯度为0求解证明的呢，有没有大佬能够在评论区说一说自己的想法……</strong></p></blockquote><h4 id="离散变量"><a href="#离散变量" class="headerlink" title="离散变量"></a>离散变量</h4><p>对于离散变量的估计则较为简单，我们选取以前这一特征出现过的值的分布情况作为估计，即：</p><p>$P(X_i^k|w_j)=\frac{|X_i^k|}{N_{w_j}}$</p><p>比如在某个类的性别特征中，男性出现了100次，女性出现了200次，我们就估计一个这个属性为男性的似然概率为100/300=0.33333…，对女性同理。</p><h3 id="标准化常量"><a href="#标准化常量" class="headerlink" title="标准化常量"></a>标准化常量</h3><ul><li>对于连续变量来说，理论上是通过$P(X_i)=\Sigma_{j=1}^cP(X_i|w_j)P(w_j)$求出概率密度，也就是说得算出对样本对每一个类的先验概率和似然的乘积之和，才能计算分母。</li><li>对于离散变量来说，则是求出这一个一模一样的样本在训练数据中出现的次数。<ul><li>如果过去没有出现过这一样本，我们总不能把0作为它的概率，这是不合理的（一来有可能是样本数量过少，二来人家是分母啊，怎么能是0），所以采用平滑（Smoothing）的技术，在分子分母同时加入一个与样本数有关的平滑项。具体技术在此不做详细介绍。</li></ul></li></ul><p>然而正常情况下，它并不在我们的考虑范围内：</p><p>因为，每一个类的计算概率的分母都是这一项。既然我们最终的目标是比较后验概率的大小（或者与风险矩阵关联后的大小，whatever），这一项作为相同的系数并不会产生影响：）</p><h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>在这里因为我们并不是采取迭代优化，而是通过假设模型直接估计得出参数，所以不需要目标函数可导。</p><p>采用0-1损失函数就可以，即分类错误，结果就加一，分类正确结果就不变化的函数。</p><p>在贝叶斯中分类中，还有一个常见的评价标准（metrics）：混淆矩阵和风险矩阵，具体见上一篇的后半段。</p><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>朴素贝叶斯（Naive Bayes）模型是贝叶斯模型的一个变种，是简化贝叶斯参数的一种模型。</p><p>在朴素贝叶斯中，我们认为各个特征之间是独立的，互相不会影响，即：</p><p>$P(X_i|w_j)=P(X^1_i,X^2_i,X^3_i,…X_i^p|w_j)=P(X_i^1|w_j)P(X_i^2|w_j)P(X_i^3|w_j)…P(X_i^p|w_j)$</p><p>$=\prod_{k=1}^pP(X_i^k|w_j)$</p><p>将这一假设应用在高斯模型中，贝叶斯-高斯模型的$\Sigma$参数就退化为对角矩阵，因为它本质是协方差矩阵，如今各个特征之间的协方差为0，每个特征服从独立的高斯分布。</p><p>所以计算公式也变得简单，只需要单独计算每一维度的高斯分布概率再相乘就可以计算。</p><p>朴素贝叶斯分类器至今也是一个简单的流行分类器，不过在很多时候，样本的属性往往是相关的，这种情形下使用朴素贝叶斯模型就不太好。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>好了，说了这么多终于要上代码了～</p><p>这里实现了高斯贝叶斯多分类器，继承自<code>SupervisedModel</code>基类（详见本人的<a href="https://github.com/Riroaki/LemonML/">repo</a>），主要方法实现了<code>fit,predict,evaluate</code>，代码中包含一定的注释，如有疑问可以在评论区留言。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> supervised<span class="token punctuation">.</span>_base <span class="token keyword">import</span> SupervisedModel<span class="token keyword">class</span> <span class="token class-name">Bayes</span><span class="token punctuation">(</span>SupervisedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Bayes model, multi-class (or binary) classifier.    Bayes models include Gaussian, Multinomial, Bernoulli,    however here I only implemented Gaussian.    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>_prior_dict <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_mean_dict <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_cov_dict <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_cov_all <span class="token operator">=</span> None        self<span class="token punctuation">.</span>_p <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>float<span class="token punctuation">:</span>        <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> label<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        n<span class="token punctuation">,</span> p <span class="token operator">=</span> x<span class="token punctuation">.</span>shape        <span class="token keyword">if</span> self<span class="token punctuation">.</span>_mean_dict <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_cov_dict <span class="token keyword">is</span> None \                <span class="token operator">or</span> self<span class="token punctuation">.</span>_prior_dict <span class="token keyword">is</span> None <span class="token operator">or</span> self<span class="token punctuation">.</span>_p <span class="token operator">!=</span> p<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>_prior_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            self<span class="token punctuation">.</span>_mean_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            self<span class="token punctuation">.</span>_cov_dict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>            self<span class="token punctuation">.</span>_p <span class="token operator">=</span> p        <span class="token comment" spellcheck="true"># Calculate mean and co-variance matrix for each class</span>        all_class <span class="token operator">=</span> np<span class="token punctuation">.</span>unique<span class="token punctuation">(</span>label<span class="token punctuation">)</span>        <span class="token keyword">for</span> c <span class="token keyword">in</span> all_class<span class="token punctuation">:</span>            group <span class="token operator">=</span> x<span class="token punctuation">[</span>label <span class="token operator">==</span> c<span class="token punctuation">]</span>            mean<span class="token punctuation">,</span> cov <span class="token operator">=</span> self<span class="token punctuation">.</span>__param_gaussian<span class="token punctuation">(</span>group<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>_prior_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> group<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> n            self<span class="token punctuation">.</span>_mean_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> mean            self<span class="token punctuation">.</span>_cov_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> cov        <span class="token comment" spellcheck="true"># Calculate the whole co-variance matrix</span>        _<span class="token punctuation">,</span> cov <span class="token operator">=</span> self<span class="token punctuation">.</span>__param_gaussian<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>_cov_all <span class="token operator">=</span> cov        <span class="token comment" spellcheck="true"># Calculate loss on x</span>        _<span class="token punctuation">,</span> loss <span class="token operator">=</span> self<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>x<span class="token punctuation">,</span> label<span class="token punctuation">)</span>        <span class="token keyword">return</span> loss    <span class="token keyword">def</span> <span class="token function">predict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_cov_dict <span class="token keyword">is</span> <span class="token operator">not</span> None <span class="token operator">and</span> self<span class="token punctuation">.</span>_mean_dict <span class="token keyword">is</span> <span class="token operator">not</span> None        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_cov_all <span class="token keyword">is</span> <span class="token operator">not</span> None        <span class="token keyword">assert</span> self<span class="token punctuation">.</span>_p <span class="token operator">==</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Default: non-linear classifier</span>        linear <span class="token operator">=</span> <span class="token boolean">False</span>        <span class="token keyword">if</span> <span class="token string">'linear'</span> <span class="token keyword">in</span> kwargs<span class="token punctuation">:</span>            <span class="token keyword">assert</span> isinstance<span class="token punctuation">(</span>kwargs<span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bool<span class="token punctuation">)</span>            linear <span class="token operator">=</span> kwargs<span class="token punctuation">[</span><span class="token string">'linear'</span><span class="token punctuation">]</span>        <span class="token comment" spellcheck="true"># Calculate posterior propability for each class</span>        <span class="token comment" spellcheck="true"># All class share a same co-variance matrix if linear == True</span>        prob<span class="token punctuation">,</span> label_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> c<span class="token punctuation">,</span> mean <span class="token keyword">in</span> self<span class="token punctuation">.</span>_mean_dict<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> linear<span class="token punctuation">:</span>                cov <span class="token operator">=</span> self<span class="token punctuation">.</span>_cov_all            <span class="token keyword">else</span><span class="token punctuation">:</span>                cov <span class="token operator">=</span> self<span class="token punctuation">.</span>_cov_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span>            prior <span class="token operator">=</span> self<span class="token punctuation">.</span>_prior_dict<span class="token punctuation">[</span>c<span class="token punctuation">]</span>            current_prob <span class="token operator">=</span> self<span class="token punctuation">.</span>__posterior_gaussian<span class="token punctuation">(</span>x<span class="token punctuation">,</span> prior<span class="token punctuation">,</span> mean<span class="token punctuation">,</span> cov<span class="token punctuation">)</span>            prob<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current_prob<span class="token punctuation">)</span>            label_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>c<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Get index of class having maximum probability for each x</span>        pred_val <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>prob<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        label_list <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>label_list<span class="token punctuation">)</span>        pred_label <span class="token operator">=</span> label_list<span class="token punctuation">[</span>pred_val<span class="token punctuation">]</span>        <span class="token keyword">return</span> pred_label    <span class="token keyword">def</span> <span class="token function">evaluate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> label<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        pred_label <span class="token operator">=</span> self<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Calculate 0-1 loss</span>        loss <span class="token operator">=</span> np<span class="token punctuation">.</span>count_nonzero<span class="token punctuation">(</span>pred_label <span class="token operator">-</span> label<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Use loss to calculate precision</span>        precision <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> loss <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> precision<span class="token punctuation">,</span> loss    @staticmethod    <span class="token keyword">def</span> <span class="token function">__param_gaussian</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> tuple<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Estimate mean and variance."""</span>        mean <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        diff <span class="token operator">=</span> x <span class="token operator">-</span> mean        cov <span class="token operator">=</span> np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>diff<span class="token punctuation">.</span>T<span class="token punctuation">,</span> diff<span class="token punctuation">)</span> <span class="token operator">/</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>        <span class="token keyword">return</span> mean<span class="token punctuation">,</span> cov    @staticmethod    <span class="token keyword">def</span> <span class="token function">__posterior_gaussian</span><span class="token punctuation">(</span>x<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> prior<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">,</span>                             mean<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">,</span> cov<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""Calculate posterior probability P(wi | x)."""</span>        <span class="token comment" spellcheck="true"># Calculate likelihood probability:</span>        <span class="token comment" spellcheck="true"># P(xj | wi) ~ 1 / sqrt(det(cov))</span>        <span class="token comment" spellcheck="true"># * exp(-0.5 * (xj - mean)^T * cov^(-1) * (xi - mean))</span>        diff <span class="token operator">=</span> x <span class="token operator">-</span> mean        coef <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>det<span class="token punctuation">(</span>cov<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5</span><span class="token punctuation">)</span>        inv <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>pinv<span class="token punctuation">(</span>cov<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Get exponent for xj (0 &lt; j &lt; n)</span>        exponents <span class="token operator">=</span> np<span class="token punctuation">.</span>apply_along_axis<span class="token punctuation">(</span>            <span class="token keyword">lambda</span> row<span class="token punctuation">:</span> np<span class="token punctuation">.</span>float<span class="token punctuation">(</span>np<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>row<span class="token punctuation">,</span> inv<span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>row<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> diff<span class="token punctuation">)</span>        likelihood <span class="token operator">=</span> coef <span class="token operator">*</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.5</span> <span class="token operator">*</span> exponents<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Posterior = prior * likelihood / evidence (omitted)</span>        posterior <span class="token operator">=</span> prior <span class="token operator">*</span> likelihood        <span class="token keyword">return</span> posterior<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="拓展：分类面形状"><a href="#拓展：分类面形状" class="headerlink" title="拓展：分类面形状"></a>拓展：分类面形状</h2><p>一般而言，贝叶斯分类器并不是一个线性分类器。</p><h3 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h3><p>不同类之间的分类面是高斯面之间的相交面，和每个类的模型参数有关。</p><p>用二维特征、多分类的高斯分类面的图像说明会更清晰（概率密度表现为图像中的高度z值，不同的峰表示不同类的中心概率密度）：</p><p><img src="/Machine-Learning-02-Bayes/multiclass.png" alt></p><p>限制了二维特征与二分类的高斯分布模型之后，分类面其实有一些有趣的规律，这是由高斯分布的形状造成的。</p><p>这里以二分类、二维特征的高斯分类模型为例说明这一点，下图展示了不同的参数（$\mu,\Sigma$）带来分类面的不同：</p><p><img src="/Machine-Learning-02-Bayes/2class.png" alt></p><p>对应的概率分布三维图：</p><p><img src="/Machine-Learning-02-Bayes/2class2.png" alt></p><p>那么，高斯模型的参数又是如何影响分类面形状的呢？</p><p>可以证明有以下结论（证明见附录）：</p><ul><li><p>类的$\mu$参数不会影响分类面的性质（线性/非线性），只会改变类在高维空间的中心位置。</p></li><li><p>如果所有特征均是独立分布，且所有类共享协方差矩阵，即：$\Sigma_j=\sigma^2I$，那么分类面是线性的，且两个类之间的分类直线垂直类中心之间的连线。</p></li><li><p>如果所有特征均共享协方差矩阵，即：$\Sigma_j=\Sigma=cov(X)$，那么分类面也是线性的，但是直线会存在一定的倾斜。</p></li><li><p>如果不满足这两个条件之间的任意情况，那么分类面是圆锥曲线（抛物线、双曲线、圆……）。</p></li></ul><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><ul><li>对偶尔的数据噪声鲁棒性好，因为使用了假设模型，相当于模型的信息不完全来自数据。</li><li>但是也因为模型自带假设，在不满足假设情形的数据上拟合效果不好。</li></ul><h2 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h2><h3 id="多维高斯参数估计推导（Deriving-the-Maximum-Likelihood-Estimators）"><a href="#多维高斯参数估计推导（Deriving-the-Maximum-Likelihood-Estimators）" class="headerlink" title="多维高斯参数估计推导（Deriving the Maximum Likelihood Estimators）"></a>多维高斯参数估计推导（Deriving the Maximum Likelihood Estimators）</h3><p>来源：<a href="https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian">https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian</a></p><p>或者查看文档版本：<a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf">https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf</a></p><p><img src="/Machine-Learning-02-Bayes/appendix1.png" alt></p><p><img src="/Machine-Learning-02-Bayes/appendix2.png" alt></p><p><img src="/Machine-Learning-02-Bayes/appendix3.png" alt></p><h3 id="高斯分布参数与分类面形状证明"><a href="#高斯分布参数与分类面形状证明" class="headerlink" title="高斯分布参数与分类面形状证明"></a>高斯分布参数与分类面形状证明</h3><p>来源：<a href="https://www.byclb.com/TR/Tutorials/neural_networks/ch4_1.htm">https://www.byclb.com/TR/Tutorials/neural_networks/ch4_1.htm</a></p><p>由于证明讨论过长，篇幅所限，在此不贴出详细证明内容。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-01（上）：机器学习综述</title>
      <link href="/Machine-Learning-01-Overview/"/>
      <url>/Machine-Learning-01-Overview/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第一篇文章的上半部分，内容包含了机器学习的理论综述、算法分类。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork和pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>如今深度学习、数据挖掘、机器学习这些概念已经🔥到成为满大街都是的概念，由于其门槛低（调包）和某些fancy的功能，加上媒体的宣传和高薪的诱惑，无论计算机专业还是非计算机专业出身的人们都热衷于在其中寻找机会，我这个软工的菜🐔也不例外。当然，目前正处于新手期。</p><p>这一个系列主要记录了我在ZJU上数据挖掘课学习和梳理的机器学习的知识，并且包含一些额外的补充知识。具体内容包括了数学理论和代码实现，希望能够给入门者（包括我自己）提供一个参考。</p><p>由于本人懒癌晚期，博客将不定期更新。</p><p>读者如果有问题或者留言可以直接在相关的博文下面留言，可以共同探讨解决。</p><p>当然也可以邮件联系本人：<a href="mailto:lilq1285@163.com">lilq1285@163.com</a>，欢迎理性讨论。</p><p><strong>本系列内容属于个人原创，转载请声明出处，商业转载请联系本人，邮箱同上。</strong></p><h2 id="机器学习总览"><a href="#机器学习总览" class="headerlink" title="机器学习总览"></a>机器学习总览</h2><p>机器学习发源于统计学，主要的目标是用数学和程序语言描述事物的规律，从而为预测、决策提供参考。</p><p>以全局的视角来看机器学习这一领域的算法，主要分为有监督（Supervised）学习和无监督（Unsupervised）学习两类，此外还有半监督（Half-supervised）学习、强化（Reinforcement）学习：</p><h3 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h3><h4 id="主要任务"><a href="#主要任务" class="headerlink" title="主要任务"></a>主要任务</h4><ul><li>回归（Regression）通常目标是得到连续的曲线，输出是连续的值</li><li>分类（Classification）通常目标是得到决策的边界，输出的是离散的类别</li></ul><h4 id="常见算法"><a href="#常见算法" class="headerlink" title="常见算法"></a>常见算法</h4><ul><li>Linear Regression：线性回归，以线性方式组合特征拟合连续曲线</li><li>Bayes：贝叶斯分类，通过概率模型计算样本属于各个分类的后验概率，进行分类</li><li>Logistic Regression：逻辑回归，在线性回归基础上增加激活函数以进行分类</li><li>Support Vector Machine：支持向量机，选取决策面较近的点用来计算决策面的参数</li><li>K Nearest Neighbor：K近邻，寻找距离较近的K个样本的标签取众数作为样本归类</li><li>Perceptron：感知机，二分类算法，最简单的神经网络</li><li>Decision Tree：决策树，可以看作从样本数据中学习if-else语句的组合，每一个判断都是数的一个节点，实现分类</li><li>Linear Discriminant Analysis：线性判别分析，通过找到特征的线性组合以用于降维，也是一种分类算法</li></ul><h4 id="算法的分类"><a href="#算法的分类" class="headerlink" title="算法的分类"></a>算法的分类</h4><ol><li>如果从算法解决的问题分类，可以分为回归和分类两大类算法：</li></ol><ul><li>其中，线性回归为回归类的算法，其余算法均主要用于分类，当然也可以有回归的作用。因为这些分类算法大多是在连续的输出外进行处理获得类别，如逻辑回归、感知机、支持向量机等，如果用在回归上则输出的是分类前计算的结果。</li></ul><ol start="2"><li>如果从决策面的角度来看，上述的分类算法可以分为线性分类算法和非线性分类算法：</li></ol><ul><li>线性分类算法：分类面为线性/输出函数为线性形式（本质相同，采用不同的目标函数得到的模型）<ul><li>包括：逻辑回归、支持向量机、感知机、线性判别分析</li></ul></li><li>非线性算法：分类面为非线性/输出函数的形式为非线性<ul><li>包括：贝叶斯、K近邻、决策树</li></ul></li></ul><ol start="3"><li>如果从算法的实际含义角度看，上述的分类算法可以分为生成模型和判别模型：</li></ol><ul><li>生成模型：按照条件概率建立模型，基于高斯分布等假设，学习模型的参数用于分类<ul><li>包括：贝叶斯模型、线性判别分析</li></ul></li><li>判别模型：出于最大化在测试集上的表现，进行训练<ul><li>包括：大部分其他分类算法</li></ul></li></ul><p>在基本算法的基础上，现代机器学习常见的还有集成（Ensemble）学习，其核心是”三个臭屁匠，顶个诸葛亮”，并不致力于产生最强的单个分类器，而是通过把训练不同的较弱分类器，并进行集合决策以获得最好的分类效果。</p><h4 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h4><ul><li>Bagging/Bootstrap Aggregating：通过随机切分数据集，并行训练相同模型以获得更好的分类效果。<ul><li>随机森林（Random Forest）算法正是基于bagging算法实现。</li></ul></li><li>Boosting：通过训练一系列弱分类器并组合获得强分类器。</li><li>Stacking：训练一个组合不同模型的高层模型进行分类（上面两种算法对底层模型的组合方式是确定的）。</li></ul><h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><h4 id="主要任务-1"><a href="#主要任务-1" class="headerlink" title="主要任务"></a>主要任务</h4><ul><li>降维（Dimensionality Reduction）指的是将样本空间从高维特征投影到较低维度的特征从而实现提高计算效率的作用。</li><li>聚类（Clustering）指的是将无标签的样本按照样本之间的距离信息等，将相近的样本归为一个簇的算法，可以理解为没有样本标签的分类算法。</li></ul><h4 id="常见算法-1"><a href="#常见算法-1" class="headerlink" title="常见算法"></a>常见算法</h4><ul><li>Principle Component Analysis：主成分分析，通过提取协方差矩阵中的特征向量作为新特征实现降维。<ul><li>与线性判别分析（LDA）相似的算法。</li></ul></li><li>K Means：K均值算法，通过抽取相近点簇的重心作为簇的代表来实现聚类。</li><li>K Medoids：K中心点算法，和K Means算法相近，不同的是选取簇中最接近重心的点作为簇的代表。</li><li>Spectral Clustering：谱聚类，通过降维方法和K Means算法实现聚类。</li><li>Gaussian Mixture Model：高斯混合模型，是基于高斯分布的假设，通过点簇的分布估计参数以实现聚类。<ul><li>K Means算法可以视为GMM的一种特殊形式。</li></ul></li><li>Matrix Factorization：矩阵分解，是一类降维算法，包括奇异值分解、矩阵非负分解和稀疏编码等算法。</li></ul><h4 id="算法的分类-1"><a href="#算法的分类-1" class="headerlink" title="算法的分类"></a>算法的分类</h4><ol><li>如果按照主要任务，可以将算法分为降维算法和聚类算法：</li></ol><ul><li>降维算法：主要包括主成分分析、矩阵分解</li><li>聚类算法：主要包括K均值、K中心点、谱聚类、高斯混合模型</li></ul><h3 id="半监督学习"><a href="#半监督学习" class="headerlink" title="半监督学习"></a>半监督学习</h3><p>利用少量标注样本和大量未标注样本进行机器学习的算法。</p><p>由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。</p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>没有特定的目标，强调环境的反馈作用，通过应对环境调整策略的算法。</p><p>由于本人并不了解这一块，所以此处内容不作详细介绍，有兴趣者请自行谷歌。</p><h3 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h3><p>这一块是近十年新的方向，也是目前机器学习最火的分支，但是预计不会在近期内容中出现。</p><p>简言之，深度学习就是基于神经网络的算法，通过组合线性的神经元和非线性的激活层，以及搭建不同结构的网络，来实现回归或者预测、聚类等工作。</p><p>其”神经网络”形态的灵感得益于生物大脑的神经元连接结构，让人联想到”机器的大脑🧠”，加上诸如alphaGo等等一些新奇的成就带来的狂热使得众人为之疯狂，许多营销号和媒体甚至脑洞大开，大肆鼓吹”人工智能有害论”。</p><p>但目前而言，可解释性差、缺乏较统一的数学理论描述是其硬伤。而且也没有出现强人工智能的迹象，目前的神经网络，本质只是一种复杂的统计模型。</p><p>随着研究陷入瓶颈，这场资本与舆论的狂欢已经在逐渐冷却，未来究竟如何发展也未可知：）</p><h2 id="机器学习方法论"><a href="#机器学习方法论" class="headerlink" title="机器学习方法论"></a>机器学习方法论</h2><p><strong>机器学习的本质在于从数据或者假设中建立模型、学习参数，去拟合一个未知的函数。</strong></p><p>根据论文《A Few Useful Things to Know about Machine Learning》，机器学习的过程可以表示为：</p><p>$LEARNING = REPRESENTATION + EVALUATION + OPTIMIZATION$</p><p>也就是说，机器学习主要分为三个过程：</p><ol><li>表示（Representation）：使用计算机能够执行的语言描述算法。这个阶段确定了模型的类型，所以决定了拟合/分类函数的假设空间（Hypothesis Space）——也就是说，在这一步，模型的参数个数和模型的计算方式已经确定，比如线性模型的$y=WX+B$，那么模型无法模拟非线性的分类/回归，这是选取的模型导致的。而具体是如何线性的函数，需要在接下来的过程中确定。</li><li>评估（Evaluation）：用于评估模型的好坏。根据任务的不同（回归、分类）确定了不同的种类，同时这个评估方法应当是能够方便地找到对应的优化函数的（更明确一点，评估的函数应该是可导的）。我们训练的目标就是最小化目标函数（误差型）或者最大化目标函数（精度型）。</li><li>优化（Optimization）：评估函数就像考试，有了考试我们就可以知道自己的薄弱环节，从而确定努力的方向。而有了评估函数，就有一个对应的优化函数用于调整模型的参数。<ul><li>通常我们采用基于梯度的方法，具体会在下面梯度下降这一概念中解释。</li></ul></li></ol><p>论文中列出了一个关于这三个部分的表格，在这里贴出来：</p><p><img src="/Machine-Learning-01-Overview/table.png" alt></p><p>从表格也可以看出来，对某一种算法，并非所有的评估函数都能够使用，有些算法是绑定了评估函数的。</p><p>同时，评估函数与优化函数存在对应关系，选择某一类评估函数时，对应的优化策略也就决定了。</p><p>（接下篇：<a href="/Machine-Learning-01-Overview-2">机器学习概念</a>）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学不动了-01（下）：机器学习概念</title>
      <link href="/Machine-Learning-01-Overview-2/"/>
      <url>/Machine-Learning-01-Overview-2/</url>
      
        <content type="html"><![CDATA[<p>本文是”机器学不动了”系列的第一篇文章的下半部分，内容包含了机器学习的概念解释。</p><p>全系列推荐结合个人实现的代码食用：<a href="https://github.com/Riroaki/LemonML/">https://github.com/Riroaki/LemonML/</a></p><p>欢迎star、fork和pr。</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>紧接上半篇文章的算法介绍，这里主要会介绍一些基本概念。</p><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><p>特征缩放（Feature Scaling）是一项预处理技术，它将所有的输入按照统一的标准进行处理：</p><ul><li>最大最小缩放（Min-Max Normalization）：把每一个特征的各个值按照大小缩放到$[0,1]$的区间中。</li><li>均值缩放（Mean Normalization）：把每一个特征的各个值按照大小缩放到$[-1, 1]$区间中。</li><li>标准化（Standardization）：把每一个特征缩放成平均值为0，方差为1的变量。</li><li>单位化（Scaling to Unit Length）：把每一个样本的长度（即向量的第二范数）缩放为1。</li></ul><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="泛化-、方差、偏差和噪声"><a href="#泛化-、方差、偏差和噪声" class="headerlink" title="泛化 、方差、偏差和噪声"></a>泛化 、方差、偏差和噪声</h3><p>首先，泛化（Generalization）是指模型在经过一定的数据训练之后对现实数据进行测试，我们希望模型能够最小化测试误差（testing error），而训练数据集上的误差与测试数据集上的误差就是泛化误差（Generalization Error）。</p><p>而泛化误差分为方差（Variance）与偏差（Bias），通过这两个方面可以描述模型与现实模型的误差：</p><ul><li>Bias是模型预测与真实结果的差距，可以直观理解为训练误差（training error），表现了模型的拟合能力；</li><li>Variance则是“<strong>（大小相同的）不同训练数据集</strong>训练出的模型”的训练误差之间的差异，表现了数据扰动的影响。</li></ul><p>通常来说，模型越复杂（组成模型的参数越多），方差越大，偏差越小，这是因为模型的描述能力越强；模型越简单，偏差也容易大，很可能无法拟合训练数据。</p><p>通常模型复杂程度与方差/偏差的关系：</p><p><img src="/Machine-Learning-01-Overview-2/complex.png" alt></p><p>在图中可以看到，随着模型变得复杂，训练误差/偏差变小，而方差（在图中可以看作测试误差与训练误差之间的差值）变大，训练误差在中间有一个较低值。</p><p>由此启发我们寻找一个复杂度的平衡点，使得模型具有较低的bias和variance；至于noise则是无法改变的。</p><p>此外，还有一个概念叫做噪声（noise）：噪声在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。</p><p>模型的误差主要来自三部分的总和。</p><h3 id="过拟合-欠拟合"><a href="#过拟合-欠拟合" class="headerlink" title="过拟合/欠拟合"></a>过拟合/欠拟合</h3><ul><li>欠拟合主要描述的是模型复杂度过低，难以拟合训练数据，此时偏差过大（上图左侧部分）</li><li>过拟合是指模型过于复杂，虽然在训练数据上能够较好拟合，但是在测试数据上误差极大，此时偏差较小而误差较大（上图右侧部分）</li></ul><p>需要注意的是，测试误差较大不能说明是过拟合还是欠拟合；需要看训练误差的大小以区分。</p><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="混淆矩阵与风险矩阵"><a href="#混淆矩阵与风险矩阵" class="headerlink" title="混淆矩阵与风险矩阵"></a>混淆矩阵与风险矩阵</h3><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><p>一张图说明混淆矩阵：</p><p><img src="/Machine-Learning-01-Overview-2/confuse.jpg" alt></p><p>混淆矩阵用于预测与实际的差距，对一个N元分类器而言是一个N*N的矩阵，$M[i][j]$表示了预测类为$i$，真实类为$j$的样本数。显然，正确的分类落在矩阵的主对角线上，即所有的$M[i][i]$元素。而其他项表示了分类错误的个数。</p><p>当然也有</p><p>对于二分类而言，我们会把某一个类叫做正类，另一个类叫做负类，预计某个类为正类叫做阳性，反之叫阴性，所以又产生了如下概念：</p><ul><li>预测和真实均为正类的叫做<strong>真阳性</strong>（TP，True Positive）</li><li>预测与真实均为负类的叫做<strong>真阴性</strong>（TF，True Negative）</li><li>预测为正类而真实为负类的叫做<strong>假阳性</strong>（FP，False Positive）</li><li>预测为负类而真实为正类的叫做<strong>假阴性</strong>（FN，False Negative）</li></ul><p>稍加拓展，当应用在多分类上，就是把分类错误统称为负类，分类正确的当做正类。</p><p>由此出发，我们得到新的概念作为评价指标：</p><ul><li><strong>准确率</strong>（Accuracy）：$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$</li><li><strong>精确率</strong>（Precision）：$Precision=\frac{TP}{TP+FP}$</li><li><strong>召回率</strong>（Recall）：$Recall=\frac{TP}{TP+FN}$</li></ul><p>这些概念容易搞混，另外与搜索引擎的评估也有一定关联。</p><p>那么这三个分类标准分别有什么特点和适用场景？</p><p>我们以检测疾病的分类器为例说明：</p><ol><li>Accuracy与分类目标无关，实际只看分对了没有；在类别不均衡的问题上评估粒度太大，不如后两种手段有效。因此，在没有特定要求某个类的准确率而是关注整体准确率的时候，使用这一指标。</li><li>Precision是指分类器所挑出的某个类中，真正是我们希望的那一类的概率。使用这个为指标就是期待分类器降低把没病的人当作有病的概率。</li><li>Recall是指没有被识别出是我们想要的那一类的概率。简单来说就是使用这个指标就是期待分类器不要错放过有病的人。</li></ol><h4 id="风险矩阵"><a href="#风险矩阵" class="headerlink" title="风险矩阵"></a>风险矩阵</h4><p>生活经验中，同样是分类错误，我们对于不同错误分类的容忍度往往是不同的，比如：</p><ul><li>垃圾邮件分类问题中，相比重要邮件被错误分类为垃圾邮件而进入垃圾箱，我们更情愿多收到一些被当作正常邮件的垃圾邮件。<ul><li>在这里如果正类是垃圾邮件，那么我们关注Precision多于Recall；反过来如果正常邮件是正类，那么我们更加关注Recall。</li></ul></li><li>当判决某个细胞是正常细胞还是癌细胞的时候，显然把一个正常细胞错判为癌细胞的风险要比把一个癌细胞错判为正常细胞的风险大很多，后者的错误是致命的。<ul><li>如果我们的正类是癌细胞，那么我们关注Recall多于Precision，因为我们不希望放过每一个正类。</li></ul></li></ul><p>在这些情形下，同样都是分类错误，某一种分类错误的影响更严重，所以并不是最小错误率（Minimum Probability Error）而是最小风险误差（Minimum Risk）才能够表示我们的期待，这个时候我们会把分类错误添加一个权重，使用权重来改变评判标准。</p><p>风险（Risk），可以理解为对某种错误分类情形下造成后果的严重程度，形状和混淆矩阵一致，是人为添加的半定量矩阵。</p><p>我们将风险矩阵与混淆矩阵对应位置元素相乘得到的总和就是新的目标函数值，我们的分类结果应当使得这一目标函数值达到最小。</p><p>这一改变将如何影响我们的分类策略？</p><ul><li>对一个样本$X_i$：<ul><li>对每一个类j，我们计算出$X_i$属于j类的概率分别为$P(w_j|X_i)$，并计算j类的误分类风险之和：$\Sigma_{i!=j}M[j][i]$</li><li>将类的误分类风险与概率相乘，乘积就是j类误分类的概率风险</li><li>将每一个类的误分类概率风险求出，找到概率风险最小的那一个分类作为当前的分类</li></ul></li></ul><p>按照这一方法分类计算得到的误分类概率风险是最小的。</p><h3 id="从二分类到多分类"><a href="#从二分类到多分类" class="headerlink" title="从二分类到多分类"></a>从二分类到多分类</h3><p>常见的分类器如支持向量机、感知机只能做到二分类，那么多分类问题应该如何解决？</p><p>主要有以下两种思路：</p><h4 id="Ont-versus-One：一对一"><a href="#Ont-versus-One：一对一" class="headerlink" title="Ont-versus-One：一对一"></a>Ont-versus-One：一对一</h4><p>对每一组不同的类$j_1,j_2$，我们构造一个二分类器；</p><p>然后，对每一个样本，计算所有分类器，对每一个类进行投票。</p><h4 id="One-versus-Rest：一对其余"><a href="#One-versus-Rest：一对其余" class="headerlink" title="One-versus-Rest：一对其余"></a>One-versus-Rest：一对其余</h4><p>对每一个类$j$，构造一个二分类器，区分的类是第j类和所有的其他类；</p><p>然后，对每一个样本，计算所有分类器，此时如果只有一个分类器预测为正类，那么就将其分类为正类；否则，在预测正类的类中挑选置信度最大分类器对应的类。</p><h4 id="二者的比较"><a href="#二者的比较" class="headerlink" title="二者的比较"></a>二者的比较</h4><p>OvO只需要两个类的样本，但是每个分类器需要训练$k(k-1)/2$个分类器；</p><p>OvR需要k个分类器，但是每个分类器需要训练全部样本。</p><p>综合来看，OvO训练的时间开销较小，OvR的存储开销较小。</p><h4 id="Multi-versus-Multi：多对多"><a href="#Multi-versus-Multi：多对多" class="headerlink" title="Multi-versus-Multi：多对多"></a>Multi-versus-Multi：多对多</h4><p>每次选取特定的多个类作为正类，特定的多个类作为负类进行分类，从而确定所属的类区间。</p><p>这里选取的类不能随意选取，主要有纠错输出码技术（Error-Correcting Output Codes，ECOC），在此不做展开 ，有兴趣可以参考：<a href="https://hyper.ai/wiki/4350">https://hyper.ai/wiki/4350</a></p><p>这个推广对其它二分类分类器也适用。</p><h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><p>类别不平衡（class imbalance），又称为数据偏斜（class skew）。</p><p>以二分类问题为例，该问题一般指的是训练集中正负样本数比例相差过大，一般会造成：</p><ol><li><p>类别少的误判惩罚过低，导致有所偏袒，当样本不确定时倾向于把样本分类为多数类。</p></li><li><p>样本数量分布很不平衡时，特征的分布同样会不平衡。</p></li><li><p>传统的评价指标变得不可靠，例如准确率。</p></li></ol><p>而在多分类问题中，尽管原始训练集中可能不同类别训练样本数目相当，通过OvR、MvM进行拆分时也有可能会造成上述情况，所以类别不平衡问题亟待解决。</p><p>常见的解决方案有：</p><ul><li>对较多的那个类别进行欠采样(under-sampling)，舍弃一部分数据，使其与较少类别的数据相当。</li><li>对较少的类别进行过采样(over-sampling)，重复使用一部分数据，使其与较多类别的数据相当。</li><li>阈值调整（threshold moving），将原本默认为0.5的阈值调整到 较少类别/（较少类别+较多类别）即可。</li></ul><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证（Cross Validation）是一种避免过拟合的训练技巧。</p><p>具体思路在于将训练切分，每一次用不同的数据集来训练，优化平均的误差，从而降低不同数据集带来模型性能的变化，达到降低方差的目的。</p><p>主要有两种方法：</p><ul><li>K折验证（K-Fold）：指将数据切分为K份，每一份轮流作为验证集（Validation Set），其他数据作为训练数据，训练K轮次获得训练误差。</li><li>留一验证（Leave-One-Out）：是K=n的K折验证，通过每次取一个样本作为验证集进行交叉验证训练。</li></ul><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降（Gradient Descent）是在通常模型中通用的迭代型参数估计方法。</p><p>我们可以认为，目标函数是一个自变量为模型参数的函数，而我们希望达到它的最大/最小值。</p><p>我们知道，一个函数在最大或者最小值的位置，它的一阶梯度为全0的向量。至于它究竟是最大值还是最小值，得看其二阶导数，或者进行测试局部变化来验证。</p><p>当然，通常我们会希望目标函数是纯凸/纯凹的，因为这样它的驻点（极大极小值点）只有一个，一旦找到极值点就能够确定它是最优的。在这一理论的驱动下，诞生了凸优化这一学科，目标就是把各种非凸问题转化成凸的问题。</p><p>因此我们会对它进行求导，寻找一阶导数为0的点对应的自变量，也就是模型参数的值。</p><p>此时，我们可以直接利用等式的梯度为0求解出参数，也可以采用迭代求解的梯度下降方法。</p><p>前者看起来不是更直接而且精确嘛？但是事实上我们大多采用的是后者。理由就是，第一个方法的实质是计算方程组的解，涉及求逆矩阵的过程，但是一来计算量大，二来难以保证矩阵非奇异或者非病态的情况下，计算过程对方程组值的扰动非常敏感，噪声带来的误差较大导致结果偏离理论解。</p><p>那么，后者是如何操作的？</p><p>在每次训练时，减去梯度值和学习率的乘积。对于一个局部凸的部分，我们可以看到在减去梯度之后我们的参数坐标会向极值点（最低点）靠近，且梯度绝对值越大，下降越快。</p><p>理论依据：梯度的反方向就是函数局部值下降最快的方向。</p><p><img src="/Machine-Learning-01-Overview-2/gradient.png" alt></p><p>为了快速收敛、避免震荡的目的，也出现了很多学习率优化算法，如自适应性优化（Adam）、Adagrad和随机梯度下降（SGD）、Momentum等策略，这一块暂时不做介绍。</p><h3 id="批量梯度下降-随机梯度下降"><a href="#批量梯度下降-随机梯度下降" class="headerlink" title="批量梯度下降/随机梯度下降"></a>批量梯度下降/随机梯度下降</h3><p>这是梯度下降的两种操作方式。</p><ul><li>随机梯度下降（Stochastic Gradient Descent）是指，对每一个训练的样本都计算一次梯度并且用梯度执行更新参数的操作。这种方法的好处是更新次数快，且存在一定的随机性不会陷入局部极小值；但是也因为随机性强，往往梯度的波动大，某一两个样本带来的参数变化太大，更新不稳定，甚至导致不收敛 。</li><li>批量梯度下降（Batch Gradient Descent）是指，每次对所有训练样本进行计算梯度并且只用所有梯度的平均值进行一次更新。这种方法的好处自然就是稳定更新；但是其更新太慢，在一定的时间里难以达到收敛，而且也容易陷入局部最小值，最终在较小的梯度下停止更新。</li></ul><p>一般来说现有的技巧在于折衷两种方案，进行小批量的梯度下降，并且打乱样本以获取随机性。</p><p>这样做的好处在于：</p><ol><li>利用了随机梯度下降的随机性，一般不会陷入局部极小值。</li><li>更新速度适中，保持较好的稳定性不会震荡，同时也能够较快达到收敛。</li><li>最重要的是，方便底层GPU优化。因为梯度计算的底层操作是矩阵运算，而GPU由于多核计算能够并行地计算某一行的计算结果，从而加速梯度更新过程。所以一般而言，小批量梯度下降的效率比随机梯度下降更高。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 机器学不动了 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Mining </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Restart</title>
      <link href="/Restart/"/>
      <url>/Restart/</url>
      
        <content type="html"><![CDATA[<p>今天把博客文章全都清空了。</p><p>主要是之前的文章太乱，缺乏整理；加上近期学了很多东西之后，回头看过去的内容觉得有些浅薄，决心从头开始写。</p><p>今后会在这里写一些机器学习，以及数据处理的东西。</p><p>当然还有一些工程向的内容，总之我会更加深思熟虑地推送文章。</p><p>（是不是也考虑一下换主题呢……哈哈还是算了估计又要挑很久）</p>]]></content>
      
      
      
        <tags>
            
            <tag> Hello, world! </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
